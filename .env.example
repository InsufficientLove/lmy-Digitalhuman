# MuseTalk GPU Configuration
# GPU环境配置文件示例

# ============ GPU配置 ============
# 可见的GPU设备ID（逗号分隔）
CUDA_VISIBLE_DEVICES=0,1,2,3

# GPU配置模式: auto, single_gpu, multi_gpu
GPU_CONFIG_MODE=auto

# 每个GPU的批处理大小
BATCH_SIZE_PER_GPU=8

# 总批处理大小（多GPU时为所有GPU的总和）
TOTAL_BATCH_SIZE=32

# GPU内存使用比例 (0.0-1.0)
GPU_MEMORY_FRACTION=0.9

# ============ 性能优化 ============
# 启用自动混合精度 (true/false)
ENABLE_AMP=true

# 启用CuDNN基准测试优化
ENABLE_CUDNN_BENCHMARK=true

# 启用TF32加速（仅限Ampere架构）
ENABLE_TF32=true

# PyTorch内存分配配置
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# ============ 并行处理 ============
# 推理模式: serial, parallel
INFERENCE_MODE=parallel

# 并行策略: round_robin, load_balance, data_parallel
PARALLEL_STRATEGY=data_parallel

# 预处理工作线程数
PREPROCESS_WORKERS=4

# ============ 模型路径 ============
# 模型根目录
MODEL_ROOT=/app/models

# MuseTalk版本: v1.0, v1.5
MUSETALK_VERSION=v1.5

# 模型文件路径
UNET_MODEL_PATH=${MODEL_ROOT}/musetalkV15/unet.pth
UNET_CONFIG_PATH=${MODEL_ROOT}/musetalkV15/musetalk.json
VAE_MODEL_PATH=${MODEL_ROOT}/sd-vae
WHISPER_MODEL_PATH=${MODEL_ROOT}/whisper
FACE_PARSING_PATH=${MODEL_ROOT}/face-parse-bisent
DWPOSE_MODEL_PATH=${MODEL_ROOT}/dwpose

# ============ 输入输出 ============
# 输入目录
INPUT_DIR=/app/inputs

# 输出目录
OUTPUT_DIR=/app/outputs

# 日志目录
LOG_DIR=/app/logs

# ============ 服务配置 ============
# API服务端口
API_PORT=8000

# Web界面端口
WEB_PORT=8080

# 监控端口
MONITOR_PORT=9090

# ============ 日志配置 ============
# 日志级别: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# 是否启用性能监控
ENABLE_MONITORING=true

# ============ 缓存配置 ============
# 启用帧缓存
CACHE_FRAMES=true

# 启用音频缓存
CACHE_AUDIO=true

# 缓存大小限制（GB）
CACHE_SIZE_LIMIT=10

# ============ Docker配置 ============
# Docker运行时
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility,video

# 共享内存大小
SHM_SIZE=8g

# ============ 网络配置 ============
# 使用国内镜像源（中国用户）
USE_CHINA_MIRROR=false

# HuggingFace镜像端点
HF_ENDPOINT=https://huggingface.co

# PyPI镜像源
PIP_INDEX_URL=https://pypi.org/simple
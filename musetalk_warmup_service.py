#!/usr/bin/env python3
"""
MuseTalkæ¨¡å‹é¢„çƒ­æœåŠ¡
ä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­ï¼Œé¿å…é‡å¤åŠ è½½ï¼Œå®ç°æ€¥é€Ÿæ¨ç†
"""

import os
import sys
import time
import torch
import threading
import queue
import json
from pathlib import Path
import argparse
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MuseTalkWarmupService:
    def __init__(self):
        self.model_loaded = False
        self.unet = None
        self.vae = None
        self.whisper_model = None
        self.dwpose_model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.request_queue = queue.Queue()
        self.response_cache = {}
        self.lock = threading.Lock()
        
    def load_models(self):
        """é¢„åŠ è½½æ‰€æœ‰MuseTalkæ¨¡å‹åˆ°å†…å­˜"""
        logger.info("ğŸš€ å¼€å§‹é¢„åŠ è½½MuseTalkæ¨¡å‹...")
        
        try:
            # è®¾ç½®ä¼˜åŒ–ç¯å¢ƒå˜é‡
            os.environ.update({
                "CUDA_VISIBLE_DEVICES": "0,1,2,3",
                "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:2048",
                "TORCH_BACKENDS_CUDNN_BENCHMARK": "1",
                "OMP_NUM_THREADS": "16",
            })
            
            # é¢„åŠ è½½UNetæ¨¡å‹
            logger.info("ğŸ“¦ åŠ è½½UNetæ¨¡å‹...")
            from musetalk.models.unet import UNet
            with open("models/musetalk/musetalk.json", 'r') as f:
                unet_config = json.load(f)
            
            self.unet = UNet(**unet_config)
            unet_checkpoint = torch.load("models/musetalk/pytorch_model.bin", map_location=self.device)
            self.unet.load_state_dict(unet_checkpoint)
            self.unet.to(self.device).eval()
            self.unet.half()  # ä½¿ç”¨FP16
            logger.info("âœ… UNetæ¨¡å‹åŠ è½½å®Œæˆ")
            
            # é¢„åŠ è½½VAEæ¨¡å‹
            logger.info("ğŸ“¦ åŠ è½½VAEæ¨¡å‹...")
            from diffusers import AutoencoderKL
            self.vae = AutoencoderKL.from_pretrained("models/sd-vae", torch_dtype=torch.float16)
            self.vae.to(self.device).eval()
            logger.info("âœ… VAEæ¨¡å‹åŠ è½½å®Œæˆ")
            
            # é¢„åŠ è½½Whisperæ¨¡å‹
            logger.info("ğŸ“¦ åŠ è½½Whisperæ¨¡å‹...")
            import whisper
            self.whisper_model = whisper.load_model("base", download_root="models/whisper")
            self.whisper_model.to(self.device)
            logger.info("âœ… Whisperæ¨¡å‹åŠ è½½å®Œæˆ")
            
            # é¢„åŠ è½½DWPoseæ¨¡å‹
            logger.info("ğŸ“¦ åŠ è½½DWPoseæ¨¡å‹...")
            from musetalk.utils.dwpose import DWposeDetector
            self.dwpose_model = DWposeDetector()
            logger.info("âœ… DWPoseæ¨¡å‹åŠ è½½å®Œæˆ")
            
            # é¢„çƒ­æ¨ç†
            self._warmup_inference()
            
            self.model_loaded = True
            logger.info("ğŸ‰ æ‰€æœ‰æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼ŒæœåŠ¡å·²å°±ç»ªï¼")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _warmup_inference(self):
        """é¢„çƒ­æ¨ç†è¿‡ç¨‹"""
        logger.info("ğŸ”¥ æ‰§è¡Œæ¨¡å‹é¢„çƒ­...")
        
        try:
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥è¿›è¡Œé¢„çƒ­
            dummy_audio = torch.randn(1, 80, 100).to(self.device).half()
            dummy_image = torch.randn(1, 3, 256, 256).to(self.device).half()
            
            with torch.no_grad():
                # é¢„çƒ­VAE
                _ = self.vae.encode(dummy_image)
                
                # é¢„çƒ­UNetï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                dummy_latent = torch.randn(1, 4, 32, 32).to(self.device).half()
                dummy_timestep = torch.tensor([1]).to(self.device)
                
            logger.info("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ é¢„çƒ­è¿‡ç¨‹å‡ºç°è­¦å‘Š: {e}")
    
    def process_request(self, video_path, audio_path, bbox_shift=0):
        """å¤„ç†æ¨ç†è¯·æ±‚"""
        if not self.model_loaded:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_models()")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{Path(video_path).stem}_{Path(audio_path).stem}_{bbox_shift}"
        
        with self.lock:
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in self.response_cache:
                logger.info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜ç»“æœ: {cache_key}")
                return self.response_cache[cache_key]
        
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†æ¨ç†è¯·æ±‚: {video_path}")
        start_time = time.time()
        
        try:
            # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ¨ç†é€»è¾‘
            # ç”±äºæ¨¡å‹å·²ç»é¢„åŠ è½½ï¼Œè¿™é‡Œåªéœ€è¦å¤„ç†æ•°æ®å’Œæ¨ç†
            result_path = self._run_inference(video_path, audio_path, bbox_shift)
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            # ç¼“å­˜ç»“æœ
            with self.lock:
                self.response_cache[cache_key] = result_path
            
            return result_path
            
        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
            raise
    
    def _run_inference(self, video_path, audio_path, bbox_shift):
        """æ‰§è¡Œå®é™…æ¨ç†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ¨ç†é€»è¾‘
        # ç”±äºæ¨¡å‹å·²ç»åœ¨å†…å­˜ä¸­ï¼Œé¿å…äº†é‡å¤åŠ è½½çš„å¼€é”€
        
        # ä¸´æ—¶è¿”å›è·¯å¾„ï¼Œå®é™…åº”è¯¥æ˜¯æ¨ç†ç»“æœ
        output_dir = "results/warmup_service"
        os.makedirs(output_dir, exist_ok=True)
        
        result_filename = f"output_{int(time.time())}.mp4"
        result_path = os.path.join(output_dir, result_filename)
        
        # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ¨ç†ä»£ç 
        # ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡å‹è¿›è¡Œå¿«é€Ÿæ¨ç†
        
        return result_path
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        with self.lock:
            self.response_cache.clear()
        logger.info("ğŸ§¹ ç¼“å­˜å·²æ¸…ç†")
    
    def get_status(self):
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            "model_loaded": self.model_loaded,
            "device": self.device,
            "cache_size": len(self.response_cache),
            "gpu_memory": torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0
        }

def start_warmup_service():
    """å¯åŠ¨é¢„çƒ­æœåŠ¡"""
    service = MuseTalkWarmupService()
    
    try:
        # é¢„åŠ è½½æ¨¡å‹
        service.load_models()
        
        # ä¿æŒæœåŠ¡è¿è¡Œ
        logger.info("ğŸŒŸ MuseTalké¢„çƒ­æœåŠ¡å·²å¯åŠ¨ï¼Œç­‰å¾…è¯·æ±‚...")
        
        while True:
            # è¿™é‡Œå¯ä»¥æ·»åŠ HTTPæœåŠ¡å™¨æˆ–å…¶ä»–é€šä¿¡æœºåˆ¶
            # ç›®å‰åªæ˜¯ä¿æŒè¿›ç¨‹è¿è¡Œ
            time.sleep(1)
            
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ æœåŠ¡æ­£åœ¨å…³é—­...")
    except Exception as e:
        logger.error(f"ğŸ’¥ æœåŠ¡é”™è¯¯: {e}")

def benchmark_warmup_vs_cold():
    """å¯¹æ¯”é¢„çƒ­æœåŠ¡ä¸å†·å¯åŠ¨çš„æ€§èƒ½"""
    logger.info("ğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    # æµ‹è¯•æ•°æ®
    test_video = "data/test_image.jpg"  # éœ€è¦å‡†å¤‡æµ‹è¯•æ–‡ä»¶
    test_audio = "data/test_audio.wav"
    
    if not (os.path.exists(test_video) and os.path.exists(test_audio)):
        logger.warning("âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åŸºå‡†æµ‹è¯•")
        return
    
    # å†·å¯åŠ¨æµ‹è¯•
    logger.info("â„ï¸ æµ‹è¯•å†·å¯åŠ¨æ€§èƒ½...")
    cold_start = time.time()
    # è¿™é‡Œåº”è¯¥è°ƒç”¨åŸå§‹çš„æ¨ç†è„šæœ¬
    cold_time = time.time() - cold_start
    
    # é¢„çƒ­æœåŠ¡æµ‹è¯•
    logger.info("ğŸ”¥ æµ‹è¯•é¢„çƒ­æœåŠ¡æ€§èƒ½...")
    service = MuseTalkWarmupService()
    service.load_models()
    
    warm_start = time.time()
    service.process_request(test_video, test_audio)
    warm_time = time.time() - warm_start
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    speedup = cold_time / warm_time if warm_time > 0 else 0
    logger.info("ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    logger.info(f"   å†·å¯åŠ¨: {cold_time:.2f}ç§’")
    logger.info(f"   é¢„çƒ­æœåŠ¡: {warm_time:.2f}ç§’")
    logger.info(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")

def main():
    parser = argparse.ArgumentParser(description="MuseTalkæ¨¡å‹é¢„çƒ­æœåŠ¡")
    parser.add_argument("--start", action="store_true", help="å¯åŠ¨é¢„çƒ­æœåŠ¡")
    parser.add_argument("--benchmark", action="store_true", help="è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--status", action="store_true", help="æ£€æŸ¥æœåŠ¡çŠ¶æ€")
    
    args = parser.parse_args()
    
    if args.start:
        start_warmup_service()
    elif args.benchmark:
        benchmark_warmup_vs_cold()
    elif args.status:
        service = MuseTalkWarmupService()
        if service.model_loaded:
            status = service.get_status()
            logger.info(f"ğŸ“Š æœåŠ¡çŠ¶æ€: {json.dumps(status, indent=2)}")
        else:
            logger.info("âŒ æœåŠ¡æœªå¯åŠ¨")
    else:
        logger.info("ğŸš€ MuseTalké¢„çƒ­æœåŠ¡")
        logger.info("ä½¿ç”¨ --start å¯åŠ¨æœåŠ¡")
        logger.info("ä½¿ç”¨ --benchmark è¿è¡Œæ€§èƒ½æµ‹è¯•")
        logger.info("ä½¿ç”¨ --status æ£€æŸ¥æœåŠ¡çŠ¶æ€")

if __name__ == "__main__":
    main()
# MuseTalk å®Œæ•´éƒ¨ç½²æµç¨‹æŒ‡å—

## ğŸ“‹ éƒ¨ç½²æµç¨‹æ¦‚è§ˆ

æ‚¨éœ€è¦åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1. **æ‹‰å–ä»£ç ** â†’ 2. **æ„å»ºDockeré•œåƒ** â†’ 3. **è¿è¡Œå®¹å™¨** â†’ 4. **æµ‹è¯•éªŒè¯**

---

## ğŸš€ Step 1: åœ¨æœåŠ¡å™¨ä¸Šæ‹‰å–ä»£ç 

### æ–¹æ³•A: ç›´æ¥å…‹éš†å®Œæ•´é¡¹ç›®ï¼ˆæ¨èï¼‰

```bash
# SSHç™»å½•åˆ°æ‚¨çš„æœåŠ¡å™¨
ssh user@your-server

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p /home/user/musetalk && cd /home/user/musetalk

# å…‹éš†æ‚¨çš„é¡¹ç›®ä»£ç ï¼ˆåŒ…å«æ‰€æœ‰é…ç½®æ–‡ä»¶ï¼‰
git clone <your-repo-url> .

# èµ‹äºˆè„šæœ¬æ‰§è¡Œæƒé™
chmod +x deploy.sh setup_gpu_env.sh
```

### æ–¹æ³•B: åªå¤åˆ¶å¿…è¦æ–‡ä»¶

å¦‚æœæ‚¨åªæƒ³å¤åˆ¶å¿…è¦çš„æ–‡ä»¶åˆ°æœåŠ¡å™¨ï¼š

```bash
# åœ¨æœ¬åœ°æ‰“åŒ…å¿…è¦æ–‡ä»¶
tar czf musetalk-docker.tar.gz \
    Dockerfile.gpu \
    docker-compose.gpu.yml \
    deploy.sh \
    benchmark_gpu.py \
    MuseTalkEngine/ \
    .env.example \
    requirements.txt

# ä¸Šä¼ åˆ°æœåŠ¡å™¨
scp musetalk-docker.tar.gz user@server:/home/user/

# åœ¨æœåŠ¡å™¨ä¸Šè§£å‹
ssh user@server
tar xzf musetalk-docker.tar.gz
cd musetalk
```

---

## ğŸ³ Step 2: æ„å»ºDockeré•œåƒ

### æ£€æŸ¥CUDAå…¼å®¹æ€§

```bash
# æ£€æŸ¥æœåŠ¡å™¨CUDAç‰ˆæœ¬
nvidia-smi

# è¾“å‡ºç¤ºä¾‹ï¼š
# CUDA Version: 12.9  â† æ‚¨çš„CUDAç‰ˆæœ¬
```

æˆ‘ä»¬çš„Dockeré•œåƒä½¿ç”¨CUDA 12.1ï¼Œ**å…¼å®¹CUDA 12.9**ã€‚

### æ„å»ºé•œåƒ

```bash
# æ–¹æ³•1: ä½¿ç”¨deployè„šæœ¬ï¼ˆæ¨èï¼‰
./deploy.sh setup    # åˆå§‹åŒ–ç¯å¢ƒ
./deploy.sh build    # æ„å»ºé•œåƒ

# æ–¹æ³•2: ç›´æ¥ä½¿ç”¨dockerå‘½ä»¤
docker build -f Dockerfile.gpu -t musetalk:gpu-latest .
```

æ„å»ºè¿‡ç¨‹ä¼šï¼š
- âœ… å®‰è£…Python 3.10ç¯å¢ƒ
- âœ… å®‰è£…PyTorch 2.2.0 (CUDA 12.1)
- âœ… å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…
- âœ… é…ç½®GPUæ”¯æŒ

---

## ğŸƒ Step 3: è¿è¡Œå®¹å™¨

### é…ç½®ç¯å¢ƒå˜é‡

```bash
# å¤åˆ¶å¹¶ç¼–è¾‘é…ç½®æ–‡ä»¶
cp .env.example .env
vim .env

# å…³é”®é…ç½®ï¼š
CUDA_VISIBLE_DEVICES=0,1,2,3  # æ ¹æ®å®é™…GPUè°ƒæ•´
BATCH_SIZE_PER_GPU=8          # æ ¹æ®GPUå†…å­˜è°ƒæ•´
RUN_MODE=shell                 # é¦–æ¬¡è¿è¡Œå»ºè®®ç”¨shellæ¨¡å¼
AUTO_DOWNLOAD_MODELS=true      # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
```

### å¯åŠ¨å®¹å™¨

#### é€‰é¡¹1: äº¤äº’å¼Shellæ¨¡å¼ï¼ˆæ¨èé¦–æ¬¡ä½¿ç”¨ï¼‰

```bash
# å¯åŠ¨äº¤äº’å¼å®¹å™¨
docker run -it --rm \
    --runtime=nvidia \
    --gpus all \
    -v $(pwd):/workspace \
    -e RUN_MODE=shell \
    musetalk:gpu-latest

# åœ¨å®¹å™¨å†…ï¼š
# 1. å…‹éš†MuseTalkä»£ç 
git clone https://github.com/TMElyralab/MuseTalk.git

# 2. ä¸‹è½½æ¨¡å‹
python /app/download_models.py

# 3. æµ‹è¯•GPU
python -c "import torch; print(f'GPUs: {torch.cuda.device_count()}')"

# 4. è¿è¡Œæµ‹è¯•
python /workspace/benchmark_gpu.py --config auto
```

#### é€‰é¡¹2: ä½¿ç”¨Docker Composeï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.gpu.yml up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.gpu.yml logs -f

# è¿›å…¥å®¹å™¨
docker exec -it musetalk-gpu-service bash
```

#### é€‰é¡¹3: ç›´æ¥è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
docker run --rm \
    --runtime=nvidia \
    --gpus all \
    -v $(pwd):/workspace \
    -e RUN_MODE=benchmark \
    -e GPU_CONFIG=auto \
    musetalk:gpu-latest
```

---

## ğŸ§ª Step 4: æµ‹è¯•éªŒè¯

### 1. éªŒè¯GPUè®¿é—®

```bash
# æµ‹è¯•Docker GPUæ”¯æŒ
docker run --rm --runtime=nvidia --gpus all \
    nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# åœ¨å®¹å™¨å†…æµ‹è¯•PyTorch GPU
docker run --rm --runtime=nvidia --gpus all \
    musetalk:gpu-latest \
    python -c "
import torch
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'CUDA Version: {torch.version.cuda}')
print(f'GPU Count: {torch.cuda.device_count()}')
for i in range(torch.cuda.device_count()):
    print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
```

### 2. è¿è¡ŒåŸºå‡†æµ‹è¯•

```bash
# å•GPUæµ‹è¯•ï¼ˆRTX 4090D 48GBï¼‰
docker run --rm --runtime=nvidia --gpus '"device=0"' \
    -v $(pwd):/workspace \
    -e CUDA_VISIBLE_DEVICES=0 \
    -e BATCH_SIZE_PER_GPU=20 \
    musetalk:gpu-latest \
    python /workspace/benchmark_gpu.py --config single_4090d_48gb

# å¤šGPUæµ‹è¯•ï¼ˆ4x RTX 4090 24GBï¼‰
docker run --rm --runtime=nvidia --gpus all \
    -v $(pwd):/workspace \
    -e CUDA_VISIBLE_DEVICES=0,1,2,3 \
    -e BATCH_SIZE_PER_GPU=8 \
    musetalk:gpu-latest \
    python /workspace/benchmark_gpu.py --config quad_4090_24gb
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„è¯´æ˜

```
æœåŠ¡å™¨ç›®å½•ç»“æ„ï¼š
/home/user/musetalk/
â”œâ”€â”€ Dockerfile.gpu          # Dockeré•œåƒå®šä¹‰
â”œâ”€â”€ docker-compose.gpu.yml  # æœåŠ¡ç¼–æ’
â”œâ”€â”€ deploy.sh              # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ .env                   # ç¯å¢ƒé…ç½®
â”œâ”€â”€ benchmark_gpu.py       # æ€§èƒ½æµ‹è¯•
â”œâ”€â”€ MuseTalkEngine/        # æ‚¨çš„å¼•æ“ä»£ç 
â”‚   â”œâ”€â”€ multi_gpu_parallel_engine.py
â”‚   â””â”€â”€ musetalk_realtime_engine.py
â”œâ”€â”€ MuseTalk/              # å®˜æ–¹MuseTalkï¼ˆå®¹å™¨å†…å…‹éš†ï¼‰
â”œâ”€â”€ models/                # æ¨¡å‹æ–‡ä»¶ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
â”œâ”€â”€ inputs/                # è¾“å…¥æ–‡ä»¶
â”œâ”€â”€ outputs/               # è¾“å‡ºç»“æœ
â””â”€â”€ logs/                  # æ—¥å¿—æ–‡ä»¶
```

---

## ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

### é—®é¢˜1: CUDAç‰ˆæœ¬ä¸åŒ¹é…

```bash
# é”™è¯¯: CUDA version mismatch
# è§£å†³: æˆ‘ä»¬çš„é•œåƒä½¿ç”¨CUDA 12.1ï¼Œå‘å‰å…¼å®¹12.x

# å¦‚æœéœ€è¦å…¶ä»–CUDAç‰ˆæœ¬ï¼Œä¿®æ”¹Dockerfileï¼š
FROM nvidia/cuda:12.9.0-cudnn8-devel-ubuntu22.04  # ä½¿ç”¨12.9
```

### é—®é¢˜2: æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# åœ¨å®¹å™¨å†…æ‰‹åŠ¨ä¸‹è½½
docker exec -it musetalk-gpu-service bash

# ä½¿ç”¨ä¸­å›½é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
python /app/download_models.py

# æˆ–æ‰‹åŠ¨ä¸‹è½½åˆ°modelsç›®å½•
cd /app/models
wget https://huggingface.co/TMElyralab/MuseTalk/...
```

### é—®é¢˜3: GPUä¸å¯è§

```bash
# æ£€æŸ¥NVIDIAè¿è¡Œæ—¶
docker info | grep nvidia

# å¦‚æœæ²¡æœ‰ï¼Œå®‰è£…nvidia-container-toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### é—®é¢˜4: å†…å­˜ä¸è¶³

```bash
# è°ƒæ•´æ‰¹å¤„ç†å¤§å°
echo "BATCH_SIZE_PER_GPU=4" >> .env

# é‡å¯æœåŠ¡
docker-compose -f docker-compose.gpu.yml restart
```

---

## ğŸš¢ å¤šæœåŠ¡å™¨éƒ¨ç½²

### ä¿å­˜é•œåƒ

```bash
# åœ¨ç¬¬ä¸€å°æœåŠ¡å™¨æ„å»ºåä¿å­˜
docker save musetalk:gpu-latest | gzip > musetalk-gpu-cuda12.tar.gz

# å¤§å°çº¦8-10GB
ls -lh musetalk-gpu-cuda12.tar.gz
```

### ä¼ è¾“åˆ°å…¶ä»–æœåŠ¡å™¨

```bash
# æ–¹æ³•1: SCPä¼ è¾“
scp musetalk-gpu-cuda12.tar.gz user@server2:/home/user/

# æ–¹æ³•2: ä½¿ç”¨rsyncï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
rsync -avP musetalk-gpu-cuda12.tar.gz user@server2:/home/user/

# æ–¹æ³•3: ä½¿ç”¨ç§æœ‰Registry
docker tag musetalk:gpu-latest your-registry.com/musetalk:gpu-latest
docker push your-registry.com/musetalk:gpu-latest
```

### åœ¨æ–°æœåŠ¡å™¨åŠ è½½

```bash
# åŠ è½½é•œåƒ
docker load < musetalk-gpu-cuda12.tar.gz

# éªŒè¯
docker images | grep musetalk

# è¿è¡Œ
docker run --rm --runtime=nvidia --gpus all \
    musetalk:gpu-latest \
    python -c "import torch; print(torch.cuda.device_count())"
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### GPUå†…å­˜ä¼˜åŒ–

```bash
# RTX 4090D 48GB
BATCH_SIZE_PER_GPU=20
GPU_MEMORY_FRACTION=0.95

# RTX 4090 24GB
BATCH_SIZE_PER_GPU=8
GPU_MEMORY_FRACTION=0.9

# RTX 3090 24GB
BATCH_SIZE_PER_GPU=6
GPU_MEMORY_FRACTION=0.85
```

### å¹¶è¡Œç­–ç•¥é€‰æ‹©

```bash
# æ•°æ®å¹¶è¡Œï¼ˆæ¨èï¼‰
INFERENCE_MODE=parallel
PARALLEL_STRATEGY=data_parallel

# è´Ÿè½½å‡è¡¡
PARALLEL_STRATEGY=load_balance

# è½®è¯¢åˆ†é…
PARALLEL_STRATEGY=round_robin
```

---

## âœ… éªŒè¯æ¸…å•

- [ ] Dockerå·²å®‰è£…å¹¶è¿è¡Œ
- [ ] NVIDIA Container Toolkitå·²å®‰è£…
- [ ] GPUåœ¨Dockerä¸­å¯è§
- [ ] PyTorchæ£€æµ‹åˆ°æ‰€æœ‰GPU
- [ ] æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½
- [ ] åŸºå‡†æµ‹è¯•è¿è¡ŒæˆåŠŸ
- [ ] æ—¥å¿—æ— é”™è¯¯

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æä¾›ï¼š

1. **ç³»ç»Ÿä¿¡æ¯**
```bash
nvidia-smi
docker --version
docker info | grep -i runtime
```

2. **é”™è¯¯æ—¥å¿—**
```bash
docker logs musetalk-gpu-service --tail 50
```

3. **é…ç½®æ–‡ä»¶**
```bash
cat .env
```

---

## ğŸ¯ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# æ„å»ºé•œåƒ
docker build -f Dockerfile.gpu -t musetalk:gpu-latest .

# è¿è¡ŒShell
docker run -it --rm --runtime=nvidia --gpus all musetalk:gpu-latest

# è¿è¡Œæµ‹è¯•
docker run --rm --runtime=nvidia --gpus all \
    -v $(pwd):/workspace \
    musetalk:gpu-latest \
    python /workspace/benchmark_gpu.py

# æŸ¥çœ‹GPUä½¿ç”¨
docker exec musetalk-gpu-service nvidia-smi

# æ¸…ç†èµ„æº
docker system prune -a
```
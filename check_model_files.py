#!/usr/bin/env python3
"""
æ£€æŸ¥MuseTalkæ¨¡å‹æ–‡ä»¶çŠ¶æ€
"""

import os
import sys
from pathlib import Path

def check_model_files():
    """æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„æ¨¡å‹æ–‡ä»¶"""
    print("ğŸ” æ£€æŸ¥MuseTalkæ¨¡å‹æ–‡ä»¶...")
    
    # ç¡®ä¿åœ¨MuseTalkç›®å½•ä¸­
    if 'MuseTalk' not in os.getcwd():
        print("âŒ è¯·åœ¨MuseTalkç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬")
        return False
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_files = {
        'UNetæ¨¡å‹': [
            'models/musetalk/pytorch_model.bin',
            'models/musetalk/musetalk.json'
        ],
        'VAEæ¨¡å‹': [
            'models/sd-vae/config.json',
            'models/sd-vae/diffusion_pytorch_model.bin',
            'models/sd-vae/diffusion_pytorch_model.safetensors'  # å¯é€‰
        ],
        'DWPoseæ¨¡å‹': [
            'models/dwpose/dw-ll_ucoco_384.pth'
        ],
        'Whisperæ¨¡å‹': [
            'models/whisper/config.json',
            'models/whisper/preprocessor_config.json'
        ]
    }
    
    all_good = True
    
    for category, files in model_files.items():
        print(f"\nğŸ“ {category}:")
        for file_path in files:
            if os.path.exists(file_path):
                size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                print(f"  âœ… {file_path} ({size:.1f}MB)")
            else:
                if 'safetensors' in file_path:
                    print(f"  âš ï¸  {file_path} (å¯é€‰ï¼Œæœ‰.binç‰ˆæœ¬å³å¯)")
                else:
                    print(f"  âŒ {file_path}")
                    all_good = False
    
    # æ£€æŸ¥VAEç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
    print(f"\nğŸ“‚ VAEç›®å½•æ‰€æœ‰æ–‡ä»¶:")
    vae_dir = Path('models/sd-vae')
    if vae_dir.exists():
        for file in vae_dir.iterdir():
            if file.is_file():
                size = file.stat().st_size / (1024 * 1024)
                print(f"  ğŸ“„ {file.name} ({size:.1f}MB)")
    else:
        print("  âŒ VAEç›®å½•ä¸å­˜åœ¨")
        all_good = False
    
    return all_good

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        # è®¾ç½®ç¯å¢ƒ
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        
        # å¯¼å…¥å¿…è¦æ¨¡å—
        import torch
        from musetalk.utils.utils import load_all_model
        
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(f"âœ… è®¾å¤‡: {device}")
        
        # å°è¯•åŠ è½½æ¨¡å‹ï¼ˆè¿™å¯èƒ½ä¼šå¾ˆæ…¢ï¼‰
        print("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹ï¼ˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰...")
        
        vae, unet, pe = load_all_model(
            unet_model_path="models/musetalk/pytorch_model.bin",
            vae_type="sd-vae",
            unet_config="models/musetalk/musetalk.json",
            device=device
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"  VAE: {type(vae)}")
        print(f"  UNet: {type(unet)}")
        print(f"  PE: {type(pe)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("ğŸš€ MuseTalkæ¨¡å‹æ–‡ä»¶æ£€æŸ¥")
    print("=" * 60)
    
    # 1. æ£€æŸ¥æ–‡ä»¶
    files_ok = check_model_files()
    
    if not files_ok:
        print("\nâŒ éƒ¨åˆ†æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·å…ˆè§£å†³æ–‡ä»¶é—®é¢˜")
        return
    
    print("\nâœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨")
    
    # 2. è¯¢é—®æ˜¯å¦æµ‹è¯•åŠ è½½
    try:
        test_load = input("\næ˜¯å¦æµ‹è¯•æ¨¡å‹åŠ è½½ï¼Ÿ(y/N): ").strip().lower()
        if test_load in ['y', 'yes']:
            test_model_loading()
    except KeyboardInterrupt:
        print("\nç”¨æˆ·å–æ¶ˆ")

if __name__ == "__main__":
    main()
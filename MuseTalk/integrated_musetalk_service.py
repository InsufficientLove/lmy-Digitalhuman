#!/usr/bin/env python3
"""
Integrated MuseTalk Service
é›†æˆçš„MuseTalkæœåŠ¡

æä¾›ä¸¤ä¸ªä¸»è¦åŠŸèƒ½ï¼š
1. æ¨¡æ¿é¢„å¤„ç†æœåŠ¡ - åˆ›å»ºå’Œç¼“å­˜é¢éƒ¨ç‰¹å¾
2. è¶…é«˜é€Ÿå®æ—¶æ¨ç†æœåŠ¡ - åŸºäºç¼“å­˜çš„æé€Ÿæ¨ç†

ä¸C#æœåŠ¡å®Œå…¨å…¼å®¹ï¼Œæ”¯æŒåŸæœ‰çš„APIæ¥å£

ä½œè€…: Claude Sonnet
ç‰ˆæœ¬: 2.0
"""

import os
import sys
import argparse
import json
import time
from pathlib import Path

# å¯¼å…¥æ–°çš„ç»„ä»¶
from enhanced_musetalk_preprocessing import EnhancedMuseTalkPreprocessor
from ultra_fast_realtime_inference import UltraFastRealtimeInference


class IntegratedMuseTalkService:
    """
    é›†æˆçš„MuseTalkæœåŠ¡
    
    ç»“åˆé¢„å¤„ç†å’Œå®æ—¶æ¨ç†åŠŸèƒ½ï¼Œæä¾›å®Œæ•´çš„æ•°å­—äººè§†é¢‘ç”ŸæˆæœåŠ¡
    """
    
    def __init__(self, 
                 model_config_path="models/musetalk/musetalk.json",
                 model_weights_path="models/musetalk/pytorch_model.bin",
                 vae_type="sd-vae",
                 whisper_dir="models/whisper",
                 device="cuda:0",
                 cache_dir="./template_cache",
                 batch_size=32,
                 fp16=True):
        """
        åˆå§‹åŒ–é›†æˆæœåŠ¡
        
        Args:
            model_config_path: UNeté…ç½®è·¯å¾„
            model_weights_path: UNetæƒé‡è·¯å¾„
            vae_type: VAEç±»å‹
            whisper_dir: Whisperæ¨¡å‹ç›®å½•
            device: è®¡ç®—è®¾å¤‡
            cache_dir: ç¼“å­˜ç›®å½•
            batch_size: æ‰¹å¤„ç†å¤§å°
            fp16: æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦
        """
        self.device = device
        self.cache_dir = Path(cache_dir)
        self.batch_size = batch_size
        
        print(f"ğŸš€ åˆå§‹åŒ–é›†æˆMuseTalkæœåŠ¡...")
        print(f"ğŸ“± è®¾å¤‡: {device}")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # åˆå§‹åŒ–é¢„å¤„ç†å™¨
        self.preprocessor = EnhancedMuseTalkPreprocessor(
            model_config_path=model_config_path,
            model_weights_path=model_weights_path,
            vae_type=vae_type,
            device=device,
            cache_dir=cache_dir
        )
        
        # åˆå§‹åŒ–å®æ—¶æ¨ç†ç³»ç»Ÿ
        self.inference_system = UltraFastRealtimeInference(
            model_config_path=model_config_path,
            model_weights_path=model_weights_path,
            vae_type=vae_type,
            whisper_dir=whisper_dir,
            device=device,
            cache_dir=cache_dir,
            batch_size=batch_size,
            fp16=fp16
        )
        
        print(f"âœ… é›†æˆæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def preprocess_template(self, 
                          template_id, 
                          template_image_path, 
                          bbox_shift=0,
                          parsing_mode="jaw",
                          force_refresh=False):
        """
        é¢„å¤„ç†æ¨¡æ¿æœåŠ¡
        
        Args:
            template_id: æ¨¡æ¿å”¯ä¸€ID
            template_image_path: æ¨¡æ¿å›¾ç‰‡è·¯å¾„
            bbox_shift: è¾¹ç•Œæ¡†åç§»
            parsing_mode: é¢éƒ¨è§£ææ¨¡å¼
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            dict: é¢„å¤„ç†ç»“æœ
        """
        print(f"ğŸ¯ æ¨¡æ¿é¢„å¤„ç†æœåŠ¡")
        print(f"ğŸ“‹ æ¨¡æ¿ID: {template_id}")
        print(f"ğŸ“¸ å›¾ç‰‡è·¯å¾„: {template_image_path}")
        
        try:
            # æ‰§è¡Œé¢„å¤„ç†
            metadata = self.preprocessor.preprocess_template(
                template_id=template_id,
                template_image_path=template_image_path,
                bbox_shift=bbox_shift,
                parsing_mode=parsing_mode,
                force_refresh=force_refresh
            )
            
            result = {
                'success': True,
                'template_id': template_id,
                'metadata': metadata,
                'message': f'æ¨¡æ¿ {template_id} é¢„å¤„ç†å®Œæˆ'
            }
            
            print(f"âœ… é¢„å¤„ç†æˆåŠŸ: {template_id}")
            return result
            
        except Exception as e:
            error_msg = f"é¢„å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            
            result = {
                'success': False,
                'template_id': template_id,
                'error': error_msg,
                'message': f'æ¨¡æ¿ {template_id} é¢„å¤„ç†å¤±è´¥'
            }
            return result
    
    def realtime_inference(self, 
                         template_id, 
                         audio_path, 
                         output_path,
                         fps=25):
        """
        å®æ—¶æ¨ç†æœåŠ¡
        
        Args:
            template_id: æ¨¡æ¿ID
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: å¸§ç‡
            
        Returns:
            dict: æ¨ç†ç»“æœ
        """
        print(f"âš¡ å®æ—¶æ¨ç†æœåŠ¡")
        print(f"ğŸ­ æ¨¡æ¿ID: {template_id}")
        print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        print(f"ğŸ“¹ è¾“å‡ºè·¯å¾„: {output_path}")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œè¶…é«˜é€Ÿæ¨ç†
            result_path = self.inference_system.ultra_fast_inference(
                template_id=template_id,
                audio_path=audio_path,
                output_path=output_path,
                fps=fps,
                save_frames=True
            )
            
            total_time = time.time() - start_time
            
            result = {
                'success': True,
                'template_id': template_id,
                'audio_path': audio_path,
                'output_path': result_path,
                'processing_time': total_time,
                'fps': fps,
                'message': f'æ¨ç†å®Œæˆï¼Œè€—æ—¶ {total_time:.2f}ç§’'
            }
            
            print(f"âœ… æ¨ç†æˆåŠŸ: {result_path}")
            return result
            
        except Exception as e:
            error_msg = f"æ¨ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            
            result = {
                'success': False,
                'template_id': template_id,
                'audio_path': audio_path,
                'error': error_msg,
                'processing_time': time.time() - start_time,
                'message': f'æ¨ç†å¤±è´¥'
            }
            return result
    
    def check_template_cache(self, template_id):
        """
        æ£€æŸ¥æ¨¡æ¿ç¼“å­˜çŠ¶æ€
        
        Args:
            template_id: æ¨¡æ¿ID
            
        Returns:
            dict: ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        """
        cache_path = self.cache_dir / f"{template_id}_preprocessed.pkl"
        metadata_path = self.cache_dir / f"{template_id}_metadata.json"
        
        exists = cache_path.exists() and metadata_path.exists()
        
        if exists:
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                return {
                    'exists': True,
                    'template_id': template_id,
                    'metadata': metadata,
                    'cache_size_mb': cache_path.stat().st_size / 1024 / 1024,
                    'message': f'ç¼“å­˜å­˜åœ¨: {template_id}'
                }
            except Exception as e:
                return {
                    'exists': False,
                    'template_id': template_id,
                    'error': str(e),
                    'message': f'ç¼“å­˜æŸå: {template_id}'
                }
        else:
            return {
                'exists': False,
                'template_id': template_id,
                'message': f'ç¼“å­˜ä¸å­˜åœ¨: {template_id}'
            }
    
    def get_service_info(self):
        """è·å–æœåŠ¡ä¿¡æ¯"""
        cache_info = self.preprocessor.get_cache_info()
        
        return {
            'service_name': 'IntegratedMuseTalkService',
            'version': '2.0',
            'device': str(self.device),
            'cache_dir': str(self.cache_dir),
            'batch_size': self.batch_size,
            'cache_info': cache_info,
            'capabilities': [
                'template_preprocessing',
                'realtime_inference', 
                'cache_management',
                'performance_optimization'
            ]
        }


def main():
    """å‘½ä»¤è¡Œç•Œé¢"""
    parser = argparse.ArgumentParser(description="é›†æˆMuseTalkæœåŠ¡")
    
    # æœåŠ¡ç±»å‹
    service_group = parser.add_mutually_exclusive_group(required=True)
    service_group.add_argument("--preprocess", action="store_true", help="é¢„å¤„ç†æ¨¡å¼")
    service_group.add_argument("--inference", action="store_true", help="æ¨ç†æ¨¡å¼")
    service_group.add_argument("--check_cache", action="store_true", help="æ£€æŸ¥ç¼“å­˜æ¨¡å¼")
    service_group.add_argument("--service_info", action="store_true", help="æœåŠ¡ä¿¡æ¯æ¨¡å¼")
    
    # å…±åŒå‚æ•°
    parser.add_argument("--template_id", help="æ¨¡æ¿ID")
    parser.add_argument("--device", default="cuda:0", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--cache_dir", default="./template_cache", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--fp16", action="store_true", default=True, help="ä½¿ç”¨åŠç²¾åº¦")
    
    # é¢„å¤„ç†å‚æ•°
    parser.add_argument("--template_image_path", help="æ¨¡æ¿å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--bbox_shift", type=int, default=0, help="è¾¹ç•Œæ¡†åç§»")
    parser.add_argument("--parsing_mode", default="jaw", help="é¢éƒ¨è§£ææ¨¡å¼")
    parser.add_argument("--force_refresh", action="store_true", help="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜")
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--audio_path", help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_path", help="è¾“å‡ºè§†é¢‘è·¯å¾„")
    parser.add_argument("--fps", type=int, default=25, help="è§†é¢‘å¸§ç‡")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--unet_config", default="models/musetalk/musetalk.json", help="UNeté…ç½®è·¯å¾„")
    parser.add_argument("--unet_model_path", default="models/musetalk/pytorch_model.bin", help="UNetæƒé‡è·¯å¾„")
    parser.add_argument("--vae_type", default="sd-vae", help="VAEç±»å‹")
    parser.add_argument("--whisper_dir", default="models/whisper", help="Whisperæ¨¡å‹ç›®å½•")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æœåŠ¡
    service = IntegratedMuseTalkService(
        model_config_path=args.unet_config,
        model_weights_path=args.unet_model_path,
        vae_type=args.vae_type,
        whisper_dir=args.whisper_dir,
        device=args.device,
        cache_dir=args.cache_dir,
        batch_size=args.batch_size,
        fp16=args.fp16
    )
    
    # æ‰§è¡Œå¯¹åº”æœåŠ¡
    if args.preprocess:
        if not args.template_id or not args.template_image_path:
            print("âŒ é¢„å¤„ç†æ¨¡å¼éœ€è¦æŒ‡å®š --template_id å’Œ --template_image_path")
            sys.exit(1)
        
        result = service.preprocess_template(
            template_id=args.template_id,
            template_image_path=args.template_image_path,
            bbox_shift=args.bbox_shift,
            parsing_mode=args.parsing_mode,
            force_refresh=args.force_refresh
        )
        
    elif args.inference:
        if not args.template_id or not args.audio_path or not args.output_path:
            print("âŒ æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š --template_id, --audio_path å’Œ --output_path")
            sys.exit(1)
        
        result = service.realtime_inference(
            template_id=args.template_id,
            audio_path=args.audio_path,
            output_path=args.output_path,
            fps=args.fps
        )
        
    elif args.check_cache:
        if not args.template_id:
            print("âŒ æ£€æŸ¥ç¼“å­˜æ¨¡å¼éœ€è¦æŒ‡å®š --template_id")
            sys.exit(1)
        
        result = service.check_template_cache(args.template_id)
        
    elif args.service_info:
        result = service.get_service_info()
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š æœåŠ¡æ‰§è¡Œç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    # é€€å‡ºçŠ¶æ€ç 
    if 'success' in result:
        sys.exit(0 if result['success'] else 1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()
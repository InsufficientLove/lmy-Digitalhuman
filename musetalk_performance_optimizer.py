#!/usr/bin/env python3
"""
MuseTalkæ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§è„šæœ¬
åŸºäºå®˜æ–¹MuseTalkä»“åº“çš„æœ€ä½³å®è·µ
"""

import os
import sys
import time
import psutil
import subprocess
import torch
import GPUtil
from pathlib import Path
import json
import argparse

def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("ğŸ” æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… CUDAå¯ç”¨ï¼Œæ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    # æ£€æŸ¥å†…å­˜
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜: {memory.total / 1024**3:.1f}GB (å¯ç”¨: {memory.available / 1024**3:.1f}GB)")
    
    # æ£€æŸ¥CPU
    cpu_count = psutil.cpu_count()
    print(f"ğŸ–¥ï¸ CPUæ ¸å¿ƒæ•°: {cpu_count}")
    
    return True

def optimize_environment():
    """è®¾ç½®ä¼˜åŒ–çš„ç¯å¢ƒå˜é‡"""
    print("âš¡ è®¾ç½®æ€§èƒ½ä¼˜åŒ–ç¯å¢ƒå˜é‡...")
    
    # MuseTalkå®˜æ–¹æ¨èçš„ç¯å¢ƒå˜é‡
    env_vars = {
        "CUDA_VISIBLE_DEVICES": "0,1,2,3",  # ä½¿ç”¨æ‰€æœ‰GPU
        "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:1024",  # å¢åŠ å†…å­˜åˆ†é…
        "OMP_NUM_THREADS": "16",  # CPUçº¿ç¨‹æ•°
        "CUDA_LAUNCH_BLOCKING": "0",  # å¼‚æ­¥æ‰§è¡Œ
        "TORCH_CUDNN_V8_API_ENABLED": "1",  # å¯ç”¨cuDNN v8
        "TORCH_BACKENDS_CUDNN_BENCHMARK": "1",  # å¯ç”¨cuDNNè‡ªåŠ¨è°ƒä¼˜
        "TOKENIZERS_PARALLELISM": "false",  # é¿å…tokenizerè­¦å‘Š
        "CUBLAS_WORKSPACE_CONFIG": ":16:8",  # CUBLASå·¥ä½œç©ºé—´
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"   {key} = {value}")

def check_musetalk_models():
    """æ£€æŸ¥MuseTalkæ¨¡å‹æ–‡ä»¶"""
    print("ğŸ“¦ æ£€æŸ¥MuseTalkæ¨¡å‹æ–‡ä»¶...")
    
    model_paths = {
        "UNeté…ç½®": "models/musetalk/musetalk.json",
        "UNetæ¨¡å‹": "models/musetalk/pytorch_model.bin",
        "Whisperæ¨¡å‹": "models/whisper",
        "DWPoseæ¨¡å‹": "models/dwpose/dw-ll_ucoco_384.pth",
        "VAEæ¨¡å‹": "models/sd-vae",
    }
    
    all_exists = True
    for name, path in model_paths.items():
        if os.path.exists(path):
            if os.path.isfile(path):
                size = os.path.getsize(path) / 1024**2  # MB
                print(f"   âœ… {name}: {path} ({size:.1f}MB)")
            else:
                print(f"   âœ… {name}: {path} (ç›®å½•)")
        else:
            print(f"   âŒ {name}: {path} (ç¼ºå¤±)")
            all_exists = False
    
    return all_exists

def monitor_inference_process(pid, duration=300):
    """ç›‘æ§æ¨ç†è¿›ç¨‹æ€§èƒ½"""
    print(f"ğŸ“Š ç›‘æ§è¿›ç¨‹ PID={pid}ï¼ŒæŒç»­{duration}ç§’...")
    
    try:
        process = psutil.Process(pid)
        start_time = time.time()
        
        while time.time() - start_time < duration:
            if not process.is_running():
                print("âš ï¸ è¿›ç¨‹å·²ç»“æŸ")
                break
                
            # CPUå’Œå†…å­˜ä½¿ç”¨
            cpu_percent = process.cpu_percent()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024**2
            
            # GPUä½¿ç”¨æƒ…å†µ
            gpus = GPUtil.getGPUs()
            gpu_usage = []
            for gpu in gpus:
                gpu_usage.append(f"GPU{gpu.id}: {gpu.load*100:.1f}%")
            
            print(f"   PID={pid} | CPU: {cpu_percent:.1f}% | å†…å­˜: {memory_mb:.1f}MB | {' | '.join(gpu_usage)}")
            
            time.sleep(10)  # æ¯10ç§’ç›‘æ§ä¸€æ¬¡
            
    except psutil.NoSuchProcess:
        print("âŒ è¿›ç¨‹ä¸å­˜åœ¨")
    except Exception as e:
        print(f"âŒ ç›‘æ§é”™è¯¯: {e}")

def benchmark_musetalk():
    """MuseTalkæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸƒ æ‰§è¡ŒMuseTalkæ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = {
        "task_001": {
            "video_path": "data/test_image.jpg",  # éœ€è¦å‡†å¤‡æµ‹è¯•å›¾ç‰‡
            "audio_path": "data/test_audio.wav",  # éœ€è¦å‡†å¤‡æµ‹è¯•éŸ³é¢‘
            "bbox_shift": 0,
            "result_name": "benchmark_output.mp4"
        }
    }
    
    config_path = "configs/inference/benchmark.yaml"
    os.makedirs(os.path.dirname(config_path), exist_ok=True)
    
    with open(config_path, 'w') as f:
        import yaml
        yaml.dump(test_config, f)
    
    # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
    configs = [
        {"batch_size": 1, "use_float16": True, "description": "ä½å†…å­˜æ¨¡å¼"},
        {"batch_size": 4, "use_float16": True, "description": "å¹³è¡¡æ¨¡å¼"},
        {"batch_size": 8, "use_float16": True, "description": "é«˜æ€§èƒ½æ¨¡å¼"},
    ]
    
    results = []
    
    for config in configs:
        print(f"\nğŸ§ª æµ‹è¯•é…ç½®: {config['description']}")
        
        cmd = [
            sys.executable, "-m", "scripts.inference",
            "--inference_config", config_path,
            "--result_dir", "benchmark_results",
            "--batch_size", str(config["batch_size"]),
            "--fps", "25",
            "--version", "v1"
        ]
        
        if config["use_float16"]:
            cmd.append("--use_float16")
        
        start_time = time.time()
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            end_time = time.time()
            
            duration = end_time - start_time
            success = result.returncode == 0
            
            results.append({
                "config": config["description"],
                "duration": duration,
                "success": success,
                "batch_size": config["batch_size"],
                "use_float16": config["use_float16"]
            })
            
            print(f"   ç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'} | è€—æ—¶: {duration:.1f}ç§’")
            if not success:
                print(f"   é”™è¯¯: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            print("   ç»“æœ: è¶…æ—¶ (>10åˆ†é’Ÿ)")
            results.append({
                "config": config["description"],
                "duration": 600,
                "success": False,
                "batch_size": config["batch_size"],
                "use_float16": config["use_float16"],
                "error": "timeout"
            })
    
    # è¾“å‡ºåŸºå‡†æµ‹è¯•ç»“æœ
    print("\nğŸ“ˆ åŸºå‡†æµ‹è¯•ç»“æœ:")
    print("=" * 60)
    for result in results:
        status = "âœ…" if result["success"] else "âŒ"
        print(f"{status} {result['config']}: {result['duration']:.1f}ç§’")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    with open("musetalk_benchmark_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    return results

def get_optimal_settings():
    """è·å–å½“å‰ç³»ç»Ÿçš„æœ€ä¼˜è®¾ç½®å»ºè®®"""
    print("ğŸ’¡ åˆ†ææœ€ä¼˜è®¾ç½®...")
    
    # æ£€æŸ¥GPUæ•°é‡å’Œå†…å­˜
    gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
    
    if gpu_count == 0:
        print("âŒ æœªæ£€æµ‹åˆ°CUDA GPUï¼Œæ— æ³•è¿è¡ŒMuseTalk")
        return None
    
    # è·å–GPUå†…å­˜
    gpu_memory = []
    for i in range(gpu_count):
        memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        gpu_memory.append(memory)
    
    min_gpu_memory = min(gpu_memory)
    total_gpu_memory = sum(gpu_memory)
    
    print(f"ğŸ¯ æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUï¼Œæœ€å°å†…å­˜: {min_gpu_memory:.1f}GBï¼Œæ€»å†…å­˜: {total_gpu_memory:.1f}GB")
    
    # æ¨èè®¾ç½®
    if min_gpu_memory >= 20:  # RTX 4090çº§åˆ«
        recommended = {
            "batch_size": 8,
            "use_float16": True,
            "cuda_devices": f"0,1,2,3" if gpu_count >= 4 else ",".join(map(str, range(gpu_count))),
            "omp_threads": 16,
            "memory_alloc": "max_split_size_mb:1024",
            "description": "é«˜æ€§èƒ½æ¨¡å¼ (RTX 4090çº§åˆ«)"
        }
    elif min_gpu_memory >= 10:  # RTX 3080çº§åˆ«
        recommended = {
            "batch_size": 4,
            "use_float16": True,
            "cuda_devices": f"0,1" if gpu_count >= 2 else "0",
            "omp_threads": 8,
            "memory_alloc": "max_split_size_mb:512",
            "description": "å¹³è¡¡æ¨¡å¼ (RTX 3080çº§åˆ«)"
        }
    else:  # è¾ƒä½ç«¯GPU
        recommended = {
            "batch_size": 1,
            "use_float16": True,
            "cuda_devices": "0",
            "omp_threads": 4,
            "memory_alloc": "max_split_size_mb:256",
            "description": "ä½å†…å­˜æ¨¡å¼"
        }
    
    print(f"ğŸ“‹ æ¨èé…ç½®: {recommended['description']}")
    print(f"   - batch_size: {recommended['batch_size']}")
    print(f"   - CUDA_VISIBLE_DEVICES: {recommended['cuda_devices']}")
    print(f"   - OMP_NUM_THREADS: {recommended['omp_threads']}")
    print(f"   - PYTORCH_CUDA_ALLOC_CONF: {recommended['memory_alloc']}")
    
    return recommended

def main():
    parser = argparse.ArgumentParser(description="MuseTalkæ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§å·¥å…·")
    parser.add_argument("--check", action="store_true", help="æ£€æŸ¥ç³»ç»Ÿè¦æ±‚å’Œæ¨¡å‹æ–‡ä»¶")
    parser.add_argument("--optimize", action="store_true", help="è®¾ç½®ä¼˜åŒ–ç¯å¢ƒå˜é‡")
    parser.add_argument("--monitor", type=int, help="ç›‘æ§æŒ‡å®šPIDçš„è¿›ç¨‹")
    parser.add_argument("--benchmark", action="store_true", help="æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--recommend", action="store_true", help="è·å–æœ€ä¼˜è®¾ç½®å»ºè®®")
    
    args = parser.parse_args()
    
    if args.check:
        check_system_requirements()
        check_musetalk_models()
    
    if args.optimize:
        optimize_environment()
    
    if args.monitor:
        monitor_inference_process(args.monitor)
    
    if args.benchmark:
        benchmark_musetalk()
    
    if args.recommend:
        get_optimal_settings()
    
    if not any(vars(args).values()):
        # é»˜è®¤æ‰§è¡Œå…¨é¢æ£€æŸ¥
        print("ğŸš€ MuseTalkæ€§èƒ½ä¼˜åŒ–å·¥å…·")
        print("=" * 50)
        
        if check_system_requirements():
            check_musetalk_models()
            get_optimal_settings()
            optimize_environment()
            
            print("\nâœ… ç³»ç»Ÿæ£€æŸ¥å®Œæˆï¼")
            print("ğŸ’¡ å»ºè®®:")
            print("   1. ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½å·²ä¸‹è½½")
            print("   2. æ ¹æ®æ¨èé…ç½®è°ƒæ•´C#ä»£ç ä¸­çš„å‚æ•°")
            print("   3. ä½¿ç”¨ --monitor <PID> ç›‘æ§è¿è¡Œä¸­çš„è¿›ç¨‹")
            print("   4. ä½¿ç”¨ --benchmark æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½")

if __name__ == "__main__":
    main()
# MuseTalk GPUç²¾ç¡®æŒ‡å®šéƒ¨ç½²æŒ‡å—

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

1. **ç²¾ç¡®GPUæŒ‡å®š** - å¯ä»¥æ‰‹åŠ¨æŒ‡å®šä½¿ç”¨å“ªäº›GPU
2. **è‡ªåŠ¨æ£€æµ‹ç©ºé—²GPU** - æ™ºèƒ½é€‰æ‹©æœªè¢«å ç”¨çš„GPU
3. **é¢„è®¾é…ç½®** - é’ˆå¯¹ä¸åŒGPUå‹å·çš„ä¼˜åŒ–é…ç½®
4. **æ‰¹å¤„ç†ä¼˜åŒ–** - åŸºäºMuseTalkå®˜æ–¹ä»£ç çš„æ‰¹å¤„ç†ç­–ç•¥
5. **é¢„åŠ è½½æœºåˆ¶** - æå‰åŠ è½½æ•°æ®åˆ°GPUå†…å­˜æå‡æ€§èƒ½

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### 1. æ£€æŸ¥GPUçŠ¶æ€

```bash
# æŸ¥çœ‹æ‰€æœ‰GPUçŠ¶æ€
nvidia-smi

# æŸ¥çœ‹GPUå ç”¨æƒ…å†µï¼ˆæ›´è¯¦ç»†ï¼‰
nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu --format=csv
```

### 2. å…‹éš†ä»£ç 

```bash
# å…‹éš†æŒ‡å®šåˆ†æ”¯
git clone -b cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5 \
    YOUR_REPO_URL musetalk-gpu
cd musetalk-gpu
```

### 3. æ„å»ºDockeré•œåƒ

```bash
# æ„å»ºæ”¯æŒCUDA 12.xçš„é•œåƒ
docker build -f Dockerfile.gpu -t musetalk:gpu-latest .
```

## ğŸš€ è¿è¡Œç¤ºä¾‹

### åœºæ™¯1: å•å¼ RTX 4090D 48GB

```bash
# æ–¹å¼1: ä½¿ç”¨é¢„è®¾é…ç½®
./run_musetalk_gpu.sh \
    --gpu-config single_4090d_48gb \
    --mode benchmark

# æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®šGPU 0
./run_musetalk_gpu.sh \
    --gpu-ids 0 \
    --batch-size 20 \
    --mode benchmark

# æ–¹å¼3: ä½¿ç”¨Docker Compose
docker-compose -f docker-compose.gpu-specific.yml \
    --profile single \
    up musetalk-single-4090d
```

### åœºæ™¯2: 4å¼ RTX 4090 24GB

```bash
# æ–¹å¼1: ä½¿ç”¨é¢„è®¾é…ç½®
./run_musetalk_gpu.sh \
    --gpu-config quad_4090_24gb \
    --mode benchmark

# æ–¹å¼2: æ‰‹åŠ¨æŒ‡å®šGPU 0,1,2,3
./run_musetalk_gpu.sh \
    --gpu-ids 0,1,2,3 \
    --batch-size 8 \
    --mode benchmark

# æ–¹å¼3: ä½¿ç”¨Docker Compose
docker-compose -f docker-compose.gpu-specific.yml \
    --profile quad \
    up musetalk-quad-4090
```

### åœºæ™¯3: éƒ¨åˆ†GPUè¢«å ç”¨ï¼ˆåªä½¿ç”¨GPU 2å’Œ3ï¼‰

```bash
# æ‰‹åŠ¨æŒ‡å®šç©ºé—²çš„GPU
./run_musetalk_gpu.sh \
    --gpu-ids 2,3 \
    --batch-size 10 \
    --mode inference \
    --video input.mp4 \
    --audio audio.wav

# æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡
export GPU_IDS=2,3
docker-compose -f docker-compose.gpu-specific.yml \
    --profile dual \
    up musetalk-dual-4090
```

### åœºæ™¯4: è‡ªåŠ¨é€‰æ‹©ç©ºé—²GPU

```bash
# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ç©ºé—²GPU
./run_musetalk_gpu.sh \
    --gpu-config auto \
    --mode benchmark

# Dockeræ–¹å¼
docker-compose -f docker-compose.gpu-specific.yml \
    --profile auto \
    up musetalk-auto
```

## ğŸ”§ é«˜çº§é…ç½®

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# GPUé…ç½®
CUDA_VISIBLE_DEVICES=0,2  # åªä½¿ç”¨GPU 0å’Œ2
GPU_MODE=manual
GPU_IDS=[0,2]
BATCH_SIZE_PER_GPU=12
MAX_BATCH_SIZE=24

# é¢„åŠ è½½é…ç½®
PRELOAD_FRAMES=150
PRELOAD_LATENTS=true

# æ€§èƒ½ä¼˜åŒ–
ENABLE_AMP=true
ENABLE_CUDNN_BENCHMARK=true
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256

# å†…å­˜ç®¡ç†
MEMORY_CLEANUP_INTERVAL=50
MEMORY_THRESHOLD=0.85
```

### Pythonä»£ç ç›´æ¥è°ƒç”¨

```python
from MuseTalkEngine.gpu_parallel_engine_v2 import MuseTalkGPUEngine, GPUConfig

# æ–¹å¼1: ä½¿ç”¨é¢„è®¾
engine = MuseTalkGPUEngine("single_4090d_48gb")

# æ–¹å¼2: æ‰‹åŠ¨é…ç½®
config = GPUConfig(
    mode="manual",
    gpu_ids=[0, 2],  # åªä½¿ç”¨GPU 0å’Œ2
    batch_size_per_gpu=12,
    max_batch_size=24,
    preload_frames=150,
    preload_latents=True,
    enable_amp=True
)
engine = MuseTalkGPUEngine(config)

# æ–¹å¼3: è‡ªåŠ¨é€‰æ‹©
engine = MuseTalkGPUEngine("auto")

# è¿è¡Œæ¨ç†
result = engine.inference(
    video_path="input.mp4",
    audio_path="audio.wav",
    models=models,  # åŠ è½½çš„æ¨¡å‹
    args={}
)

print(f"FPS: {result['statistics']['fps']}")
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•

```bash
# å•GPUæµ‹è¯•
docker run --rm \
    --runtime=nvidia \
    --gpus '"device=0"' \
    -v $(pwd):/workspace \
    musetalk:gpu-latest \
    python /workspace/benchmark_gpu.py --config single_4090d_48gb

# å¤šGPUæµ‹è¯•
docker run --rm \
    --runtime=nvidia \
    --gpus '"device=0,1,2,3"' \
    -v $(pwd):/workspace \
    musetalk:gpu-latest \
    python /workspace/benchmark_gpu.py --config quad_4090_24gb

# è‡ªå®šä¹‰GPUæµ‹è¯•
docker run --rm \
    --runtime=nvidia \
    --gpus all \
    -v $(pwd):/workspace \
    -e CUDA_VISIBLE_DEVICES=1,3 \
    musetalk:gpu-latest \
    python /workspace/benchmark_gpu.py --gpu_ids 1,3 --batch_size 10
```

### ç›‘æ§GPUä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# ç›‘æ§ç‰¹å®šGPU
nvidia-smi -i 0,2 -l 1

# è¯¦ç»†ç›‘æ§
nvidia-smi dmon -i 0,1,2,3
```

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜1: GPUä¸å¯è§

```bash
# æ£€æŸ¥Docker GPUæ”¯æŒ
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# æ£€æŸ¥CUDA_VISIBLE_DEVICES
echo $CUDA_VISIBLE_DEVICES
```

### é—®é¢˜2: æ˜¾å­˜ä¸è¶³

```bash
# å‡å°æ‰¹å¤„ç†å¤§å°
./run_musetalk_gpu.sh --gpu-ids 0 --batch-size 4 --mode inference

# æ¸…ç†GPUå†…å­˜
nvidia-smi --gpu-reset -i 0
```

### é—®é¢˜3: GPUè¢«å ç”¨

```bash
# æŸ¥æ‰¾å ç”¨GPUçš„è¿›ç¨‹
nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv

# ä½¿ç”¨å…¶ä»–ç©ºé—²GPU
./run_musetalk_gpu.sh --gpu-ids 2,3 --mode inference
```

## ğŸ“ˆ æ‰¹å¤„ç†å¤§å°å»ºè®®

| GPUå‹å· | æ˜¾å­˜ | æ¨èbatch_size | æœ€å¤§batch_size |
|---------|------|----------------|----------------|
| RTX 4090D | 48GB | 20 | 32 |
| RTX 4090 | 24GB | 8 | 12 |
| RTX 4080 | 16GB | 4 | 8 |
| RTX 3090 | 24GB | 6 | 10 |
| RTX 3080 | 10GB | 2 | 4 |

## ğŸ¯ æœ€ä½³å®è·µ

1. **é¢„åŠ è½½ä¼˜åŒ–**
   - å¯¹äºé•¿è§†é¢‘ï¼Œå¢åŠ  `PRELOAD_FRAMES`
   - å¯ç”¨ `PRELOAD_LATENTS` æå‡æ€§èƒ½

2. **å†…å­˜ç®¡ç†**
   - è®¾ç½®åˆé€‚çš„ `MEMORY_THRESHOLD`
   - å®šæœŸæ¸…ç†GPUå†…å­˜

3. **æ‰¹å¤„ç†ç­–ç•¥**
   - æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ `BATCH_SIZE_PER_GPU`
   - ä½¿ç”¨ `MAX_BATCH_SIZE` é™åˆ¶æ€»æ‰¹æ¬¡å¤§å°

4. **GPUé€‰æ‹©**
   - ä¼˜å…ˆä½¿ç”¨ç©ºé—²GPU
   - é¿å…ä¸å…¶ä»–ä»»åŠ¡å…±äº«GPU

## ğŸ“ å®Œæ•´å‘½ä»¤é€ŸæŸ¥

```bash
# æŸ¥çœ‹GPUçŠ¶æ€
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.free --format=csv

# å•GPUè¿è¡Œ
./run_musetalk_gpu.sh --gpu-ids 0 --batch-size 20 --mode benchmark

# å¤šGPUè¿è¡Œ
./run_musetalk_gpu.sh --gpu-ids 0,1,2,3 --batch-size 8 --mode benchmark

# è‡ªåŠ¨é€‰æ‹©GPU
./run_musetalk_gpu.sh --gpu-config auto --mode inference --video in.mp4 --audio audio.wav

# Dockerè¿è¡Œ
docker-compose -f docker-compose.gpu-specific.yml --profile single up

# è¿›å…¥å®¹å™¨è°ƒè¯•
docker exec -it musetalk-single-4090d bash

# æŸ¥çœ‹æ—¥å¿—
docker logs musetalk-single-4090d --tail 100
```

## âœ… éªŒè¯éƒ¨ç½²

```bash
# è¿è¡ŒéªŒè¯è„šæœ¬
docker run --rm \
    --runtime=nvidia \
    --gpus all \
    musetalk:gpu-latest \
    python -c "
import torch
import pynvml
from MuseTalkEngine.gpu_parallel_engine_v2 import GPUManager, GPUConfig

# åˆå§‹åŒ–
pynvml.nvmlInit()
config = GPUConfig(mode='auto')
manager = GPUManager(config)

# æ˜¾ç¤ºä¿¡æ¯
print(f'æ£€æµ‹åˆ°GPU: {manager.available_gpus}')
print(f'ä½¿ç”¨GPU: {manager.gpu_ids}')
for gpu_id in manager.gpu_ids:
    info = manager.get_gpu_memory_info(gpu_id)
    print(f'GPU {gpu_id}: Total={info[\"total\"]:.1f}GB, Free={info[\"free\"]:.1f}GB')
"
```

è¿™æ ·é…ç½®åï¼Œæ‚¨å¯ä»¥ï¼š
1. **ç²¾ç¡®æŒ‡å®šGPU** - å®Œå…¨æ§åˆ¶ä½¿ç”¨å“ªäº›GPU
2. **è‡ªåŠ¨æ£€æµ‹ç©ºé—²GPU** - æ™ºèƒ½é¿å¼€è¢«å ç”¨çš„GPU
3. **ä¼˜åŒ–æ‰¹å¤„ç†** - æ ¹æ®GPUé…ç½®è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°
4. **é¢„åŠ è½½åŠ é€Ÿ** - æå‰åŠ è½½æ•°æ®åˆ°GPUå†…å­˜

ç°åœ¨æ‚¨å¯ä»¥æ ¹æ®æœåŠ¡å™¨çš„å®é™…GPUä½¿ç”¨æƒ…å†µï¼Œçµæ´»é€‰æ‹©ç©ºé—²çš„GPUæ¥è¿è¡ŒMuseTalkï¼
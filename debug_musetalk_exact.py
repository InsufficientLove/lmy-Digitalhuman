#!/usr/bin/env python3
"""
ç²¾ç¡®æ¨¡æ‹ŸC#è°ƒç”¨ç¯å¢ƒçš„MuseTalkè¯Šæ–­è„šæœ¬
"""

import os
import sys
import subprocess
import time
from pathlib import Path

def simulate_csharp_environment():
    """æ¨¡æ‹ŸC#è®¾ç½®çš„ç¯å¢ƒå˜é‡"""
    print("ğŸ”§ æ¨¡æ‹ŸC#ç¯å¢ƒé…ç½®...")
    
    # æ¨¡æ‹ŸC#è®¾ç½®çš„ç¯å¢ƒå˜é‡
    env_vars = {
        'CUDA_VISIBLE_DEVICES': '0,1,2,3',
        'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:1024,garbage_collection_threshold:0.6',
        'OMP_NUM_THREADS': '16',
        'CUDA_LAUNCH_BLOCKING': '0',
        'TORCH_CUDNN_V8_API_ENABLED': '1',
        'CUBLAS_WORKSPACE_CONFIG': ':4096:8'
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"âœ… {key} = {value}")
    
    # è®¾ç½®PYTHONPATHï¼ˆå‡è®¾åœ¨MuseTalkç›®å½•ä¸­è¿è¡Œï¼‰
    current_dir = os.getcwd()
    if 'MuseTalk' not in current_dir:
        print("âŒ è¯·åœ¨MuseTalkç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬")
        return False
        
    os.environ['PYTHONPATH'] = current_dir
    print(f"âœ… PYTHONPATH = {current_dir}")
    
    return True

def test_inference_command():
    """æµ‹è¯•å®é™…çš„inferenceå‘½ä»¤"""
    print("\nğŸ§ª æµ‹è¯•scripts.inferenceå‘½ä»¤...")
    
    # æ„å»ºä¸C#ç›¸åŒçš„å‘½ä»¤
    cmd = [
        sys.executable, '-m', 'scripts.inference',
        '--inference_config', 'configs/inference/test.yaml',
        '--result_dir', './test_output',
        '--unet_model_path', 'models/musetalk/pytorch_model.bin',
        '--unet_config', 'models/musetalk/musetalk.json',
        '--version', 'v1',
        '--use_float16',
        '--batch_size', '8',
        '--fps', '25',
        '--gpu_id', '0',
        '--help'  # åªæ˜¾ç¤ºå¸®åŠ©ï¼Œä¸å®é™…è¿è¡Œ
    ]
    
    print(f"å‘½ä»¤: {' '.join(cmd)}")
    
    try:
        # ä½¿ç”¨çŸ­è¶…æ—¶æ¥æµ‹è¯•å‘½ä»¤æ˜¯å¦èƒ½å¯åŠ¨
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=10,
            cwd=os.getcwd()
        )
        
        print(f"è¿”å›ç : {result.returncode}")
        if result.stdout:
            print(f"æ ‡å‡†è¾“å‡º:\n{result.stdout}")
        if result.stderr:
            print(f"é”™è¯¯è¾“å‡º:\n{result.stderr}")
            
        return result.returncode == 0
        
    except subprocess.TimeoutExpired:
        print("âš ï¸  å‘½ä»¤è¶…æ—¶ï¼ˆ10ç§’ï¼‰- å¯èƒ½åœ¨ç­‰å¾…è¾“å…¥æˆ–å¡ä½")
        return False
    except Exception as e:
        print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
        return False

def test_step_by_step_import():
    """é€æ­¥æµ‹è¯•å¯¼å…¥è¿‡ç¨‹"""
    print("\nğŸ” é€æ­¥å¯¼å…¥æµ‹è¯•...")
    
    steps = [
        ("å¯¼å…¥PyTorch", "import torch"),
        ("æ£€æŸ¥CUDA", "print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"),
        ("è®¾ç½®GPU", "torch.cuda.set_device(0)"),
        ("å¯¼å…¥scripts", "import scripts"),
        ("å¯¼å…¥inference", "from scripts import inference"),
        ("å¯¼å…¥musetalk", "import musetalk"),
        ("å¯¼å…¥utils", "from musetalk.utils.utils import load_all_model"),
    ]
    
    for step_name, code in steps:
        print(f"\nğŸ“ {step_name}...")
        try:
            exec(code)
            print(f"âœ… æˆåŠŸ")
        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    return True

def test_model_loading_minimal():
    """æœ€å°åŒ–æ¨¡å‹åŠ è½½æµ‹è¯•"""
    print("\nğŸ—ï¸  æœ€å°åŒ–æ¨¡å‹åŠ è½½æµ‹è¯•...")
    
    try:
        print("æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        model_files = [
            'models/musetalk/musetalk.json',
            'models/musetalk/pytorch_model.bin'
        ]
        
        for file_path in model_files:
            if os.path.exists(file_path):
                size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                print(f"âœ… {file_path} ({size:.1f}MB)")
            else:
                print(f"âŒ {file_path} ä¸å­˜åœ¨")
                return False
        
        print("\nå¼€å§‹å¯¼å…¥load_all_model...")
        from musetalk.utils.utils import load_all_model
        print("âœ… load_all_modelå¯¼å…¥æˆåŠŸ")
        
        print("\nâš ï¸  è·³è¿‡å®é™…æ¨¡å‹åŠ è½½ä»¥é¿å…å¡æ­»")
        print("å¦‚éœ€æµ‹è¯•å®é™…åŠ è½½ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ:")
        print("vae, unet, pe = load_all_model('models/musetalk/pytorch_model.bin', 'models/musetalk/musetalk.json', 'v1')")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_config_files():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶"""
    print("\nğŸ“„ æ£€æŸ¥é…ç½®æ–‡ä»¶...")
    
    config_files = [
        'configs/inference/test.yaml',
        'musetalk/utils/dwpose/rtmpose-l_8xb32-270e_coco-ubody-wholebody-384x288.py',
        'musetalk/utils/dwpose/default_runtime.py'
    ]
    
    for config_file in config_files:
        if os.path.exists(config_file):
            print(f"âœ… {config_file}")
        else:
            print(f"âŒ {config_file} ä¸å­˜åœ¨")
            return False
    
    return True

def main():
    print("ğŸš€ MuseTalkç²¾ç¡®ç¯å¢ƒè¯Šæ–­")
    print("=" * 80)
    
    # 1. ç¯å¢ƒé…ç½®
    if not simulate_csharp_environment():
        return
    
    # 2. é…ç½®æ–‡ä»¶æ£€æŸ¥
    if not check_config_files():
        print("\nâŒ é…ç½®æ–‡ä»¶æ£€æŸ¥å¤±è´¥")
        return
    
    # 3. é€æ­¥å¯¼å…¥æµ‹è¯•
    if not test_step_by_step_import():
        print("\nâŒ æ¨¡å—å¯¼å…¥å¤±è´¥")
        return
    
    # 4. æ¨¡å‹åŠ è½½æµ‹è¯•
    if not test_model_loading_minimal():
        print("\nâŒ æ¨¡å‹åŠ è½½å‡†å¤‡å¤±è´¥")
        return
    
    # 5. å‘½ä»¤æµ‹è¯•
    if not test_inference_command():
        print("\nâŒ inferenceå‘½ä»¤æµ‹è¯•å¤±è´¥")
        return
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰åŸºç¡€æµ‹è¯•é€šè¿‡!")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨GPU")
    print("2. å°è¯•å•GPUè¿è¡Œï¼ˆCUDA_VISIBLE_DEVICES=0ï¼‰")
    print("3. å‡å°‘batch_sizeåˆ°1æˆ–2")
    print("4. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦æœ‰é—®é¢˜")

if __name__ == "__main__":
    main()
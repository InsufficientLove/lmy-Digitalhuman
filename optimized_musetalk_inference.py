#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
åŸºäºå®˜æ–¹realtime_inference.pyæºç ï¼Œé’ˆå¯¹å›ºå®šæ¨¡æ¿åœºæ™¯çš„æè‡´ä¼˜åŒ–

ç‰¹æ€§:
1. æ¨¡æ¿é¢„å¤„ç†ä¸€æ¬¡ï¼Œæ°¸ä¹…ä½¿ç”¨
2. 4x RTX 4090å¹¶è¡Œæ¨ç†
3. å†…å­˜é¢„åŠ è½½ï¼Œå‡å°‘é‡å¤è®¡ç®—
4. æ‰¹å¤„ç†ä¼˜åŒ–ï¼Œæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡
"""

import argparse
import os
import json
import time
import threading
import queue
import multiprocessing as mp
from pathlib import Path
from omegaconf import OmegaConf
import numpy as np
import cv2
import torch
import glob
import pickle
import copy
from tqdm import tqdm
from transformers import WhisperModel

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image_prepare_material, get_image_blending
from musetalk.utils.audio_processor import AudioProcessor

class OptimizedMuseTalkInference:
    """MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–æ¨ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device_count = torch.cuda.device_count()
        self.models = {}  # æ¯ä¸ªGPUçš„æ¨¡å‹å®ä¾‹
        self.templates = {}  # é¢„å¤„ç†çš„æ¨¡æ¿ç¼“å­˜
        self.audio_processor = None
        self.fp = None
        
        print(f"[START] åˆå§‹åŒ–MuseTalkä¼˜åŒ–æ¨ç†å™¨ - æ£€æµ‹åˆ° {self.device_count} ä¸ªGPU")
        
        # åˆå§‹åŒ–æ¯ä¸ªGPUçš„æ¨¡å‹
        self._initialize_models()
        
        # é¢„å¤„ç†æ‰€æœ‰æ¨¡æ¿
        self._preprocess_templates()
        
        print("[OK] MuseTalkä¼˜åŒ–æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¯ä¸ªGPUçš„æ¨¡å‹å®ä¾‹"""
        print("[CONFIG] åˆå§‹åŒ–GPUæ¨¡å‹...")
        
        for gpu_id in range(self.device_count):
            device = torch.device(f"cuda:{gpu_id}")
            
            print(f"ğŸ® åˆå§‹åŒ–GPU {gpu_id} æ¨¡å‹...")
            
            # åŠ è½½æ¨¡å‹
            vae, unet, pe = load_all_model(
                unet_model_path=self.config.unet_model_path,
                vae_type=self.config.vae_type,
                unet_config=self.config.unet_config,
                device=device
            )
            
            # è½¬æ¢ä¸ºåŠç²¾åº¦å¹¶ç§»åŠ¨åˆ°æŒ‡å®šGPU
            pe = pe.half().to(device)
            vae.vae = vae.vae.half().to(device)
            unet.model = unet.model.half().to(device)
            
            # åˆå§‹åŒ–Whisperæ¨¡å‹
            whisper = WhisperModel.from_pretrained(self.config.whisper_dir)
            whisper = whisper.to(device=device, dtype=unet.model.dtype).eval()
            whisper.requires_grad_(False)
            
            # åˆå§‹åŒ–æ—¶é—´æ­¥
            timesteps = torch.tensor([0], device=device)
            
            self.models[gpu_id] = {
                'vae': vae,
                'unet': unet,
                'pe': pe,
                'whisper': whisper,
                'timesteps': timesteps,
                'device': device
            }
            
            print(f"[OK] GPU {gpu_id} æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨ï¼ˆå…±äº«ï¼‰
        self.audio_processor = AudioProcessor(feature_extractor_path=self.config.whisper_dir)
        
        # åˆå§‹åŒ–é¢éƒ¨è§£æå™¨ï¼ˆå…±äº«ï¼‰
        if self.config.version == "v15":
            self.fp = FaceParsing(
                left_cheek_width=self.config.left_cheek_width,
                right_cheek_width=self.config.right_cheek_width
            )
        else:
            self.fp = FaceParsing()
        
        # âœ… ä½¿ç”¨MuseTalkåŸç”Ÿé¢éƒ¨æ£€æµ‹ï¼Œæ— éœ€é¢å¤–åˆå§‹åŒ–
        print("[OK] ä½¿ç”¨MuseTalkåŸç”Ÿé¢éƒ¨æ£€æµ‹é€»è¾‘")
    
    def _preprocess_templates(self):
        """é¢„å¤„ç†æ‰€æœ‰æ¨¡æ¿"""
        print("ğŸ”„ å¼€å§‹é¢„å¤„ç†æ¨¡æ¿...")
        
        # ä»wwwroot/templatesç›®å½•è¯»å–æ¨¡æ¿åˆ—è¡¨
        template_dir = getattr(self.config, 'template_dir', './templates')
        if os.path.exists(template_dir):
            template_files = []
            # æ”¯æŒå¸¸è§å›¾ç‰‡æ ¼å¼
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.webp']:
                template_files.extend(glob.glob(os.path.join(template_dir, ext)))
        else:
            print(f"âš ï¸ æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {template_dir}")
            template_files = []
        
        if not template_files:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ¨¡æ¿æ–‡ä»¶ï¼Œå°†åœ¨è¿è¡Œæ—¶åŠ¨æ€é¢„å¤„ç†")
            return
        
        for template_file in template_files:
            template_id = Path(template_file).stem
            print(f"ğŸ”„ é¢„å¤„ç†æ¨¡æ¿: {template_id}")
            
            try:
                template_data = self._preprocess_single_template(template_id, template_file)
                self.templates[template_id] = template_data
                print(f"[OK] æ¨¡æ¿ {template_id} é¢„å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"[ERROR] æ¨¡æ¿ {template_id} é¢„å¤„ç†å¤±è´¥: {e}")
        
        print(f"ğŸ‰ æ‰€æœ‰æ¨¡æ¿é¢„å¤„ç†å®Œæˆï¼Œå…± {len(self.templates)} ä¸ªæ¨¡æ¿")
    
    def _preprocess_single_template(self, template_id, template_path):
        """é¢„å¤„ç†å•ä¸ªæ¨¡æ¿"""
        # åˆ›å»ºæ¨¡æ¿ç›®å½•
        template_dir = f"./results/optimized_templates/{template_id}"
        os.makedirs(template_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»é¢„å¤„ç†è¿‡
        cache_file = f"{template_dir}/preprocessed_cache.pkl"
        if os.path.exists(cache_file):
            print(f"ğŸ“‹ åŠ è½½å·²é¢„å¤„ç†çš„æ¨¡æ¿ç¼“å­˜: {template_id}")
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        
        # ä½¿ç”¨GPU 0è¿›è¡Œé¢„å¤„ç†
        device = self.models[0]['device']
        vae = self.models[0]['vae']
        
        # ğŸ”§ è¯»å–æ¨¡æ¿å›¾ç‰‡ - å¤„ç†ä¸­æ–‡è·¯å¾„
        print(f"[CONFIG] è¯»å–æ¨¡æ¿å›¾ç‰‡: {template_path}")
        print(f"[CONFIG] æ–‡ä»¶å­˜åœ¨æ£€æŸ¥: {os.path.exists(template_path)}")
        
        if os.path.isfile(template_path):
            # æ–¹æ³•1: ç›´æ¥è¯»å–
            img = cv2.imread(template_path)
            
            if img is None:
                # æ–¹æ³•2: ä½¿ç”¨numpyè¯»å–ï¼ˆå¤„ç†ä¸­æ–‡è·¯å¾„ï¼‰
                try:
                    import numpy as np
                    print(f"[CONFIG] å°è¯•ä½¿ç”¨numpyè¯»å–ä¸­æ–‡è·¯å¾„å›¾ç‰‡")
                    with open(template_path, 'rb') as f:
                        img_data = f.read()
                    img_array = np.frombuffer(img_data, np.uint8)
                    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                    print(f"[OK] numpyæ–¹æ³•è¯»å–æˆåŠŸ")
                except Exception as e:
                    print(f"[ERROR] numpyè¯»å–å¤±è´¥: {e}")
                    
            if img is None:
                # åˆ—å‡ºç›®å½•æ–‡ä»¶è¿›è¡Œè°ƒè¯•
                dir_path = os.path.dirname(template_path)
                if os.path.exists(dir_path):
                    files = os.listdir(dir_path)
                    print(f"[ERROR] ç›®å½•ä¸­çš„æ–‡ä»¶: {files}")
                raise ValueError(f"æ— æ³•è¯»å–æ¨¡æ¿å›¾ç‰‡: {template_path}")
            
            frame_list = [img]
            print(f"[OK] æˆåŠŸè¯»å–æ¨¡æ¿å›¾ç‰‡ï¼Œå°ºå¯¸: {img.shape}")
        else:
            # åˆ—å‡ºç›®å½•æ–‡ä»¶è¿›è¡Œè°ƒè¯•
            dir_path = os.path.dirname(template_path)
            if os.path.exists(dir_path):
                files = os.listdir(dir_path)
                print(f"[ERROR] ç›®å½•ä¸­çš„æ–‡ä»¶: {files}")
            raise ValueError(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
        
        # ğŸ”§ æå–é¢éƒ¨åæ ‡ - ä½¿ç”¨MuseTalkåŸç”Ÿé€»è¾‘
        print(f"ğŸ” æå–é¢éƒ¨åæ ‡: {template_id}")
        print(f"[CONFIG] æ¨¡æ¿è·¯å¾„: {template_path}")
        
        try:
            # ğŸ¯ ä½¿ç”¨MuseTalkåŸç”Ÿçš„get_landmark_and_bboxå‡½æ•°
            # ç°åœ¨ä½¿ç”¨è‹±æ–‡SystemNameï¼Œä¸å†æœ‰ä¸­æ–‡è·¯å¾„é—®é¢˜
            coord_list, frame_list = get_landmark_and_bbox([template_path], self.config.bbox_shift)
            print(f"[OK] MuseTalkåŸç”Ÿé¢éƒ¨æ£€æµ‹æˆåŠŸ: {len(coord_list)} ä¸ªåæ ‡")
            
            # éªŒè¯åæ ‡æœ‰æ•ˆæ€§
            if coord_list and len(coord_list) > 0:
                for i, coord in enumerate(coord_list):
                    print(f"[CONFIG] åæ ‡ {i+1}: {coord}")
            
        except Exception as e:
            print(f"[ERROR] MuseTalkåŸç”Ÿé¢éƒ¨æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # ğŸ¯ ä½¿ç”¨MuseTalkçš„æ ‡å‡†å ä½ç¬¦é€»è¾‘
            print(f"[FALLBACK] ä½¿ç”¨MuseTalkæ ‡å‡†çš„å ä½ç¬¦åæ ‡")
            coord_placeholder = (0.0, 0.0, 0.0, 0.0)
            coord_list = [coord_placeholder]
            frame_list = [img]
            print(f"[FALLBACK] ä½¿ç”¨å ä½ç¬¦åæ ‡: {coord_placeholder}")
            print("[INFO] è¿™å°†ä½¿ç”¨MuseTalkçš„å†…ç½®é”™è¯¯å¤„ç†é€»è¾‘")
        
        # é¢„è®¡ç®—VAEç¼–ç 
        print(f"ğŸ§  é¢„è®¡ç®—VAEç¼–ç : {template_id}")
        input_latent_list = []
        
        for bbox, frame in zip(coord_list, frame_list):
            # æ£€æŸ¥åæ ‡æ˜¯å¦æœ‰æ•ˆï¼ˆä¸ä¸º0ä¸”æœ‰å®é™…åŒºåŸŸï¼‰
            x1, y1, x2, y2 = bbox
            if x1 >= x2 or y1 >= y2:
                print(f"[WARN] è·³è¿‡æ— æ•ˆåæ ‡: {bbox}")
                continue
                
            # ç¡®ä¿åæ ‡åœ¨å›¾ç‰‡èŒƒå›´å†…
            h, w = frame.shape[:2]
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w, int(x2)), min(h, int(y2))
            
            if self.config.version == "v15":
                y2 = y2 + self.config.extra_margin
                y2 = min(y2, frame.shape[0])
            
            # è£å‰ªé¢éƒ¨åŒºåŸŸ
            crop_frame = frame[y1:y2, x1:x2]
            
            # æ£€æŸ¥è£å‰ªåçš„å›¾ç‰‡æ˜¯å¦æœ‰æ•ˆ
            if crop_frame.size == 0:
                print(f"[WARN] è£å‰ªåçš„é¢éƒ¨åŒºåŸŸä¸ºç©º: ({x1}, {y1}, {x2}, {y2})")
                continue
                
            print(f"[OK] é¢éƒ¨åŒºåŸŸè£å‰ªæˆåŠŸ: ({x1}, {y1}, {x2}, {y2}), å°ºå¯¸: {crop_frame.shape}")
            
            # è°ƒæ•´åˆ°MuseTalkæ ‡å‡†å°ºå¯¸256x256
            resized_crop_frame = cv2.resize(crop_frame, (256, 256), interpolation=cv2.INTER_LANCZOS4)
            
            with torch.no_grad():
                latents = vae.get_latents_for_unet(resized_crop_frame)
                input_latent_list.append(latents)
        
        # åˆ›å»ºå¾ªç¯åˆ—è¡¨ï¼ˆå‰å‘+åå‘ï¼Œç”¨äºå¹³æ»‘ï¼‰
        frame_list_cycle = frame_list + frame_list[::-1]
        coord_list_cycle = coord_list + coord_list[::-1]
        input_latent_list_cycle = input_latent_list + input_latent_list[::-1]
        
        # é¢„è®¡ç®—é¢éƒ¨è§£æmask
        print(f"ğŸ­ é¢„è®¡ç®—é¢éƒ¨è§£æ: {template_id}")
        mask_coords_list_cycle = []
        mask_list_cycle = []
        
        for i, frame in enumerate(tqdm(frame_list_cycle, desc="é¢„è®¡ç®—é¢éƒ¨è§£æ")):
            x1, y1, x2, y2 = coord_list_cycle[i]
            
            # ç¡®ä¿åæ ‡æœ‰æ•ˆ
            h, w = frame.shape[:2]
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w, int(x2)), min(h, int(y2))
            
            # æ£€æŸ¥åæ ‡æ˜¯å¦æœ‰æ•ˆ
            if x1 >= x2 or y1 >= y2:
                print(f"[WARN] è·³è¿‡æ— æ•ˆçš„é¢éƒ¨è§£æåæ ‡: ({x1}, {y1}, {x2}, {y2})")
                # ä½¿ç”¨é»˜è®¤å€¼
                mask_coords_list_cycle.append([0, 0, 256, 256])
                mask_list_cycle.append(np.ones((256, 256), dtype=np.uint8) * 255)
                continue
            
            if self.config.version == "v15":
                mode = self.config.parsing_mode
            else:
                mode = "raw"
            
            try:
                mask, crop_box = get_image_prepare_material(frame, [x1, y1, x2, y2], fp=self.fp, mode=mode)
                mask_coords_list_cycle.append(crop_box)
                mask_list_cycle.append(mask)
                print(f"[OK] é¢éƒ¨è§£æå®Œæˆ: åæ ‡({x1}, {y1}, {x2}, {y2})")
            except Exception as e:
                print(f"[WARN] é¢éƒ¨è§£æå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤mask")
                # ä½¿ç”¨é»˜è®¤å€¼
                mask_coords_list_cycle.append([x1, y1, x2, y2])
                mask_list_cycle.append(np.ones((256, 256), dtype=np.uint8) * 255)
        
        # æ„å»ºæ¨¡æ¿æ•°æ®
        template_data = {
            'template_id': template_id,
            'template_path': template_path,
            'frame_list_cycle': frame_list_cycle,
            'coord_list_cycle': coord_list_cycle,
            'input_latent_list_cycle': input_latent_list_cycle,
            'mask_coords_list_cycle': mask_coords_list_cycle,
            'mask_list_cycle': mask_list_cycle,
            'preprocessed_at': time.time()
        }
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_file, 'wb') as f:
            pickle.dump(template_data, f)
        
        return template_data
    
    def inference_parallel(self, template_id, audio_path, output_path, fps=25):
        """å¹¶è¡Œæ¨ç† - 4GPUååŒå·¥ä½œ"""
        # å¦‚æœæ¨¡æ¿æœªé¢„å¤„ç†ï¼ŒåŠ¨æ€é¢„å¤„ç†
        if template_id not in self.templates:
            print(f"ğŸ”„ æ¨¡æ¿ {template_id} æœªé¢„å¤„ç†ï¼Œå¼€å§‹åŠ¨æ€é¢„å¤„ç†...")
            template_path = self._find_template_path(template_id)
            if not template_path:
                raise ValueError(f"æ¨¡æ¿ {template_id} æ–‡ä»¶æœªæ‰¾åˆ°")
            
            template_data = self._preprocess_single_template(template_id, template_path)
            self.templates[template_id] = template_data
            print(f"[OK] æ¨¡æ¿ {template_id} åŠ¨æ€é¢„å¤„ç†å®Œæˆ")
        
        template_data = self.templates[template_id]
        
        print(f"[START] å¼€å§‹å¹¶è¡Œæ¨ç†: æ¨¡æ¿={template_id}, éŸ³é¢‘={audio_path}")
        start_time = time.time()
        
        # 1. éŸ³é¢‘ç‰¹å¾æå–ï¼ˆä½¿ç”¨GPU 0ï¼‰
        print("ğŸµ æå–éŸ³é¢‘ç‰¹å¾...")
        audio_start = time.time()
        
        device = self.models[0]['device']
        whisper = self.models[0]['whisper']
        weight_dtype = self.models[0]['unet'].model.dtype
        
        whisper_input_features, librosa_length = self.audio_processor.get_audio_feature(
            audio_path, weight_dtype=weight_dtype
        )
        
        whisper_chunks = self.audio_processor.get_whisper_chunk(
            whisper_input_features,
            device,
            weight_dtype,
            whisper,
            librosa_length,
            fps=fps,
            audio_padding_length_left=self.config.audio_padding_length_left,
            audio_padding_length_right=self.config.audio_padding_length_right,
        )
        
        audio_time = time.time() - audio_start
        print(f"[OK] éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {audio_time:.2f}s, å…± {len(whisper_chunks)} å¸§")
        
        # 2. ğŸš€ ä¼˜åŒ–å•GPUé«˜é€Ÿæ¨ç†ï¼ˆé¿å…å¤šGPUé€šä¿¡å¼€é”€ï¼‰
        print("ğŸ® å¼€å§‹ä¼˜åŒ–GPUæ¨ç†...")
        inference_start = time.time()
        
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨å•GPUï¼Œå¢å¤§æ‰¹å¤„ç†å¤§å°
        gpu_id = 0
        model = self.models[gpu_id]
        device = model['device']
        
        # ğŸš€ ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°ï¼ˆæ ¹æ®GPUå†…å­˜åŠ¨æ€è°ƒæ•´ï¼‰
        optimized_batch_size = min(self.config.batch_size * 4, 256)
        print(f"ğŸ® GPU {gpu_id} é«˜é€Ÿå¤„ç†ï¼Œä¼˜åŒ–æ‰¹å¤§å°: {optimized_batch_size}")
        
        # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
        gen = datagen(
            whisper_chunks,
            template_data['input_latent_list_cycle'],
            optimized_batch_size
        )
        
        res_frame_list = []
        batch_count = 0
        
        # ğŸš€ å•GPUæ‰¹å¤„ç†æ¨ç†
        for whisper_batch, latent_batch in gen:
            batch_count += 1
            
            with torch.no_grad():
                # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨åŠç²¾åº¦æ¨ç†åŠ é€Ÿ
                whisper_batch = whisper_batch.half().to(device)
                latent_batch = latent_batch.half().to(device)
                
                # MuseTalkæ¨ç†
                pred_latents = model['unet'].model(
                    latent_batch,
                    model['timesteps'],
                    encoder_hidden_states=whisper_batch
                ).sample
                
                # VAEè§£ç 
                pred_latents = 1 / 0.18215 * pred_latents
                pred_frames = model['vae'].vae.decode(pred_latents).sample
                pred_frames = (pred_frames / 2 + 0.5).clamp(0, 1)
                
                # è½¬æ¢ä¸ºnumpyï¼ˆè½¬å›float32é¿å…ç²¾åº¦é—®é¢˜ï¼‰
                pred_frames = pred_frames.cpu().float().numpy()
                pred_frames = (pred_frames * 255).astype(np.uint8)
                
                # è°ƒæ•´ç»´åº¦ (B, C, H, W) -> (B, H, W, C)
                pred_frames = np.transpose(pred_frames, (0, 2, 3, 1))
                
                res_frame_list.extend(pred_frames)
        
        inference_time = time.time() - inference_start
        print(f"[OK] ä¼˜åŒ–GPUæ¨ç†å®Œæˆ: {inference_time:.2f}s, å¤„ç† {batch_count} æ‰¹æ¬¡ï¼Œå…± {len(res_frame_list)} å¸§")
        
        # 3. åå¤„ç†å’Œè§†é¢‘åˆæˆ
        print("ğŸ¬ å¼€å§‹è§†é¢‘åˆæˆ...")
        postprocess_start = time.time()
        
        self._postprocess_and_save(
            template_data, res_frame_list, audio_path, output_path, fps
        )
        
        postprocess_time = time.time() - postprocess_start
        total_time = time.time() - start_time
        
        print(f"ğŸ‰ æ¨ç†å®Œæˆ!")
        print(f"[STATS] æ€§èƒ½ç»Ÿè®¡:")
        print(f"   éŸ³é¢‘å¤„ç†: {audio_time:.2f}s")
        print(f"   GPUæ¨ç†: {inference_time:.2f}s")
        print(f"   åå¤„ç†: {postprocess_time:.2f}s")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"   è§†é¢‘é•¿åº¦: {len(res_frame_list)/fps:.1f}s")
        print(f"   å®æ—¶ç‡: {(len(res_frame_list)/fps)/total_time:.2f}x")
        
        return output_path
    
    def _gpu_worker(self, gpu_id, task_queue, result_queue):
        """GPUå·¥ä½œçº¿ç¨‹"""
        models = self.models[gpu_id]
        device = models['device']
        
        print(f"ğŸ® GPU {gpu_id} å·¥ä½œçº¿ç¨‹å¯åŠ¨")
        
        while True:
            try:
                # è·å–ä»»åŠ¡ï¼ˆè¶…æ—¶1ç§’ï¼‰
                batch_idx, whisper_batch, latent_batch = task_queue.get(timeout=1)
                
                # æ‰§è¡Œæ¨ç†
                with torch.no_grad():
                    # éŸ³é¢‘ç‰¹å¾ç¼–ç 
                    audio_feature_batch = models['pe'](whisper_batch.to(device))
                    latent_batch = latent_batch.to(device=device, dtype=models['unet'].model.dtype)
                    
                    # UNetæ¨ç†
                    pred_latents = models['unet'].model(
                        latent_batch,
                        models['timesteps'],
                        encoder_hidden_states=audio_feature_batch
                    ).sample
                    
                    # VAEè§£ç 
                    pred_latents = pred_latents.to(device=device, dtype=models['vae'].vae.dtype)
                    recon = models['vae'].decode_latents(pred_latents)
                
                # è¿”å›ç»“æœ
                result_queue.put((batch_idx, recon))
                task_queue.task_done()
                
            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºçº¿ç¨‹
                break
            except Exception as e:
                print(f"[ERROR] GPU {gpu_id} å¤„ç†é”™è¯¯: {e}")
                task_queue.task_done()
        
        print(f"ğŸ›‘ GPU {gpu_id} å·¥ä½œçº¿ç¨‹ç»“æŸ")
    
    def _find_template_path(self, template_id):
        """æŸ¥æ‰¾æ¨¡æ¿æ–‡ä»¶è·¯å¾„"""
        template_dir = getattr(self.config, 'template_dir', './templates')
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.webp']
        
        for ext in extensions:
            template_path = os.path.join(template_dir, f"{template_id}{ext}")
            if os.path.exists(template_path):
                return template_path
        
        return None
    
    def _postprocess_and_save(self, template_data, res_frame_list, audio_path, output_path, fps):
        """åå¤„ç†å’Œä¿å­˜è§†é¢‘"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.dirname(output_path)
        os.makedirs(output_dir, exist_ok=True)
        
        temp_dir = f"{output_dir}/temp_{int(time.time())}"
        os.makedirs(temp_dir, exist_ok=True)
        
        # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥åœ¨å†…å­˜ä¸­åˆæˆå¸§ï¼Œä½¿ç”¨OpenCV VideoWriter
        print("ğŸ–¼ï¸ é«˜é€Ÿåˆæˆæœ€ç»ˆå¸§...")
        
        # è·å–è§†é¢‘å°ºå¯¸ï¼ˆä½¿ç”¨ç¬¬ä¸€å¸§ï¼‰
        first_frame_idx = 0 % len(template_data['coord_list_cycle'])
        sample_ori_frame = template_data['frame_list_cycle'][first_frame_idx]
        height, width = sample_ori_frame.shape[:2]
        
        # åˆå§‹åŒ–VideoWriter - ç›´æ¥å†™å…¥æœ€ç»ˆè§†é¢‘æ–‡ä»¶
        temp_video = f"{temp_dir}/temp_video.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(temp_video, fourcc, fps, (width, height))
        
        if not video_writer.isOpened():
            print("âŒ VideoWriteråˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°PNGæ–¹å¼")
            # å›é€€åˆ°åŸæ–¹å¼
            for i, res_frame in enumerate(tqdm(res_frame_list, desc="åˆæˆå¸§")):
                cycle_idx = i % len(template_data['coord_list_cycle'])
                bbox = template_data['coord_list_cycle'][cycle_idx]
                ori_frame = copy.deepcopy(template_data['frame_list_cycle'][cycle_idx])
                
                x1, y1, x2, y2 = bbox
                
                try:
                    res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                    mask = template_data['mask_list_cycle'][cycle_idx]
                    mask_crop_box = template_data['mask_coords_list_cycle'][cycle_idx]
                    combine_frame = get_image_blending(ori_frame, res_frame, bbox, mask, mask_crop_box)
                    cv2.imwrite(f"{temp_dir}/{i:08d}.png", combine_frame)
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {i} å¸§å¤„ç†å¤±è´¥: {e}")
                    continue
            
            print("ğŸ¬ ç”Ÿæˆè§†é¢‘æ–‡ä»¶...")
            cmd_img2video = f"ffmpeg -y -v warning -r {fps} -f image2 -i {temp_dir}/%08d.png -vcodec libx264 -vf format=yuv420p -crf 18 {temp_video}"
            os.system(cmd_img2video)
        else:
            # ğŸš€ é«˜é€Ÿæ¨¡å¼ï¼šç›´æ¥å†™å…¥è§†é¢‘
            for i, res_frame in enumerate(tqdm(res_frame_list, desc="é«˜é€Ÿåˆæˆ")):
                cycle_idx = i % len(template_data['coord_list_cycle'])
                bbox = template_data['coord_list_cycle'][cycle_idx]
                ori_frame = copy.deepcopy(template_data['frame_list_cycle'][cycle_idx])
                
                x1, y1, x2, y2 = bbox
                
                try:
                    res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                    mask = template_data['mask_list_cycle'][cycle_idx]
                    mask_crop_box = template_data['mask_coords_list_cycle'][cycle_idx]
                    combine_frame = get_image_blending(ori_frame, res_frame, bbox, mask, mask_crop_box)
                    
                    # ç›´æ¥å†™å…¥è§†é¢‘æ–‡ä»¶ï¼Œæ— éœ€ä¿å­˜PNG
                    video_writer.write(combine_frame)
                    
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {i} å¸§å¤„ç†å¤±è´¥: {e}")
                    continue
            
            video_writer.release()
            print("ğŸ¬ é«˜é€Ÿè§†é¢‘åˆæˆå®Œæˆ")
        
        # åˆå¹¶éŸ³é¢‘
        print("ğŸ”Š åˆå¹¶éŸ³é¢‘...")
        cmd_combine_audio = f"ffmpeg -y -v warning -i {audio_path} -i {temp_video} {output_path}"
        os.system(cmd_combine_audio)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)
        
        print(f"[OK] è§†é¢‘ä¿å­˜å®Œæˆ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description="MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–æ¨ç†")
    parser.add_argument("--config", type=str, default="configs/optimized_inference.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--template_id", type=str, required=True, help="æ¨¡æ¿ID")
    parser.add_argument("--audio_path", type=str, required=True, help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True, help="è¾“å‡ºè§†é¢‘è·¯å¾„")
    parser.add_argument("--fps", type=int, default=25, help="è§†é¢‘å¸§ç‡")
    
    # MuseTalkæ ‡å‡†å‚æ•°
    parser.add_argument("--version", type=str, default="v1", choices=["v1", "v15"])
    parser.add_argument("--unet_config", type=str, default="./models/musetalk/musetalk.json")
    parser.add_argument("--unet_model_path", type=str, default="./models/musetalk/pytorch_model.bin")
    parser.add_argument("--whisper_dir", type=str, default="./models/whisper")
    parser.add_argument("--vae_type", type=str, default="sd-vae")
    parser.add_argument("--batch_size", type=int, default=64, help="æ‰¹å¤„ç†å¤§å° - 4x RTX 4090ä¼˜åŒ–")
    parser.add_argument("--bbox_shift", type=int, default=0)
    parser.add_argument("--extra_margin", type=int, default=10)
    parser.add_argument("--audio_padding_length_left", type=int, default=2)
    parser.add_argument("--audio_padding_length_right", type=int, default=2)
    parser.add_argument("--parsing_mode", default='jaw')
    parser.add_argument("--left_cheek_width", type=int, default=90)
    parser.add_argument("--right_cheek_width", type=int, default=90)
    parser.add_argument("--template_dir", type=str, default="./templates", help="æ¨¡æ¿ç›®å½•")
    
    args = parser.parse_args()
    
    print("å¯åŠ¨MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–æ¨ç†å™¨")
    print(f"é…ç½®å‚æ•°:")
    print(f"   æ¨¡æ¿ID: {args.template_id}")
    print(f"   éŸ³é¢‘æ–‡ä»¶: {args.audio_path}")
    print(f"   è¾“å‡ºè·¯å¾„: {args.output_path}")
    print(f"   æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    inference_engine = OptimizedMuseTalkInference(args)
    
    # æ‰§è¡Œæ¨ç†
    result_path = inference_engine.inference_parallel(
        template_id=args.template_id,
        audio_path=args.audio_path,
        output_path=args.output_path,
        fps=args.fps
    )
    
    print(f"ğŸ‰ æ¨ç†å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {result_path}")

if __name__ == "__main__":
    main()
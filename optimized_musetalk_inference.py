#!/usr/bin/env python3
"""
ğŸš€ MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
åŸºäºå®˜æ–¹realtime_inference.pyæºç ï¼Œé’ˆå¯¹å›ºå®šæ¨¡æ¿åœºæ™¯çš„æè‡´ä¼˜åŒ–

ç‰¹æ€§:
1. æ¨¡æ¿é¢„å¤„ç†ä¸€æ¬¡ï¼Œæ°¸ä¹…ä½¿ç”¨
2. 4x RTX 4090å¹¶è¡Œæ¨ç†
3. å†…å­˜é¢„åŠ è½½ï¼Œå‡å°‘é‡å¤è®¡ç®—
4. æ‰¹å¤„ç†ä¼˜åŒ–ï¼Œæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡
"""

import argparse
import os
import json
import time
import threading
import queue
import multiprocessing as mp
from pathlib import Path
from omegaconf import OmegaConf
import numpy as np
import cv2
import torch
import glob
import pickle
import copy
from tqdm import tqdm
from transformers import WhisperModel

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image_prepare_material, get_image_blending
from musetalk.utils.audio_processor import AudioProcessor

class OptimizedMuseTalkInference:
    """ğŸš€ MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–æ¨ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device_count = torch.cuda.device_count()
        self.models = {}  # æ¯ä¸ªGPUçš„æ¨¡å‹å®ä¾‹
        self.templates = {}  # é¢„å¤„ç†çš„æ¨¡æ¿ç¼“å­˜
        self.audio_processor = None
        self.fp = None
        
        print(f"ğŸš€ åˆå§‹åŒ–MuseTalkä¼˜åŒ–æ¨ç†å™¨ - æ£€æµ‹åˆ° {self.device_count} ä¸ªGPU")
        
        # åˆå§‹åŒ–æ¯ä¸ªGPUçš„æ¨¡å‹
        self._initialize_models()
        
        # é¢„å¤„ç†æ‰€æœ‰æ¨¡æ¿
        self._preprocess_templates()
        
        print("âœ… MuseTalkä¼˜åŒ–æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¯ä¸ªGPUçš„æ¨¡å‹å®ä¾‹"""
        print("ğŸ”§ åˆå§‹åŒ–GPUæ¨¡å‹...")
        
        for gpu_id in range(self.device_count):
            device = torch.device(f"cuda:{gpu_id}")
            
            print(f"ğŸ® åˆå§‹åŒ–GPU {gpu_id} æ¨¡å‹...")
            
            # åŠ è½½æ¨¡å‹
            vae, unet, pe = load_all_model(
                unet_model_path=self.config.unet_model_path,
                vae_type=self.config.vae_type,
                unet_config=self.config.unet_config,
                device=device
            )
            
            # è½¬æ¢ä¸ºåŠç²¾åº¦å¹¶ç§»åŠ¨åˆ°æŒ‡å®šGPU
            pe = pe.half().to(device)
            vae.vae = vae.vae.half().to(device)
            unet.model = unet.model.half().to(device)
            
            # åˆå§‹åŒ–Whisperæ¨¡å‹
            whisper = WhisperModel.from_pretrained(self.config.whisper_dir)
            whisper = whisper.to(device=device, dtype=unet.model.dtype).eval()
            whisper.requires_grad_(False)
            
            # åˆå§‹åŒ–æ—¶é—´æ­¥
            timesteps = torch.tensor([0], device=device)
            
            self.models[gpu_id] = {
                'vae': vae,
                'unet': unet,
                'pe': pe,
                'whisper': whisper,
                'timesteps': timesteps,
                'device': device
            }
            
            print(f"âœ… GPU {gpu_id} æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨ï¼ˆå…±äº«ï¼‰
        self.audio_processor = AudioProcessor(feature_extractor_path=self.config.whisper_dir)
        
        # åˆå§‹åŒ–é¢éƒ¨è§£æå™¨ï¼ˆå…±äº«ï¼‰
        if self.config.version == "v15":
            self.fp = FaceParsing(
                left_cheek_width=self.config.left_cheek_width,
                right_cheek_width=self.config.right_cheek_width
            )
        else:
            self.fp = FaceParsing()
    
    def _preprocess_templates(self):
        """é¢„å¤„ç†æ‰€æœ‰æ¨¡æ¿"""
        print("ğŸ”„ å¼€å§‹é¢„å¤„ç†æ¨¡æ¿...")
        
        # ä»wwwroot/templatesç›®å½•è¯»å–æ¨¡æ¿åˆ—è¡¨
        template_dir = getattr(self.config, 'template_dir', './templates')
        if os.path.exists(template_dir):
            template_files = []
            # æ”¯æŒå¸¸è§å›¾ç‰‡æ ¼å¼
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.webp']:
                template_files.extend(glob.glob(os.path.join(template_dir, ext)))
        else:
            print(f"âš ï¸ æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {template_dir}")
            template_files = []
        
        if not template_files:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ¨¡æ¿æ–‡ä»¶ï¼Œå°†åœ¨è¿è¡Œæ—¶åŠ¨æ€é¢„å¤„ç†")
            return
        
        for template_file in template_files:
            template_id = Path(template_file).stem
            print(f"ğŸ”„ é¢„å¤„ç†æ¨¡æ¿: {template_id}")
            
            try:
                template_data = self._preprocess_single_template(template_id, template_file)
                self.templates[template_id] = template_data
                print(f"âœ… æ¨¡æ¿ {template_id} é¢„å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ æ¨¡æ¿ {template_id} é¢„å¤„ç†å¤±è´¥: {e}")
        
        print(f"ğŸ‰ æ‰€æœ‰æ¨¡æ¿é¢„å¤„ç†å®Œæˆï¼Œå…± {len(self.templates)} ä¸ªæ¨¡æ¿")
    
    def _preprocess_single_template(self, template_id, template_path):
        """é¢„å¤„ç†å•ä¸ªæ¨¡æ¿"""
        # åˆ›å»ºæ¨¡æ¿ç›®å½•
        template_dir = f"./results/optimized_templates/{template_id}"
        os.makedirs(template_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»é¢„å¤„ç†è¿‡
        cache_file = f"{template_dir}/preprocessed_cache.pkl"
        if os.path.exists(cache_file):
            print(f"ğŸ“‹ åŠ è½½å·²é¢„å¤„ç†çš„æ¨¡æ¿ç¼“å­˜: {template_id}")
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        
        # ä½¿ç”¨GPU 0è¿›è¡Œé¢„å¤„ç†
        device = self.models[0]['device']
        vae = self.models[0]['vae']
        
        # è¯»å–æ¨¡æ¿å›¾ç‰‡
        if os.path.isfile(template_path):
            frame_list = [cv2.imread(template_path)]
            if frame_list[0] is None:
                raise ValueError(f"æ— æ³•è¯»å–æ¨¡æ¿å›¾ç‰‡: {template_path}")
        else:
            raise ValueError(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
        
        # æå–é¢éƒ¨åæ ‡
        print(f"ğŸ” æå–é¢éƒ¨åæ ‡: {template_id}")
        coord_list, frame_list = get_landmark_and_bbox([template_path], self.config.bbox_shift)
        
        # é¢„è®¡ç®—VAEç¼–ç 
        print(f"ğŸ§  é¢„è®¡ç®—VAEç¼–ç : {template_id}")
        input_latent_list = []
        coord_placeholder = (0.0, 0.0, 0.0, 0.0)
        
        for bbox, frame in zip(coord_list, frame_list):
            if bbox == coord_placeholder:
                continue
            
            x1, y1, x2, y2 = bbox
            if self.config.version == "v15":
                y2 = y2 + self.config.extra_margin
                y2 = min(y2, frame.shape[0])
            
            crop_frame = frame[y1:y2, x1:x2]
            resized_crop_frame = cv2.resize(crop_frame, (256, 256), interpolation=cv2.INTER_LANCZOS4)
            
            with torch.no_grad():
                latents = vae.get_latents_for_unet(resized_crop_frame)
                input_latent_list.append(latents)
        
        # åˆ›å»ºå¾ªç¯åˆ—è¡¨ï¼ˆå‰å‘+åå‘ï¼Œç”¨äºå¹³æ»‘ï¼‰
        frame_list_cycle = frame_list + frame_list[::-1]
        coord_list_cycle = coord_list + coord_list[::-1]
        input_latent_list_cycle = input_latent_list + input_latent_list[::-1]
        
        # é¢„è®¡ç®—é¢éƒ¨è§£æmask
        print(f"ğŸ­ é¢„è®¡ç®—é¢éƒ¨è§£æ: {template_id}")
        mask_coords_list_cycle = []
        mask_list_cycle = []
        
        for i, frame in enumerate(tqdm(frame_list_cycle, desc="é¢„è®¡ç®—é¢éƒ¨è§£æ")):
            x1, y1, x2, y2 = coord_list_cycle[i]
            if self.config.version == "v15":
                mode = self.config.parsing_mode
            else:
                mode = "raw"
            
            mask, crop_box = get_image_prepare_material(frame, [x1, y1, x2, y2], fp=self.fp, mode=mode)
            mask_coords_list_cycle.append(crop_box)
            mask_list_cycle.append(mask)
        
        # æ„å»ºæ¨¡æ¿æ•°æ®
        template_data = {
            'template_id': template_id,
            'template_path': template_path,
            'frame_list_cycle': frame_list_cycle,
            'coord_list_cycle': coord_list_cycle,
            'input_latent_list_cycle': input_latent_list_cycle,
            'mask_coords_list_cycle': mask_coords_list_cycle,
            'mask_list_cycle': mask_list_cycle,
            'preprocessed_at': time.time()
        }
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_file, 'wb') as f:
            pickle.dump(template_data, f)
        
        return template_data
    
    def inference_parallel(self, template_id, audio_path, output_path, fps=25):
        """å¹¶è¡Œæ¨ç† - 4GPUååŒå·¥ä½œ"""
        # å¦‚æœæ¨¡æ¿æœªé¢„å¤„ç†ï¼ŒåŠ¨æ€é¢„å¤„ç†
        if template_id not in self.templates:
            print(f"ğŸ”„ æ¨¡æ¿ {template_id} æœªé¢„å¤„ç†ï¼Œå¼€å§‹åŠ¨æ€é¢„å¤„ç†...")
            template_path = self._find_template_path(template_id)
            if not template_path:
                raise ValueError(f"æ¨¡æ¿ {template_id} æ–‡ä»¶æœªæ‰¾åˆ°")
            
            template_data = self._preprocess_single_template(template_id, template_path)
            self.templates[template_id] = template_data
            print(f"âœ… æ¨¡æ¿ {template_id} åŠ¨æ€é¢„å¤„ç†å®Œæˆ")
        
        template_data = self.templates[template_id]
        
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ¨ç†: æ¨¡æ¿={template_id}, éŸ³é¢‘={audio_path}")
        start_time = time.time()
        
        # 1. éŸ³é¢‘ç‰¹å¾æå–ï¼ˆä½¿ç”¨GPU 0ï¼‰
        print("ğŸµ æå–éŸ³é¢‘ç‰¹å¾...")
        audio_start = time.time()
        
        device = self.models[0]['device']
        whisper = self.models[0]['whisper']
        weight_dtype = self.models[0]['unet'].model.dtype
        
        whisper_input_features, librosa_length = self.audio_processor.get_audio_feature(
            audio_path, weight_dtype=weight_dtype
        )
        
        whisper_chunks = self.audio_processor.get_whisper_chunk(
            whisper_input_features,
            device,
            weight_dtype,
            whisper,
            librosa_length,
            fps=fps,
            audio_padding_length_left=self.config.audio_padding_length_left,
            audio_padding_length_right=self.config.audio_padding_length_right,
        )
        
        audio_time = time.time() - audio_start
        print(f"âœ… éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {audio_time:.2f}s, å…± {len(whisper_chunks)} å¸§")
        
        # 2. å¹¶è¡ŒGPUæ¨ç†
        print("ğŸ® å¼€å§‹4GPUå¹¶è¡Œæ¨ç†...")
        inference_start = time.time()
        
        # å°†ä»»åŠ¡åˆ†é…ç»™4ä¸ªGPU
        video_num = len(whisper_chunks)
        batch_size = self.config.batch_size
        
        # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
        gen = datagen(
            whisper_chunks,
            template_data['input_latent_list_cycle'],
            batch_size
        )
        
        # ä½¿ç”¨é˜Ÿåˆ—è¿›è¡ŒGPUé—´é€šä¿¡
        task_queue = queue.Queue()
        result_queue = queue.Queue()
        
        # å°†æ‰€æœ‰æ‰¹æ¬¡åŠ å…¥ä»»åŠ¡é˜Ÿåˆ—
        batch_list = list(gen)
        for i, (whisper_batch, latent_batch) in enumerate(batch_list):
            task_queue.put((i, whisper_batch, latent_batch))
        
        # å¯åŠ¨GPUå·¥ä½œçº¿ç¨‹
        gpu_threads = []
        for gpu_id in range(self.device_count):
            thread = threading.Thread(
                target=self._gpu_worker,
                args=(gpu_id, task_queue, result_queue)
            )
            thread.start()
            gpu_threads.append(thread)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        task_queue.join()
        
        # æ”¶é›†ç»“æœ
        results = {}
        for _ in range(len(batch_list)):
            batch_idx, batch_results = result_queue.get()
            results[batch_idx] = batch_results
        
        # æŒ‰é¡ºåºç»„è£…ç»“æœ
        res_frame_list = []
        for i in range(len(batch_list)):
            if i in results:
                res_frame_list.extend(results[i])
        
        inference_time = time.time() - inference_start
        print(f"âœ… 4GPUå¹¶è¡Œæ¨ç†å®Œæˆ: {inference_time:.2f}s")
        
        # 3. åå¤„ç†å’Œè§†é¢‘åˆæˆ
        print("ğŸ¬ å¼€å§‹è§†é¢‘åˆæˆ...")
        postprocess_start = time.time()
        
        self._postprocess_and_save(
            template_data, res_frame_list, audio_path, output_path, fps
        )
        
        postprocess_time = time.time() - postprocess_start
        total_time = time.time() - start_time
        
        print(f"ğŸ‰ æ¨ç†å®Œæˆ!")
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"   éŸ³é¢‘å¤„ç†: {audio_time:.2f}s")
        print(f"   GPUæ¨ç†: {inference_time:.2f}s")
        print(f"   åå¤„ç†: {postprocess_time:.2f}s")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        print(f"   è§†é¢‘é•¿åº¦: {len(res_frame_list)/fps:.1f}s")
        print(f"   å®æ—¶ç‡: {(len(res_frame_list)/fps)/total_time:.2f}x")
        
        return output_path
    
    def _gpu_worker(self, gpu_id, task_queue, result_queue):
        """GPUå·¥ä½œçº¿ç¨‹"""
        models = self.models[gpu_id]
        device = models['device']
        
        print(f"ğŸ® GPU {gpu_id} å·¥ä½œçº¿ç¨‹å¯åŠ¨")
        
        while True:
            try:
                # è·å–ä»»åŠ¡ï¼ˆè¶…æ—¶1ç§’ï¼‰
                batch_idx, whisper_batch, latent_batch = task_queue.get(timeout=1)
                
                # æ‰§è¡Œæ¨ç†
                with torch.no_grad():
                    # éŸ³é¢‘ç‰¹å¾ç¼–ç 
                    audio_feature_batch = models['pe'](whisper_batch.to(device))
                    latent_batch = latent_batch.to(device=device, dtype=models['unet'].model.dtype)
                    
                    # UNetæ¨ç†
                    pred_latents = models['unet'].model(
                        latent_batch,
                        models['timesteps'],
                        encoder_hidden_states=audio_feature_batch
                    ).sample
                    
                    # VAEè§£ç 
                    pred_latents = pred_latents.to(device=device, dtype=models['vae'].vae.dtype)
                    recon = models['vae'].decode_latents(pred_latents)
                
                # è¿”å›ç»“æœ
                result_queue.put((batch_idx, recon))
                task_queue.task_done()
                
            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºçº¿ç¨‹
                break
            except Exception as e:
                print(f"âŒ GPU {gpu_id} å¤„ç†é”™è¯¯: {e}")
                task_queue.task_done()
        
        print(f"ğŸ›‘ GPU {gpu_id} å·¥ä½œçº¿ç¨‹ç»“æŸ")
    
    def _find_template_path(self, template_id):
        """æŸ¥æ‰¾æ¨¡æ¿æ–‡ä»¶è·¯å¾„"""
        template_dir = getattr(self.config, 'template_dir', './templates')
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.webp']
        
        for ext in extensions:
            template_path = os.path.join(template_dir, f"{template_id}{ext}")
            if os.path.exists(template_path):
                return template_path
        
        return None
    
    def _postprocess_and_save(self, template_data, res_frame_list, audio_path, output_path, fps):
        """åå¤„ç†å’Œä¿å­˜è§†é¢‘"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.dirname(output_path)
        os.makedirs(output_dir, exist_ok=True)
        
        temp_dir = f"{output_dir}/temp_{int(time.time())}"
        os.makedirs(temp_dir, exist_ok=True)
        
        # åˆæˆæœ€ç»ˆå¸§
        print("ğŸ–¼ï¸ åˆæˆæœ€ç»ˆå¸§...")
        for i, res_frame in enumerate(tqdm(res_frame_list, desc="åˆæˆå¸§")):
            # è·å–å¯¹åº”çš„åŸå§‹å¸§å’Œåæ ‡
            cycle_idx = i % len(template_data['coord_list_cycle'])
            bbox = template_data['coord_list_cycle'][cycle_idx]
            ori_frame = copy.deepcopy(template_data['frame_list_cycle'][cycle_idx])
            
            x1, y1, x2, y2 = bbox
            
            try:
                # è°ƒæ•´ç”Ÿæˆå¸§å¤§å°
                res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                
                # è·å–maskå’Œåæ ‡
                mask = template_data['mask_list_cycle'][cycle_idx]
                mask_crop_box = template_data['mask_coords_list_cycle'][cycle_idx]
                
                # æ··åˆå›¾åƒ
                combine_frame = get_image_blending(ori_frame, res_frame, bbox, mask, mask_crop_box)
                
                # ä¿å­˜å¸§
                cv2.imwrite(f"{temp_dir}/{i:08d}.png", combine_frame)
                
            except Exception as e:
                print(f"âš ï¸ ç¬¬ {i} å¸§å¤„ç†å¤±è´¥: {e}")
                continue
        
        # ç”Ÿæˆè§†é¢‘
        print("ğŸ¬ ç”Ÿæˆè§†é¢‘æ–‡ä»¶...")
        temp_video = f"{temp_dir}/temp_video.mp4"
        cmd_img2video = f"ffmpeg -y -v warning -r {fps} -f image2 -i {temp_dir}/%08d.png -vcodec libx264 -vf format=yuv420p -crf 18 {temp_video}"
        os.system(cmd_img2video)
        
        # åˆå¹¶éŸ³é¢‘
        print("ğŸ”Š åˆå¹¶éŸ³é¢‘...")
        cmd_combine_audio = f"ffmpeg -y -v warning -i {audio_path} -i {temp_video} {output_path}"
        os.system(cmd_combine_audio)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)
        
        print(f"âœ… è§†é¢‘ä¿å­˜å®Œæˆ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description="MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–æ¨ç†")
    parser.add_argument("--config", type=str, default="configs/optimized_inference.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--template_id", type=str, required=True, help="æ¨¡æ¿ID")
    parser.add_argument("--audio_path", type=str, required=True, help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True, help="è¾“å‡ºè§†é¢‘è·¯å¾„")
    parser.add_argument("--fps", type=int, default=25, help="è§†é¢‘å¸§ç‡")
    
    # MuseTalkæ ‡å‡†å‚æ•°
    parser.add_argument("--version", type=str, default="v1", choices=["v1", "v15"])
    parser.add_argument("--unet_config", type=str, default="./models/musetalk/musetalk.json")
    parser.add_argument("--unet_model_path", type=str, default="./models/musetalk/pytorch_model.bin")
    parser.add_argument("--whisper_dir", type=str, default="./models/whisper")
    parser.add_argument("--vae_type", type=str, default="sd-vae")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å° - 4GPUä¼˜åŒ–")
    parser.add_argument("--bbox_shift", type=int, default=0)
    parser.add_argument("--extra_margin", type=int, default=10)
    parser.add_argument("--audio_padding_length_left", type=int, default=2)
    parser.add_argument("--audio_padding_length_right", type=int, default=2)
    parser.add_argument("--parsing_mode", default='jaw')
    parser.add_argument("--left_cheek_width", type=int, default=90)
    parser.add_argument("--right_cheek_width", type=int, default=90)
    parser.add_argument("--template_dir", type=str, default="./templates", help="æ¨¡æ¿ç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–æ¨ç†å™¨")
    print(f"ğŸ“Š é…ç½®å‚æ•°:")
    print(f"   æ¨¡æ¿ID: {args.template_id}")
    print(f"   éŸ³é¢‘æ–‡ä»¶: {args.audio_path}")
    print(f"   è¾“å‡ºè·¯å¾„: {args.output_path}")
    print(f"   æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    inference_engine = OptimizedMuseTalkInference(args)
    
    # æ‰§è¡Œæ¨ç†
    result_path = inference_engine.inference_parallel(
        template_id=args.template_id,
        audio_path=args.audio_path,
        output_path=args.output_path,
        fps=args.fps
    )
    
    print(f"ğŸ‰ æ¨ç†å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {result_path}")

if __name__ == "__main__":
    main()
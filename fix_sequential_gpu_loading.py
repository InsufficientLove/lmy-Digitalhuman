#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
from pathlib import Path
import shutil
from datetime import datetime

def backup_original_file(file_path):
    """å¤‡ä»½åŸå§‹æ–‡ä»¶"""
    backup_path = f"{file_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    shutil.copy2(file_path, backup_path)
    print(f"âœ… åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")
    return backup_path

def fix_ultra_fast_service():
    """ä¿®å¤Ultra Fast Serviceçš„å¹¶è¡ŒåŠ è½½é—®é¢˜"""
    print("MuseTalk Ultra Fast Service å¹¶è¡ŒåŠ è½½ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    script_dir = Path(__file__).parent
    service_file = script_dir / "MuseTalkEngine" / "ultra_fast_realtime_inference_v2.py"
    
    if not service_file.exists():
        print(f"âŒ æœåŠ¡æ–‡ä»¶ä¸å­˜åœ¨: {service_file}")
        return False
    
    print(f"âœ… æ‰¾åˆ°æœåŠ¡æ–‡ä»¶: {service_file}")
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_path = backup_original_file(service_file)
    
    try:
        # è¯»å–åŸæ–‡ä»¶å†…å®¹
        with open(service_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¿®å¤è¿‡
        if "# SEQUENTIAL_LOADING_FIXED" in content:
            print("âœ… æ–‡ä»¶å·²ç»ä¿®å¤è¿‡ï¼Œæ— éœ€é‡å¤ä¿®å¤")
            return True
        
        # æ›¿æ¢å¹¶è¡Œåˆå§‹åŒ–ä¸ºé¡ºåºåˆå§‹åŒ–
        old_parallel_code = '''            # çœŸæ­£çš„å¹¶è¡Œåˆå§‹åŒ– - å…è®¸éƒ¨åˆ†GPUå¤±è´¥
            print(f"å¼€å§‹å¹¶è¡Œåˆå§‹åŒ–{self.gpu_count}ä¸ªGPU...")
            successful_gpus = []
            with ThreadPoolExecutor(max_workers=self.gpu_count) as executor:
                futures = {executor.submit(init_gpu_model, i): i for i in range(self.gpu_count)}
                
                for future in as_completed(futures, timeout=300):  # 5åˆ†é’Ÿè¶…æ—¶
                    gpu_id = futures[future]
                    try:
                        result = future.result()
                        if result is not None:
                            successful_gpus.append(gpu_id)
                            print(f"GPU{gpu_id} å°±ç»ª ({len(successful_gpus)}/{self.gpu_count})")
                        else:
                            print(f"GPU{gpu_id} åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡")
                    except Exception as e:
                        print(f"GPU{gpu_id} åˆå§‹åŒ–å¼‚å¸¸: {e}")'''
        
        new_sequential_code = '''            # SEQUENTIAL_LOADING_FIXED: é¡ºåºåˆå§‹åŒ–é¿å…å¹¶å‘å†²çª
            print(f"å¼€å§‹é¡ºåºåˆå§‹åŒ–{self.gpu_count}ä¸ªGPUï¼ˆé¿å…å¹¶å‘å†²çªï¼‰...")
            successful_gpus = []
            
            for i in range(self.gpu_count):
                print(f"æ­£åœ¨åˆå§‹åŒ–GPU {i}/{self.gpu_count}...")
                try:
                    # åœ¨æ¯ä¸ªGPUåˆå§‹åŒ–å‰æ¸…ç†å†…å­˜
                    torch.cuda.set_device(i)
                    torch.cuda.empty_cache()
                    
                    result = init_gpu_model(i)
                    if result is not None:
                        successful_gpus.append(i)
                        print(f"âœ… GPU{i} åˆå§‹åŒ–æˆåŠŸ ({len(successful_gpus)}/{self.gpu_count})")
                    else:
                        print(f"âŒ GPU{i} åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡")
                except Exception as e:
                    print(f"âŒ GPU{i} åˆå§‹åŒ–å¼‚å¸¸: {e}")
                    # å¦‚æœæ˜¯meta tensoré”™è¯¯ï¼Œå°è¯•é‡è¯•ä¸€æ¬¡
                    if "meta tensor" in str(e) or "Cannot copy out" in str(e):
                        print(f"æ£€æµ‹åˆ°meta tensoré”™è¯¯ï¼Œæ¸…ç†å†…å­˜åé‡è¯•GPU{i}...")
                        try:
                            torch.cuda.empty_cache()
                            import gc
                            gc.collect()
                            result = init_gpu_model(i)
                            if result is not None:
                                successful_gpus.append(i)
                                print(f"âœ… GPU{i} é‡è¯•æˆåŠŸ")
                            else:
                                print(f"âŒ GPU{i} é‡è¯•å¤±è´¥")
                        except Exception as retry_e:
                            print(f"âŒ GPU{i} é‡è¯•å¼‚å¸¸: {retry_e}")'''
        
        # æ‰§è¡Œæ›¿æ¢
        if old_parallel_code in content:
            content = content.replace(old_parallel_code, new_sequential_code)
            print("âœ… å·²å°†å¹¶è¡Œåˆå§‹åŒ–æ›¿æ¢ä¸ºé¡ºåºåˆå§‹åŒ–")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°é¢„æœŸçš„å¹¶è¡Œåˆå§‹åŒ–ä»£ç ï¼Œå°è¯•å…¶ä»–æ›¿æ¢æ–¹å¼...")
            
            # å°è¯•æ›¿æ¢å…³é”®éƒ¨åˆ†
            if "with ThreadPoolExecutor(max_workers=self.gpu_count) as executor:" in content:
                # æ‰¾åˆ°å¹¶æ›¿æ¢ThreadPoolExecutoréƒ¨åˆ†
                lines = content.split('\n')
                new_lines = []
                in_executor_block = False
                indent_level = 0
                
                for line in lines:
                    if "with ThreadPoolExecutor(max_workers=self.gpu_count) as executor:" in line:
                        # æ›¿æ¢ä¸ºé¡ºåºåˆå§‹åŒ–
                        indent = len(line) - len(line.lstrip())
                        new_lines.append(' ' * indent + '# SEQUENTIAL_LOADING_FIXED: æ”¹ä¸ºé¡ºåºåˆå§‹åŒ–')
                        new_lines.append(' ' * indent + 'for i in range(self.gpu_count):')
                        new_lines.append(' ' * indent + '    print(f"æ­£åœ¨åˆå§‹åŒ–GPU {i}/{self.gpu_count}...")')
                        new_lines.append(' ' * indent + '    try:')
                        new_lines.append(' ' * indent + '        torch.cuda.set_device(i)')
                        new_lines.append(' ' * indent + '        torch.cuda.empty_cache()')
                        new_lines.append(' ' * indent + '        result = init_gpu_model(i)')
                        new_lines.append(' ' * indent + '        if result is not None:')
                        new_lines.append(' ' * indent + '            successful_gpus.append(i)')
                        new_lines.append(' ' * indent + '            print(f"âœ… GPU{i} å°±ç»ª ({len(successful_gpus)}/{self.gpu_count})")')
                        new_lines.append(' ' * indent + '        else:')
                        new_lines.append(' ' * indent + '            print(f"âŒ GPU{i} åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡")')
                        new_lines.append(' ' * indent + '    except Exception as e:')
                        new_lines.append(' ' * indent + '        print(f"âŒ GPU{i} åˆå§‹åŒ–å¼‚å¸¸: {e}")')
                        new_lines.append(' ' * indent + '        if "meta tensor" in str(e):')
                        new_lines.append(' ' * indent + '            print(f"æ£€æµ‹åˆ°meta tensoré”™è¯¯ï¼Œæ¸…ç†å†…å­˜åé‡è¯•...")')
                        new_lines.append(' ' * indent + '            torch.cuda.empty_cache()')
                        new_lines.append(' ' * indent + '            import gc; gc.collect()')
                        in_executor_block = True
                        indent_level = indent
                        continue
                    
                    if in_executor_block:
                        # è·³è¿‡executorå—å†…çš„å†…å®¹
                        current_indent = len(line) - len(line.lstrip()) if line.strip() else indent_level + 4
                        if current_indent <= indent_level and line.strip():
                            in_executor_block = False
                            new_lines.append(line)
                        # ç»§ç»­è·³è¿‡executorå—å†…å®¹
                    else:
                        new_lines.append(line)
                
                content = '\n'.join(new_lines)
                print("âœ… å·²ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æ›¿æ¢å¹¶è¡Œåˆå§‹åŒ–")
        
        # æ·»åŠ å¯¼å…¥å¿…è¦æ¨¡å—
        if "import gc" not in content:
            # åœ¨å…¶ä»–importè¯­å¥é™„è¿‘æ·»åŠ gcå¯¼å…¥
            content = content.replace("import warnings", "import warnings\nimport gc")
        
        # å†™å…¥ä¿®å¤åçš„æ–‡ä»¶
        with open(service_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("âœ… æ–‡ä»¶ä¿®å¤å®Œæˆ")
        print(f"å¤‡ä»½æ–‡ä»¶: {backup_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        # æ¢å¤å¤‡ä»½
        if backup_path and os.path.exists(backup_path):
            shutil.copy2(backup_path, service_file)
            print("å·²æ¢å¤åŸæ–‡ä»¶")
        return False

def create_single_gpu_config():
    """åˆ›å»ºå•GPUé…ç½®æ–‡ä»¶"""
    print("\n=== åˆ›å»ºå•GPUé…ç½® ===")
    
    script_dir = Path(__file__).parent
    config_file = script_dir / "single_gpu_config.py"
    
    config_content = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•GPUé…ç½® - å¦‚æœ4GPUå¹¶è¡Œä»æœ‰é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨æ­¤é…ç½®
"""

import os
import sys

def modify_for_single_gpu():
    """ä¿®æ”¹ä¸ºå•GPUé…ç½®"""
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    print("å·²è®¾ç½®ä¸ºå•GPUæ¨¡å¼ (GPU 0)")
    print("é‡å¯æœåŠ¡åå°†åªä½¿ç”¨ç¬¬ä¸€ä¸ªGPU")

if __name__ == "__main__":
    modify_for_single_gpu()
'''
    
    with open(config_file, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"âœ… å•GPUé…ç½®å·²åˆ›å»º: {config_file}")
    print("å¦‚æœ4GPUé¡ºåºåŠ è½½ä»æœ‰é—®é¢˜ï¼Œå¯ä»¥è¿è¡Œæ­¤è„šæœ¬åˆ‡æ¢åˆ°å•GPUæ¨¡å¼")

def main():
    print("å¼€å§‹ä¿®å¤MuseTalk GPUå¹¶è¡ŒåŠ è½½é—®é¢˜...")
    
    if fix_ultra_fast_service():
        print("\nğŸ‰ ä¿®å¤å®Œæˆï¼")
        print("\nå»ºè®®æ­¥éª¤:")
        print("1. é‡æ–°å¯åŠ¨MuseTalkæœåŠ¡")
        print("2. è§‚å¯Ÿ4GPUæ˜¯å¦èƒ½æ­£å¸¸é¡ºåºåˆå§‹åŒ–")
        print("3. å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯ä»¥å°è¯•å•GPUæ¨¡å¼")
        
        # åˆ›å»ºå•GPUé…ç½®ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        create_single_gpu_config()
        
    else:
        print("\nâŒ ä¿®å¤å¤±è´¥")
        print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯æˆ–æ‰‹åŠ¨ä¿®æ”¹é…ç½®æ–‡ä»¶")
    
    print("\næŒ‰ä»»æ„é”®é€€å‡º...")
    try:
        input()
    except:
        pass

if __name__ == "__main__":
    main()
# MuseTalk Dockerå¿«é€Ÿéƒ¨ç½²æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿéƒ¨ç½²ï¼‰

### 1. å‰ç½®è¦æ±‚

- LinuxæœåŠ¡å™¨ï¼ˆUbuntu 20.04/22.04 æ¨èï¼‰
- NVIDIA GPUï¼ˆæ”¯æŒCUDA 11.8+ï¼‰
- Docker 20.10+
- NVIDIA Container Toolkit

### 2. ä¸€é”®éƒ¨ç½²

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd musetalk-docker

# èµ‹äºˆæ‰§è¡Œæƒé™
chmod +x deploy.sh

# åˆå§‹åŒ–ç¯å¢ƒå¹¶éƒ¨ç½²
./deploy.sh setup
./deploy.sh build
./deploy.sh deploy
```

å°±è¿™ä¹ˆç®€å•ï¼æœåŠ¡å·²ç»åœ¨è¿è¡Œäº†ã€‚

### 3. éªŒè¯éƒ¨ç½²

```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
./deploy.sh status

# è¿è¡Œå¿«é€Ÿæµ‹è¯•
./deploy.sh test

# æŸ¥çœ‹æ—¥å¿—
./deploy.sh logs -f
```

---

## ğŸ“¦ Dockeré•œåƒè¯´æ˜

### é•œåƒç‰¹æ€§

- **å®Œæ•´çš„Pythonç¯å¢ƒ**ï¼šåŒ…å«æ‰€æœ‰å¿…éœ€çš„PythonåŒ…å’Œä¾èµ–
- **é¢„è£…PyTorch**ï¼šæ”¯æŒCUDA 11.8ï¼Œä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ ç¯å¢ƒ
- **è‡ªåŠ¨æ¨¡å‹ä¸‹è½½**ï¼šé¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨ä¸‹è½½æ‰€éœ€æ¨¡å‹
- **å¤šè¿è¡Œæ¨¡å¼**ï¼šæ”¯æŒæ¨ç†ã€APIã€Webç•Œé¢ç­‰å¤šç§æ¨¡å¼
- **å¥åº·æ£€æŸ¥**ï¼šå†…ç½®å¥åº·æ£€æŸ¥ï¼Œç¡®ä¿æœåŠ¡ç¨³å®šè¿è¡Œ

### é•œåƒå¤§å°

- åŸºç¡€é•œåƒï¼šçº¦8GB
- åŒ…å«æ¨¡å‹ï¼šçº¦15GBï¼ˆæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°æŒä¹…å·ï¼‰

---

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

ç¼–è¾‘ `.env` æ–‡ä»¶æ¥é…ç½®æœåŠ¡ï¼š

```bash
# GPUé…ç½®
CUDA_VISIBLE_DEVICES=0,1,2,3  # ä½¿ç”¨å“ªäº›GPU
GPU_CONFIG_MODE=auto           # auto/single_gpu/multi_gpu
BATCH_SIZE_PER_GPU=8          # æ¯ä¸ªGPUçš„æ‰¹å¤„ç†å¤§å°

# è¿è¡Œæ¨¡å¼
RUN_MODE=inference             # inference/benchmark/gradio/api

# æ€§èƒ½ä¼˜åŒ–
ENABLE_AMP=true               # å¯ç”¨æ··åˆç²¾åº¦
ENABLE_CUDNN_BENCHMARK=true   # CuDNNä¼˜åŒ–

# æœåŠ¡ç«¯å£
API_PORT=8000                 # APIæœåŠ¡ç«¯å£
WEB_PORT=7860                 # Webç•Œé¢ç«¯å£
```

### è¿è¡Œæ¨¡å¼

Dockerå®¹å™¨æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼ï¼š

1. **æ¨ç†æ¨¡å¼**ï¼ˆé»˜è®¤ï¼‰
```bash
docker run --rm --runtime=nvidia --gpus all \
  -e RUN_MODE=inference \
  musetalk:gpu-latest
```

2. **Gradio Webç•Œé¢**
```bash
docker run -d --runtime=nvidia --gpus all \
  -e RUN_MODE=gradio \
  -p 7860:7860 \
  musetalk:gpu-latest
```

3. **APIæœåŠ¡**
```bash
docker run -d --runtime=nvidia --gpus all \
  -e RUN_MODE=api \
  -p 8000:8000 \
  musetalk:gpu-latest
```

4. **äº¤äº’å¼Shell**
```bash
docker run -it --runtime=nvidia --gpus all \
  -e RUN_MODE=shell \
  musetalk:gpu-latest
```

---

## ğŸ–¥ï¸ å¤šæœåŠ¡å™¨éƒ¨ç½²

### æ–¹æ³•1ï¼šä½¿ç”¨Docker Hub

```bash
# åœ¨ç¬¬ä¸€å°æœåŠ¡å™¨ä¸Šæ„å»ºå¹¶æ¨é€
./deploy.sh build
docker tag musetalk:gpu-latest your-dockerhub/musetalk:gpu-latest
docker push your-dockerhub/musetalk:gpu-latest

# åœ¨å…¶ä»–æœåŠ¡å™¨ä¸Šæ‹‰å–å¹¶è¿è¡Œ
docker pull your-dockerhub/musetalk:gpu-latest
docker-compose -f docker-compose.gpu.yml up -d
```

### æ–¹æ³•2ï¼šä½¿ç”¨ç§æœ‰Registry

```bash
# å¯åŠ¨ç§æœ‰Registry
docker run -d -p 5000:5000 --name registry registry:2

# æ ‡è®°å¹¶æ¨é€
docker tag musetalk:gpu-latest localhost:5000/musetalk:gpu-latest
docker push localhost:5000/musetalk:gpu-latest

# åœ¨å…¶ä»–æœåŠ¡å™¨ä¸Š
docker pull <registry-server>:5000/musetalk:gpu-latest
```

### æ–¹æ³•3ï¼šå¯¼å‡ºé•œåƒæ–‡ä»¶

```bash
# å¯¼å‡ºé•œåƒ
docker save musetalk:gpu-latest | gzip > musetalk-gpu.tar.gz

# ä¼ è¾“åˆ°å…¶ä»–æœåŠ¡å™¨
scp musetalk-gpu.tar.gz user@server:/path/

# åœ¨ç›®æ ‡æœåŠ¡å™¨å¯¼å…¥
docker load < musetalk-gpu.tar.gz
```

---

## ğŸ“Š æ€§èƒ½æµ‹è¯•

### å•GPUæµ‹è¯•ï¼ˆRTX 4090D 48GBï¼‰

```bash
docker run --rm --runtime=nvidia --gpus '"device=0"' \
  -e GPU_CONFIG_MODE=single_gpu \
  -e BATCH_SIZE_PER_GPU=20 \
  musetalk:gpu-latest \
  python /app/benchmark_gpu.py --config single_4090d_48gb
```

### å¤šGPUæµ‹è¯•ï¼ˆ4x RTX 4090 24GBï¼‰

```bash
docker run --rm --runtime=nvidia --gpus all \
  -e CUDA_VISIBLE_DEVICES=0,1,2,3 \
  -e GPU_CONFIG_MODE=multi_gpu \
  -e BATCH_SIZE_PER_GPU=8 \
  musetalk:gpu-latest \
  python /app/benchmark_gpu.py --config quad_4090_24gb
```

### æ€§èƒ½ç›‘æ§

```bash
# å¯åŠ¨ç›‘æ§æœåŠ¡
./deploy.sh monitor

# è®¿é—®ç›‘æ§é¢æ¿
# Grafana: http://localhost:3000 (admin/admin)
# Prometheus: http://localhost:9091
```

---

## ğŸ” æ•…éšœæ’é™¤

### 1. Dockeræ— æ³•è®¿é—®GPU

```bash
# æ£€æŸ¥NVIDIAè¿è¡Œæ—¶
docker info | grep nvidia

# æµ‹è¯•GPUè®¿é—®
docker run --rm --runtime=nvidia --gpus all \
  nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### 2. æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
docker exec -it musetalk-gpu-service python /app/download_models.py

# æˆ–ä½¿ç”¨å›½å†…é•œåƒ
docker exec -it musetalk-gpu-service bash
export HF_ENDPOINT=https://hf-mirror.com
python /app/download_models.py
```

### 3. å†…å­˜ä¸è¶³

```bash
# è°ƒæ•´æ‰¹å¤„ç†å¤§å°
echo "BATCH_SIZE_PER_GPU=4" >> .env
./deploy.sh restart

# æ¸…ç†GPUå†…å­˜
docker exec musetalk-gpu-service python -c "
import torch
torch.cuda.empty_cache()
"
```

### 4. å®¹å™¨å¯åŠ¨å¤±è´¥

```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker logs musetalk-gpu-service --tail 100

# è¿›å…¥è°ƒè¯•æ¨¡å¼
docker run -it --rm --runtime=nvidia --gpus all \
  --entrypoint /bin/bash \
  musetalk:gpu-latest
```

---

## ğŸ“ˆ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 1. ä½¿ç”¨Docker Swarm

```bash
# åˆå§‹åŒ–Swarm
docker swarm init

# éƒ¨ç½²æœåŠ¡
docker stack deploy -c docker-compose.gpu.yml musetalk

# æ‰©å±•æœåŠ¡
docker service scale musetalk_gpu=3
```

### 2. ä½¿ç”¨Kubernetes

```yaml
# musetalk-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: musetalk-gpu
spec:
  replicas: 2
  selector:
    matchLabels:
      app: musetalk
  template:
    metadata:
      labels:
        app: musetalk
    spec:
      containers:
      - name: musetalk
        image: musetalk:gpu-latest
        resources:
          limits:
            nvidia.com/gpu: 1
```

### 3. è´Ÿè½½å‡è¡¡

```nginx
# nginx.conf
upstream musetalk_backend {
    server musetalk-gpu-1:8000;
    server musetalk-gpu-2:8000;
    server musetalk-gpu-3:8000;
}

server {
    listen 80;
    location / {
        proxy_pass http://musetalk_backend;
    }
}
```

---

## ğŸ› ï¸ ç»´æŠ¤æ“ä½œ

### å¤‡ä»½å’Œæ¢å¤

```bash
# å¤‡ä»½æ¨¡å‹å’Œé…ç½®
./deploy.sh backup

# æ¢å¤å¤‡ä»½
./deploy.sh restore backups/20240101_120000
```

### æ›´æ–°é•œåƒ

```bash
# æ‹‰å–æœ€æ–°ä»£ç 
git pull

# é‡æ–°æ„å»ºé•œåƒ
./deploy.sh build --no-cache

# æ›´æ–°æœåŠ¡
./deploy.sh deploy
```

### æ¸…ç†èµ„æº

```bash
# åœæ­¢æœåŠ¡
./deploy.sh stop

# æ¸…ç†æ‰€æœ‰å®¹å™¨å’Œå·ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
./deploy.sh clean
```

---

## ğŸ“ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

```bash
# éƒ¨ç½²ç®¡ç†
./deploy.sh setup          # åˆå§‹åŒ–ç¯å¢ƒ
./deploy.sh build          # æ„å»ºé•œåƒ
./deploy.sh deploy         # éƒ¨ç½²æœåŠ¡
./deploy.sh start          # å¯åŠ¨æœåŠ¡
./deploy.sh stop           # åœæ­¢æœåŠ¡
./deploy.sh restart        # é‡å¯æœåŠ¡
./deploy.sh status         # æŸ¥çœ‹çŠ¶æ€

# è°ƒè¯•å’Œæµ‹è¯•
./deploy.sh logs -f        # æŸ¥çœ‹æ—¥å¿—
./deploy.sh shell          # è¿›å…¥å®¹å™¨
./deploy.sh test           # è¿è¡Œæµ‹è¯•
./deploy.sh benchmark      # æ€§èƒ½æµ‹è¯•

# ç›‘æ§å’Œç»´æŠ¤
./deploy.sh monitor        # å¯åŠ¨ç›‘æ§
./deploy.sh backup         # å¤‡ä»½æ•°æ®
./deploy.sh clean          # æ¸…ç†èµ„æº
```

---

## ğŸ¤ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. Dockerç‰ˆæœ¬ï¼š`docker --version`
2. GPUä¿¡æ¯ï¼š`nvidia-smi`
3. é”™è¯¯æ—¥å¿—ï¼š`./deploy.sh logs --tail 100`
4. ç¯å¢ƒé…ç½®ï¼š`.env`æ–‡ä»¶å†…å®¹

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMuseTalkå¼€æºé¡¹ç›®ï¼Œè¯·éµå®ˆç›¸åº”çš„å¼€æºåè®®ã€‚
#!/usr/bin/env python3
"""
GPUè¯Šæ–­è„šæœ¬ - ç¡®ä¿MuseTalkå¯ä»¥æ­£ç¡®ä½¿ç”¨GPU
"""

import os
import sys
import torch
import subprocess

def diagnose_cuda_environment():
    """è¯Šæ–­CUDAç¯å¢ƒ"""
    print("ğŸ” CUDAç¯å¢ƒè¯Šæ–­")
    print("=" * 50)
    
    # 1. æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDAå¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")
    
    if not cuda_available:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥:")
        print("   1. NVIDIAé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("   2. CUDAå·¥å…·åŒ…æ˜¯å¦å®‰è£…")
        print("   3. PyTorchæ˜¯å¦ä¸ºCUDAç‰ˆæœ¬")
        return False
    
    # 2. æ£€æŸ¥GPUæ•°é‡å’Œä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"GPUæ•°é‡: {gpu_count}")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # 3. æ£€æŸ¥å½“å‰GPU
    current_device = torch.cuda.current_device()
    print(f"å½“å‰GPU: {current_device}")
    
    # 4. æµ‹è¯•GPUå†…å­˜åˆ†é…
    try:
        test_tensor = torch.randn(1000, 1000).cuda()
        print("âœ… GPUå†…å­˜åˆ†é…æµ‹è¯•é€šè¿‡")
        del test_tensor
        torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ GPUå†…å­˜åˆ†é…å¤±è´¥: {e}")
        return False
    
    return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½åˆ°GPU"""
    print("\nğŸ§ª æ¨¡å‹GPUåŠ è½½æµ‹è¯•")
    print("=" * 50)
    
    try:
        # æµ‹è¯•ç®€å•æ¨¡å‹åŠ è½½
        import torch.nn as nn
        
        model = nn.Linear(100, 10)
        model = model.cuda()
        print("âœ… ç®€å•æ¨¡å‹GPUåŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•æ¨ç†
        input_tensor = torch.randn(32, 100).cuda()
        output = model(input_tensor)
        print("âœ… GPUæ¨ç†æµ‹è¯•é€šè¿‡")
        
        del model, input_tensor, output
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹GPUåŠ è½½å¤±è´¥: {e}")
        return False
    
    return True

def test_musetalk_dependencies():
    """æµ‹è¯•MuseTalkä¾èµ–çš„GPUå…¼å®¹æ€§"""
    print("\nğŸ“¦ MuseTalkä¾èµ–GPUå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•mmpose (DWPoseä¾èµ–)
    try:
        import mmpose
        print("âœ… mmposeå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ mmposeå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•diffusers (VAEä¾èµ–)
    try:
        from diffusers import AutoencoderKL
        print("âœ… diffuserså¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ diffuserså¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•whisper
    try:
        import whisper
        print("âœ… whisperå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ whisperå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True

def check_environment_variables():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®"""
    print("\nğŸŒ ç¯å¢ƒå˜é‡æ£€æŸ¥")
    print("=" * 50)
    
    important_vars = [
        "CUDA_VISIBLE_DEVICES",
        "PYTORCH_CUDA_ALLOC_CONF", 
        "TORCH_CUDNN_V8_API_ENABLED",
        "TORCH_BACKENDS_CUDNN_BENCHMARK"
    ]
    
    for var in important_vars:
        value = os.environ.get(var, "æœªè®¾ç½®")
        print(f"{var}: {value}")

def test_dwpose_gpu_loading():
    """ä¸“é—¨æµ‹è¯•DWPoseæ¨¡å‹GPUåŠ è½½"""
    print("\nğŸ¤– DWPose GPUåŠ è½½æµ‹è¯•")
    print("=" * 50)
    
    try:
        # æ¨¡æ‹ŸDWPoseæ¨¡å‹åŠ è½½è¿‡ç¨‹
        from mmpose.apis import init_model
        
        config_file = "./musetalk/utils/dwpose/rtmpose-l_8xb32-270e_coco-ubody-wholebody-384x288.py"
        checkpoint_file = "./models/dwpose/dw-ll_ucoco_384.pth"
        
        if not os.path.exists(config_file):
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            return False
            
        if not os.path.exists(checkpoint_file):
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_file}")
            return False
        
        # å°è¯•åŠ è½½æ¨¡å‹åˆ°GPU
        device = 'cuda:0'
        print(f"å°è¯•åŠ è½½DWPoseæ¨¡å‹åˆ° {device}...")
        
        model = init_model(config_file, checkpoint_file, device=device)
        print("âœ… DWPoseæ¨¡å‹GPUåŠ è½½æˆåŠŸ!")
        
        del model
        torch.cuda.empty_cache()
        return True
        
    except Exception as e:
        print(f"âŒ DWPoseæ¨¡å‹GPUåŠ è½½å¤±è´¥: {e}")
        print("è¿™æ˜¯å¯¼è‡´å›é€€åˆ°CPUçš„ä¸»è¦åŸå› !")
        return False

def suggest_fixes():
    """å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
    print("\nğŸ’¡ ä¿®å¤å»ºè®®")
    print("=" * 50)
    
    print("1. ç¡®ä¿CUDAç¯å¢ƒ:")
    print("   nvidia-smi  # æ£€æŸ¥GPUçŠ¶æ€")
    print("   python -c \"import torch; print(torch.cuda.is_available())\"")
    
    print("\n2. é‡æ–°å®‰è£…PyTorch CUDAç‰ˆæœ¬:")
    print("   pip uninstall torch torchvision torchaudio")
    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    print("\n3. è®¾ç½®ç¯å¢ƒå˜é‡:")
    print("   export CUDA_VISIBLE_DEVICES=0")
    print("   export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512")
    
    print("\n4. æ¸…ç†GPUå†…å­˜:")
    print("   python -c \"import torch; torch.cuda.empty_cache()\"")

def main():
    print("ğŸš€ MuseTalk GPUè¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    # æ‰§è¡Œæ‰€æœ‰è¯Šæ–­
    tests = [
        ("CUDAç¯å¢ƒ", diagnose_cuda_environment),
        ("æ¨¡å‹åŠ è½½", test_model_loading),
        ("ä¾èµ–å…¼å®¹æ€§", test_musetalk_dependencies),
        ("DWPose GPUåŠ è½½", test_dwpose_gpu_loading)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nğŸ§ª æ‰§è¡Œæµ‹è¯•: {test_name}")
        try:
            result = test_func()
            results.append((test_name, result))
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            print(f"ç»“æœ: {status}")
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    check_environment_variables()
    
    # è¾“å‡ºæ€»ç»“
    print("\nğŸ“Š è¯Šæ–­æ€»ç»“")
    print("=" * 50)
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ…" if result else "âŒ"
        print(f"{status} {test_name}")
    
    print(f"\né€šè¿‡ç‡: {passed}/{total} ({passed/total*100:.1f}%)")
    
    if passed < total:
        suggest_fixes()
    else:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! GPUç¯å¢ƒé…ç½®æ­£ç¡®!")

if __name__ == "__main__":
    main()
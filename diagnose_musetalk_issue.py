#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import torch
import json
from pathlib import Path
import subprocess
import platform

class MuseTalkDiagnostics:
    def __init__(self):
        self.script_dir = Path(__file__).parent
        self.musetalk_path = self.script_dir / "MuseTalk"
        self.models_path = self.musetalk_path / "models"
        
        # Expected model files and their properties
        self.expected_models = {
            "unet": {
                "path": self.models_path / "musetalkV15" / "unet.pth",
                "config_path": self.models_path / "musetalkV15" / "musetalk.json",
                "description": "MuseTalk UNet Model",
                "expected_size_mb": (350, 450),  # Min, Max range
                "critical": True
            },
            "vae_bin": {
                "path": self.models_path / "sd-vae" / "diffusion_pytorch_model.bin",
                "config_path": self.models_path / "sd-vae" / "config.json",
                "description": "VAE Model (.bin)",
                "expected_size_mb": (300, 350),
                "critical": True
            },
            "vae_safetensors": {
                "path": self.models_path / "sd-vae" / "diffusion_pytorch_model.safetensors",
                "description": "VAE Model (.safetensors)",
                "expected_size_mb": (300, 350),
                "critical": False  # Alternative to .bin
            },
            "dwpose": {
                "path": self.models_path / "dwpose" / "dw-ll_ucoco_384.pth",
                "description": "DWPose Model",
                "expected_size_mb": (180, 220),
                "critical": True
            }
        }
        
        self.issues = []
        self.solutions = []
    
    def add_issue(self, issue_type, description, solution=None):
        """Add an issue and its solution"""
        self.issues.append({
            "type": issue_type,
            "description": description,
            "solution": solution
        })
    
    def check_python_environment(self):
        """Check Python environment and dependencies"""
        print("=== Pythonç¯å¢ƒæ£€æŸ¥ ===")
        
        # Check Python version
        python_version = sys.version
        print(f"Pythonç‰ˆæœ¬: {python_version}")
        
        # Check PyTorch
        try:
            import torch
            print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
            print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
            if torch.cuda.is_available():
                print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
                print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
                for i in range(torch.cuda.device_count()):
                    gpu_name = torch.cuda.get_device_name(i)
                    print(f"  GPU {i}: {gpu_name}")
        except ImportError:
            self.add_issue("dependency", "PyTorchæœªå®‰è£…", 
                          "è¿è¡Œ: pip install torch torchvision torchaudio")
        
        # Check other dependencies
        required_packages = ["requests", "numpy", "PIL", "cv2"]
        missing_packages = []
        
        for package in required_packages:
            try:
                if package == "PIL":
                    import PIL
                elif package == "cv2":
                    import cv2
                else:
                    __import__(package)
                print(f"âœ… {package} å·²å®‰è£…")
            except ImportError:
                missing_packages.append(package)
                print(f"âŒ {package} æœªå®‰è£…")
        
        if missing_packages:
            package_map = {"PIL": "Pillow", "cv2": "opencv-python"}
            install_packages = [package_map.get(pkg, pkg) for pkg in missing_packages]
            self.add_issue("dependency", f"ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_packages)}", 
                          f"è¿è¡Œ: pip install {' '.join(install_packages)}")
    
    def check_directory_structure(self):
        """Check MuseTalk directory structure"""
        print("\n=== ç›®å½•ç»“æ„æ£€æŸ¥ ===")
        
        if not self.musetalk_path.exists():
            print(f"âŒ MuseTalkç›®å½•ä¸å­˜åœ¨: {self.musetalk_path}")
            self.add_issue("structure", "MuseTalkç›®å½•ä¸å­˜åœ¨", 
                          "è¿è¡Œ: python setup_musetalk_models.py")
            return False
        
        print(f"âœ… MuseTalkç›®å½•å­˜åœ¨: {self.musetalk_path}")
        
        # Check models directory
        if not self.models_path.exists():
            print(f"âŒ modelsç›®å½•ä¸å­˜åœ¨: {self.models_path}")
            self.add_issue("structure", "modelsç›®å½•ä¸å­˜åœ¨", 
                          "è¿è¡Œ: python setup_musetalk_models.py")
            return False
        
        print(f"âœ… modelsç›®å½•å­˜åœ¨: {self.models_path}")
        
        # Check subdirectories
        required_dirs = ["musetalkV15", "sd-vae", "dwpose"]
        for dir_name in required_dirs:
            dir_path = self.models_path / dir_name
            if dir_path.exists():
                print(f"âœ… {dir_name}ç›®å½•å­˜åœ¨")
            else:
                print(f"âŒ {dir_name}ç›®å½•ä¸å­˜åœ¨")
                self.add_issue("structure", f"{dir_name}ç›®å½•ä¸å­˜åœ¨", 
                              "è¿è¡Œ: python setup_musetalk_models.py")
        
        return True
    
    def check_model_files(self):
        """Check model files integrity"""
        print("\n=== æ¨¡å‹æ–‡ä»¶æ£€æŸ¥ ===")
        
        critical_missing = []
        
        for model_name, model_info in self.expected_models.items():
            model_path = model_info["path"]
            description = model_info["description"]
            expected_size = model_info["expected_size_mb"]
            is_critical = model_info["critical"]
            
            print(f"\næ£€æŸ¥ {description}...")
            
            if not model_path.exists():
                status = "âŒ ç¼ºå¤±"
                if is_critical:
                    critical_missing.append(model_name)
                self.add_issue("model_missing", f"{description}æ–‡ä»¶ä¸å­˜åœ¨: {model_path}", 
                              "è¿è¡Œ: python setup_musetalk_models.py")
            else:
                # Check file size
                size_mb = model_path.stat().st_size / (1024 * 1024)
                min_size, max_size = expected_size
                
                if min_size <= size_mb <= max_size:
                    status = f"âœ… æ­£å¸¸ ({size_mb:.1f}MB)"
                else:
                    status = f"âš ï¸ å¤§å°å¼‚å¸¸ ({size_mb:.1f}MB, æœŸæœ›: {min_size}-{max_size}MB)"
                    self.add_issue("model_corrupt", f"{description}æ–‡ä»¶å¤§å°å¼‚å¸¸", 
                                  "é‡æ–°ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
            
            print(f"  {description}: {status}")
            
            # Check config files if specified
            if "config_path" in model_info and model_path.exists():
                config_path = model_info["config_path"]
                if config_path.exists():
                    print(f"  é…ç½®æ–‡ä»¶: âœ… å­˜åœ¨")
                else:
                    print(f"  é…ç½®æ–‡ä»¶: âŒ ç¼ºå¤±")
                    self.add_issue("config_missing", f"{description}é…ç½®æ–‡ä»¶ä¸å­˜åœ¨", 
                                  "è¿è¡Œ: python setup_musetalk_models.py")
        
        return len(critical_missing) == 0
    
    def test_model_loading(self):
        """Test model loading capabilities"""
        print("\n=== æ¨¡å‹åŠ è½½æµ‹è¯• ===")
        
        # Test PyTorch tensor operations
        try:
            print("æµ‹è¯•PyTorchåŸºæœ¬æ“ä½œ...")
            x = torch.randn(2, 3, 4, 4)
            y = torch.nn.functional.relu(x)
            print("âœ… PyTorchåŸºæœ¬æ“ä½œæ­£å¸¸")
        except Exception as e:
            print(f"âŒ PyTorchåŸºæœ¬æ“ä½œå¤±è´¥: {e}")
            self.add_issue("pytorch", "PyTorchåŸºæœ¬æ“ä½œå¤±è´¥", 
                          "é‡æ–°å®‰è£…PyTorch")
            return False
        
        # Test CUDA if available
        if torch.cuda.is_available():
            try:
                print("æµ‹è¯•CUDAæ“ä½œ...")
                x_cuda = torch.randn(2, 3, 4, 4).cuda()
                y_cuda = torch.nn.functional.relu(x_cuda)
                print("âœ… CUDAæ“ä½œæ­£å¸¸")
            except Exception as e:
                print(f"âŒ CUDAæ“ä½œå¤±è´¥: {e}")
                self.add_issue("cuda", "CUDAæ“ä½œå¤±è´¥", 
                              "æ£€æŸ¥CUDAé©±åŠ¨å’ŒPyTorch CUDAç‰ˆæœ¬å…¼å®¹æ€§")
        
        # Test model file loading
        unet_path = self.models_path / "musetalkV15" / "unet.pth"
        if unet_path.exists():
            try:
                print("æµ‹è¯•UNetæ¨¡å‹åŠ è½½...")
                model_data = torch.load(unet_path, map_location='cpu')
                print("âœ… UNetæ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸ")
                
                # Check for meta tensor issue
                if isinstance(model_data, dict):
                    for key, value in model_data.items():
                        if hasattr(value, 'is_meta') and value.is_meta:
                            print(f"âŒ å‘ç°meta tensor: {key}")
                            self.add_issue("meta_tensor", "UNetæ¨¡å‹åŒ…å«meta tensor", 
                                          "è¿è¡Œ: python fix_unet_model.py")
                            return False
                
            except Exception as e:
                print(f"âŒ UNetæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                if "Cannot copy out of meta tensor" in str(e):
                    self.add_issue("meta_tensor", "UNetæ¨¡å‹meta tensoré”™è¯¯", 
                                  "è¿è¡Œ: python fix_unet_model.py æˆ–é‡æ–°ä¸‹è½½æ¨¡å‹")
                else:
                    self.add_issue("model_corrupt", "UNetæ¨¡å‹æ–‡ä»¶æŸå", 
                                  "é‡æ–°ä¸‹è½½UNetæ¨¡å‹æ–‡ä»¶")
                return False
        
        return True
    
    def check_system_resources(self):
        """Check system resources"""
        print("\n=== ç³»ç»Ÿèµ„æºæ£€æŸ¥ ===")
        
        # Check disk space
        try:
            import shutil
            total, used, free = shutil.disk_usage(str(self.script_dir))
            free_gb = free / (1024**3)
            print(f"å¯ç”¨ç£ç›˜ç©ºé—´: {free_gb:.1f}GB")
            
            if free_gb < 2:
                self.add_issue("disk_space", "ç£ç›˜ç©ºé—´ä¸è¶³", 
                              "æ¸…ç†ç£ç›˜ç©ºé—´ï¼Œè‡³å°‘éœ€è¦2GB")
        except Exception as e:
            print(f"æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}")
        
        # Check memory (approximate)
        try:
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    memory_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    print(f"GPU {i} æ˜¾å­˜: {memory_total:.1f}GB")
                    
                    if memory_total < 4:
                        self.add_issue("gpu_memory", f"GPU {i} æ˜¾å­˜ä¸è¶³", 
                                      "MuseTalkå»ºè®®è‡³å°‘4GBæ˜¾å­˜")
        except Exception as e:
            print(f"æ— æ³•æ£€æŸ¥GPUæ˜¾å­˜: {e}")
    
    def generate_report(self):
        """Generate diagnostic report"""
        print("\n" + "="*50)
        print("=== è¯Šæ–­æŠ¥å‘Š ===")
        print("="*50)
        
        if not self.issues:
            print("ğŸ‰ æ­å–œï¼æ²¡æœ‰å‘ç°é—®é¢˜ï¼ŒMuseTalkåº”è¯¥å¯ä»¥æ­£å¸¸è¿è¡Œ")
            return True
        
        print(f"å‘ç° {len(self.issues)} ä¸ªé—®é¢˜:")
        
        # Group issues by type
        issue_groups = {}
        for issue in self.issues:
            issue_type = issue["type"]
            if issue_type not in issue_groups:
                issue_groups[issue_type] = []
            issue_groups[issue_type].append(issue)
        
        # Display issues by priority
        priority_order = ["structure", "model_missing", "meta_tensor", "model_corrupt", 
                         "config_missing", "dependency", "pytorch", "cuda", 
                         "disk_space", "gpu_memory"]
        
        for issue_type in priority_order:
            if issue_type in issue_groups:
                print(f"\nã€{issue_type.upper()}ã€‘")
                for issue in issue_groups[issue_type]:
                    print(f"  âŒ {issue['description']}")
                    if issue['solution']:
                        print(f"     ğŸ’¡ è§£å†³æ–¹æ¡ˆ: {issue['solution']}")
        
        # Generate action plan
        print("\n=== æ¨èè§£å†³æ­¥éª¤ ===")
        
        if any(i["type"] in ["structure", "model_missing"] for i in self.issues):
            print("1. é¦–å…ˆè¿è¡Œæ¨¡å‹è®¾ç½®è„šæœ¬:")
            print("   python setup_musetalk_models.py")
        
        if any(i["type"] == "meta_tensor" for i in self.issues):
            print("2. ä¿®å¤UNetæ¨¡å‹meta tensoré—®é¢˜:")
            print("   python fix_unet_model.py")
        
        if any(i["type"] == "dependency" for i in self.issues):
            print("3. å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…:")
            missing_deps = [i["solution"] for i in self.issues if i["type"] == "dependency"]
            for dep in missing_deps:
                print(f"   {dep}")
        
        if any(i["type"] in ["model_corrupt", "config_missing"] for i in self.issues):
            print("4. é‡æ–°ä¸‹è½½æŸåçš„æ¨¡å‹æ–‡ä»¶:")
            print("   python setup_musetalk_models.py")
        
        print("\nå¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†çš„ä¸‹è½½è¯´æ˜:")
        print("   cat MuseTalk/DOWNLOAD_INSTRUCTIONS.md")
        
        return False
    
    def run_full_diagnosis(self):
        """Run complete diagnosis"""
        print("MuseTalk é—®é¢˜è¯Šæ–­å·¥å…·")
        print("="*50)
        
        self.check_python_environment()
        self.check_directory_structure()
        self.check_model_files()
        self.test_model_loading()
        self.check_system_resources()
        
        return self.generate_report()

def main():
    diagnostics = MuseTalkDiagnostics()
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--models-only":
            diagnostics.check_directory_structure()
            diagnostics.check_model_files()
            diagnostics.generate_report()
        elif sys.argv[1] == "--env-only":
            diagnostics.check_python_environment()
            diagnostics.generate_report()
        else:
            print("ç”¨æ³•:")
            print("  python diagnose_musetalk_issue.py           # å®Œæ•´è¯Šæ–­")
            print("  python diagnose_musetalk_issue.py --models-only  # ä»…æ£€æŸ¥æ¨¡å‹")
            print("  python diagnose_musetalk_issue.py --env-only     # ä»…æ£€æŸ¥ç¯å¢ƒ")
    else:
        success = diagnostics.run_full_diagnosis()
        
        if success:
            print("\nâœ… è¯Šæ–­å®Œæˆï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½")
            sys.exit(0)
        else:
            print("\nâŒ å‘ç°é—®é¢˜ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°å»ºè®®è¿›è¡Œä¿®å¤")
            sys.exit(1)

if __name__ == "__main__":
    main()
using LmyDigitalHuman.Models;
using Microsoft.Extensions.Caching.Memory;
using System.Collections.Concurrent;
using System.Diagnostics;
using System.Text.Json;

namespace LmyDigitalHuman.Services
{
    public class ConversationService : IConversationService
    {
        private readonly IDigitalHumanTemplateService _templateService;
        private readonly IMuseTalkService _museTalkService;
        private readonly IAudioPipelineService _audioPipelineService;
        private readonly ILocalLLMService _llmService;
        private readonly IMemoryCache _cache;
        private readonly IPathManager _pathManager;
        private readonly ILogger<ConversationService> _logger;
        
        // å¹¶å‘å­—å…¸ç”¨äºç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡
        private readonly ConcurrentDictionary<string, ConversationContext> _conversationContexts = new();
        
        // ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡
        private readonly SemaphoreSlim _concurrencySemaphore;
        private readonly SemaphoreSlim _realtimeSemaphore;
        
        // æ€§èƒ½ç»Ÿè®¡
        private long _totalConversations = 0;
        private long _totalProcessingTime = 0;
        private readonly ConcurrentDictionary<string, long> _templateUsageCount = new();

        public ConversationService(
            IDigitalHumanTemplateService templateService,
            IMuseTalkService museTalkService,
            IAudioPipelineService audioPipelineService,
            ILocalLLMService llmService,
            IMemoryCache cache,
            IPathManager pathManager,
            ILogger<ConversationService> logger,
            IConfiguration configuration)
        {
            _templateService = templateService;
            _museTalkService = museTalkService;
            _audioPipelineService = audioPipelineService;
            _llmService = llmService;
            _cache = cache;
            _pathManager = pathManager;
            _logger = logger;
            
            // ä»é…ç½®ä¸­è·å–å¹¶å‘é™åˆ¶
            var maxConcurrency = configuration.GetValue<int>("DigitalHuman:MaxConcurrentConversations", 10);
            var maxRealtimeConcurrency = configuration.GetValue<int>("DigitalHuman:MaxRealtimeConversations", 5);
            
            _concurrencySemaphore = new SemaphoreSlim(maxConcurrency, maxConcurrency);
            _realtimeSemaphore = new SemaphoreSlim(maxRealtimeConcurrency, maxRealtimeConcurrency);
        }

        public async Task<ConversationResponse> GenerateWelcomeVideoAsync(WelcomeVideoRequest request)
        {
            var stopwatch = Stopwatch.StartNew();
            
            try
            {
                _logger.LogInformation("å¼€å§‹ç”Ÿæˆæ¬¢è¿è§†é¢‘: TemplateId={TemplateId}", request.TemplateId);

                // è·å–æ¨¡æ¿ä¿¡æ¯
                var template = await _templateService.GetTemplateByIdAsync(request.TemplateId);
                if (template == null)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "æ¨¡æ¿ä¸å­˜åœ¨"
                    };
                }

                // ç”Ÿæˆæ¬¢è¿è¯­
                var welcomeText = $"ä½ å¥½ï¼Œæˆ‘æ˜¯{template.TemplateName}ï¼Œæ¬¢è¿å’¨è¯¢ï¼";
                
                // ç›´æ¥ç”ŸæˆTTSï¼Œè·³è¿‡LLMè°ƒç”¨
                _logger.LogInformation("ç”Ÿæˆæ¬¢è¿è¯­TTS: {WelcomeText}", welcomeText);
                
                var ttsOutputPath = _pathManager.CreateTempAudioPath("wav");
                var ttsRequest = new TTSRequest
                {
                    Text = welcomeText,
                    Voice = template.DefaultVoiceSettings?.Voice ?? "zh-CN-XiaoxiaoNeural",
                    Rate = template.DefaultVoiceSettings?.Rate ?? "medium",
                    Pitch = template.DefaultVoiceSettings?.Pitch ?? "medium",
                    Emotion = "friendly",
                    OutputPath = ttsOutputPath
                };
                
                var ttsResult = await _audioPipelineService.ConvertTextToSpeechAsync(ttsRequest);
                
                if (!ttsResult.Success)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "æ¬¢è¿è¯­TTSç”Ÿæˆå¤±è´¥: " + ttsResult.Error
                    };
                }

                // ğŸ¬ ç›´æ¥ä½¿ç”¨æ¨¡æ¿çš„é¢„è§ˆè§†é¢‘ï¼ˆå·²åœ¨åˆ›å»ºæ—¶ç”Ÿæˆï¼‰
                _logger.LogInformation("ä½¿ç”¨é¢„ç”Ÿæˆçš„é¢„è§ˆè§†é¢‘: {PreviewVideoPath}", template.PreviewVideoPath);
                
                string videoUrl = template.PreviewVideoPath;
                
                // å¦‚æœæ²¡æœ‰é¢„è§ˆè§†é¢‘ï¼Œæ‰è¿›è¡Œå®æ—¶ç”Ÿæˆ
                if (string.IsNullOrEmpty(videoUrl))
                {
                    _logger.LogInformation("é¢„è§ˆè§†é¢‘ä¸å­˜åœ¨ï¼Œå¼€å§‹å®æ—¶ç”Ÿæˆ...");
                    
                    var videoResponse = await _museTalkService.GenerateVideoAsync(new DigitalHumanRequest
                    {
                        AvatarImagePath = template.ImagePath,
                        AudioPath = ttsResult.AudioPath,
                        Quality = request.Quality,
                        EnableEmotion = true,
                        CacheKey = $"welcome_{request.TemplateId}_{request.Quality}"
                    });
                    
                    if (!videoResponse.Success)
                    {
                        return new ConversationResponse
                        {
                            Success = false,
                            Message = $"æ¬¢è¿è§†é¢‘ç”Ÿæˆå¤±è´¥: {videoResponse.Message}"
                        };
                    }
                    
                    videoUrl = videoResponse.VideoUrl;
                }

                stopwatch.Stop();

                return new ConversationResponse
                {
                    Success = true,
                    Message = "æ¬¢è¿è§†é¢‘å·²å°±ç»ª",
                    InputText = "æ¨¡æ¿é€‰æ‹©", 
                    ResponseText = welcomeText,
                    VideoUrl = videoUrl,
                    // AudioUrl å·²ç§»é™¤ - ä¸å†æ˜¾ç¤ºéŸ³é¢‘
                    DetectedEmotion = "friendly",
                    ProcessingTime = $"{stopwatch.ElapsedMilliseconds}ms",
                    FromCache = !string.IsNullOrEmpty(template.PreviewVideoPath), // ä½¿ç”¨é¢„è§ˆè§†é¢‘ç®—ä½œç¼“å­˜
                    HasVideo = !string.IsNullOrEmpty(videoUrl),
                    Duration = "3.1s" // é¢„ä¼°æ—¶é•¿
                };
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "ç”Ÿæˆæ¬¢è¿è§†é¢‘å¤±è´¥");
                stopwatch.Stop();
                
                return new ConversationResponse
                {
                    Success = false,
                    Message = ex.Message,
                    ProcessingTime = $"{stopwatch.ElapsedMilliseconds}ms"
                };
            }
        }

        public async Task<ConversationResponse> ProcessTextConversationAsync(TextConversationRequest request)
        {
            var stopwatch = Stopwatch.StartNew();
            
            await _concurrencySemaphore.WaitAsync();
            try
            {
                _logger.LogInformation("å¼€å§‹å¤„ç†æ–‡æœ¬å¯¹è¯: TemplateId={TemplateId}, Text={Text}", 
                    request.TemplateId, request.Text[..Math.Min(50, request.Text.Length)]);

                // è·å–æˆ–åˆ›å»ºå¯¹è¯ä¸Šä¸‹æ–‡
                var conversationId = request.ConversationId ?? Guid.NewGuid().ToString("N");
                var context = await GetOrCreateConversationContextAsync(conversationId, request.TemplateId);

                // æ£€æŸ¥ç¼“å­˜
                var cacheKey = GenerateCacheKey("text", request.TemplateId, request.Text, request.Emotion);
                if (request.UseCache && _cache.TryGetValue(cacheKey, out ConversationResponse? cachedResponse))
                {
                    _logger.LogInformation("ä»ç¼“å­˜è¿”å›å¯¹è¯ç»“æœ: CacheKey={CacheKey}", cacheKey);
                    cachedResponse!.FromCache = true;
                    cachedResponse.ConversationId = conversationId;
                    return cachedResponse;
                }

                var metrics = new ConversationMetrics();
                var metricsStopwatch = Stopwatch.StartNew();

                // 1. è°ƒç”¨LLMè·å–å“åº”
                metricsStopwatch.Restart();
                var llmResponse = await _llmService.GenerateResponseAsync(new LocalLLMRequest
                {
                    Message = request.Text,
                    MaxTokens = 500,
                    Temperature = 0.7f
                });
                metrics.LLMResponseTime = metricsStopwatch.ElapsedMilliseconds;

                _logger.LogInformation("LLMå“åº”ç»“æœ: Success={Success}, ResponseTexté•¿åº¦={ResponseLength}, Error={Error}",
                    llmResponse.Success, llmResponse.ResponseText?.Length ?? 0, llmResponse.Error);

                if (!llmResponse.Success)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "LLMå“åº”å¤±è´¥: " + llmResponse.Error,
                        ConversationId = conversationId
                    };
                }

                // æ£€æŸ¥LLMå“åº”æ˜¯å¦ä¸ºç©º
                if (string.IsNullOrWhiteSpace(llmResponse.ResponseText))
                {
                    _logger.LogWarning("LLMè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨é»˜è®¤å›å¤");
                    llmResponse.ResponseText = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚";
                }

                // 2. è·å–æ¨¡æ¿è¯­éŸ³è®¾ç½®
                var template = await _templateService.GetTemplateByIdAsync(request.TemplateId);
                if (template == null)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "æ¨¡æ¿ä¸å­˜åœ¨",
                        ConversationId = conversationId
                    };
                }

                // 3. æ–‡æœ¬è½¬è¯­éŸ³
                _logger.LogInformation("å¼€å§‹TTSè½¬æ¢: ResponseText={ResponseText}, Voice={Voice}", 
                    llmResponse.ResponseText?.Length > 50 ? llmResponse.ResponseText.Substring(0, 50) + "..." : llmResponse.ResponseText, 
                    template.DefaultVoiceSettings?.Voice ?? "zh-CN-XiaoxiaoNeural");
                
                metricsStopwatch.Restart();
                var ttsOutputPath = _pathManager.CreateTempAudioPath("wav");
                var ttsRequest = new TTSRequest
                {
                    Text = llmResponse.ResponseText,
                    Voice = template.DefaultVoiceSettings?.Voice ?? "zh-CN-XiaoxiaoNeural",
                    Rate = template.DefaultVoiceSettings?.Rate ?? "medium",
                    Pitch = template.DefaultVoiceSettings?.Pitch ?? "medium",
                    Emotion = request.Emotion,
                    OutputPath = ttsOutputPath
                };
                
                _logger.LogInformation("TTSè¯·æ±‚å‚æ•°: Texté•¿åº¦={TextLength}, Voice={Voice}, Rate={Rate}, Pitch={Pitch}, OutputPath={OutputPath}",
                    ttsRequest.Text?.Length ?? 0, ttsRequest.Voice, ttsRequest.Rate, ttsRequest.Pitch, ttsRequest.OutputPath);
                
                var ttsResult = await _audioPipelineService.ConvertTextToSpeechAsync(ttsRequest);
                metrics.TTSProcessingTime = metricsStopwatch.ElapsedMilliseconds;

                _logger.LogInformation("TTSè½¬æ¢ç»“æœ: Success={Success}, Error={Error}, AudioPath={AudioPath}",
                    ttsResult.Success, ttsResult.Error, ttsResult.AudioPath);

                if (!ttsResult.Success)
                {
                    _logger.LogError("TTSè½¬æ¢å¤±è´¥: {Error}", ttsResult.Error);
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "TTSç”Ÿæˆå¤±è´¥: " + ttsResult.Error,
                        ConversationId = conversationId
                    };
                }

                // 4. ç”Ÿæˆæ•°å­—äººè§†é¢‘
                metricsStopwatch.Restart();
                var videoResponse = await _museTalkService.GenerateVideoAsync(new DigitalHumanRequest
                {
                    AvatarImagePath = template.ImagePath,
                    AudioPath = ttsResult.AudioPath,
                    Quality = request.Quality,
                    EnableEmotion = true,
                    CacheKey = request.UseCache ? cacheKey : null
                });
                metrics.VideoGenerationTime = metricsStopwatch.ElapsedMilliseconds;

                stopwatch.Stop();
                metrics.TotalProcessingTime = stopwatch.ElapsedMilliseconds;

                var response = new ConversationResponse
                {
                    Success = videoResponse.Success,
                    Message = videoResponse.Message,
                    ConversationId = conversationId,
                    InputText = request.Text,
                    ResponseText = llmResponse.ResponseText,
                    VideoUrl = videoResponse.VideoUrl,
                    AudioUrl = $"/temp/{Path.GetFileName(ttsResult.AudioPath)}",
                    DetectedEmotion = request.Emotion,
                    ProcessingTime = $"{stopwatch.ElapsedMilliseconds}ms",
                    FromCache = false,
                    Metrics = new ServiceMetrics
                    {
                        ActiveWorkers = metrics.ActiveWorkers,
                        QueueLength = metrics.QueueLength,
                        AverageProcessingTime = metrics.AverageProcessingTime,
                        ThroughputPerHour = metrics.ThroughputPerHour,
                        ResourceUsage = metrics.ResourceUsage,
                        PerformanceWarnings = metrics.PerformanceWarnings
                    }
                };

                // æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡
                await UpdateConversationContextAsync(conversationId, context, request.Text, llmResponse.ResponseText, request.Emotion, stopwatch.ElapsedMilliseconds);

                // ç¼“å­˜ç»“æœ
                if (request.UseCache && response.Success)
                {
                    _cache.Set(cacheKey, response, TimeSpan.FromHours(1));
                }

                // æ›´æ–°ç»Ÿè®¡
                Interlocked.Increment(ref _totalConversations);
                Interlocked.Add(ref _totalProcessingTime, stopwatch.ElapsedMilliseconds);
                _templateUsageCount.AddOrUpdate(request.TemplateId, 1, (key, value) => value + 1);

                return response;
            }
            finally
            {
                _concurrencySemaphore.Release();
            }
        }

        public async Task<ConversationResponse> ProcessAudioConversationAsync(AudioConversationRequest request)
        {
            var stopwatch = Stopwatch.StartNew();
            
            await _concurrencySemaphore.WaitAsync();
            try
            {
                _logger.LogInformation("å¼€å§‹å¤„ç†éŸ³é¢‘å¯¹è¯: TemplateId={TemplateId}, AudioPath={AudioPath}", 
                    request.TemplateId, request.AudioPath);

                var conversationId = request.ConversationId ?? Guid.NewGuid().ToString("N");
                var context = await GetOrCreateConversationContextAsync(conversationId, request.TemplateId);
                var metrics = new ConversationMetrics();
                var metricsStopwatch = Stopwatch.StartNew();

                // 1. è¯­éŸ³è¯†åˆ«
                metricsStopwatch.Restart();
                var speechResult = await _audioPipelineService.RecognizeSpeechFromFileAsync(request.AudioPath);
                metrics.AudioProcessingTime = metricsStopwatch.ElapsedMilliseconds;

                if (!speechResult.Success)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "è¯­éŸ³è¯†åˆ«å¤±è´¥: " + speechResult.Error,
                        ConversationId = conversationId
                    };
                }

                // 2. æƒ…æ„Ÿæ£€æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                string detectedEmotion = "neutral";
                if (request.EnableEmotionDetection)
                {
                    var emotionResult = await _audioPipelineService.DetectEmotionFromAudioAsync(request.AudioPath);
                    if (emotionResult.Success)
                    {
                        detectedEmotion = emotionResult.PrimaryEmotion;
                    }
                }

                // æ£€æŸ¥ç¼“å­˜
                var cacheKey = GenerateCacheKey("audio", request.TemplateId, speechResult.Text, detectedEmotion);
                if (request.UseCache && _cache.TryGetValue(cacheKey, out ConversationResponse? cachedResponse))
                {
                    cachedResponse!.FromCache = true;
                    cachedResponse.ConversationId = conversationId;
                    cachedResponse.InputText = speechResult.Text;
                    cachedResponse.DetectedEmotion = detectedEmotion;
                    return cachedResponse;
                }

                // 3. è°ƒç”¨LLMè·å–å“åº”
                metricsStopwatch.Restart();
                var llmResponse = await _llmService.GenerateResponseAsync(new LocalLLMRequest
                {
                    Message = speechResult.Text,
                    MaxTokens = 500,
                    Temperature = 0.7f
                });
                metrics.LLMResponseTime = metricsStopwatch.ElapsedMilliseconds;

                if (!llmResponse.Success)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "LLMå“åº”å¤±è´¥: " + llmResponse.Error,
                        ConversationId = conversationId,
                        InputText = speechResult.Text
                    };
                }

                // 4. æ–‡æœ¬è½¬è¯­éŸ³
                metricsStopwatch.Restart();
                var ttsResult = await _audioPipelineService.ConvertTextToSpeechAsync(new TTSRequest
                {
                    Text = llmResponse.ResponseText,
                    Emotion = detectedEmotion,
                    OutputPath = _pathManager.CreateTempAudioPath("wav")
                });
                metrics.TTSProcessingTime = metricsStopwatch.ElapsedMilliseconds;

                if (!ttsResult.Success)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "TTSç”Ÿæˆå¤±è´¥: " + ttsResult.Error,
                        ConversationId = conversationId,
                        InputText = speechResult.Text
                    };
                }

                // 5. ç”Ÿæˆæ•°å­—äººè§†é¢‘
                var template = await _templateService.GetTemplateByIdAsync(request.TemplateId);
                if (template == null)
                {
                    return new ConversationResponse
                    {
                        Success = false,
                        Message = "æ¨¡æ¿ä¸å­˜åœ¨",
                        ConversationId = conversationId,
                        InputText = speechResult.Text
                    };
                }

                metricsStopwatch.Restart();
                var videoResponse = await _museTalkService.GenerateVideoAsync(new DigitalHumanRequest
                {
                    AvatarImagePath = template.ImagePath,
                    AudioPath = ttsResult.AudioPath,
                    Quality = request.Quality,
                    EnableEmotion = true,
                    CacheKey = request.UseCache ? cacheKey : null
                });
                metrics.VideoGenerationTime = metricsStopwatch.ElapsedMilliseconds;

                stopwatch.Stop();
                metrics.TotalProcessingTime = stopwatch.ElapsedMilliseconds;

                var response = new ConversationResponse
                {
                    Success = videoResponse.Success,
                    Message = videoResponse.Message,
                    ConversationId = conversationId,
                    InputText = speechResult.Text,
                    ResponseText = llmResponse.ResponseText,
                    VideoUrl = videoResponse.VideoUrl,
                    AudioUrl = $"/temp/{Path.GetFileName(ttsResult.AudioPath)}",
                    DetectedEmotion = detectedEmotion,
                    ProcessingTime = $"{stopwatch.ElapsedMilliseconds}ms",
                    FromCache = false,
                    Metrics = new ServiceMetrics
                    {
                        ActiveWorkers = metrics.ActiveWorkers,
                        QueueLength = metrics.QueueLength,
                        AverageProcessingTime = metrics.AverageProcessingTime,
                        ThroughputPerHour = metrics.ThroughputPerHour,
                        ResourceUsage = metrics.ResourceUsage,
                        PerformanceWarnings = metrics.PerformanceWarnings
                    }
                };

                // æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡
                await UpdateConversationContextAsync(conversationId, context, speechResult.Text, llmResponse.ResponseText, detectedEmotion, stopwatch.ElapsedMilliseconds);

                // ç¼“å­˜ç»“æœ
                if (request.UseCache && response.Success)
                {
                    _cache.Set(cacheKey, response, TimeSpan.FromHours(1));
                }

                // æ›´æ–°ç»Ÿè®¡
                Interlocked.Increment(ref _totalConversations);
                Interlocked.Add(ref _totalProcessingTime, stopwatch.ElapsedMilliseconds);
                _templateUsageCount.AddOrUpdate(request.TemplateId, 1, (key, value) => value + 1);

                return response;
            }
            finally
            {
                _concurrencySemaphore.Release();
            }
        }

        public async Task<RealtimeConversationResponse> StartRealtimeConversationAsync(StartRealtimeConversationRequest request)
        {
            await _realtimeSemaphore.WaitAsync();
            try
            {
                var conversationId = Guid.NewGuid().ToString("N");
                var context = await GetOrCreateConversationContextAsync(conversationId, request.TemplateId);
                
                _logger.LogInformation("å¼€å§‹å®æ—¶å¯¹è¯: ConversationId={ConversationId}, TemplateId={TemplateId}", 
                    conversationId, request.TemplateId);

                // é¢„çƒ­æ¨¡æ¿
                await _templateService.WarmupTemplateAsync(request.TemplateId);
                await _museTalkService.WarmupTemplateAsync(request.TemplateId);

                return new RealtimeConversationResponse
                {
                    Success = true,
                    ConversationId = conversationId,
                    Message = "å®æ—¶å¯¹è¯å·²å¯åŠ¨"
                };
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "å¯åŠ¨å®æ—¶å¯¹è¯å¤±è´¥");
                return new RealtimeConversationResponse
                {
                    Success = false,
                    Message = "å¯åŠ¨å®æ—¶å¯¹è¯å¤±è´¥: " + ex.Message
                };
            }
        }

        public async Task<RealtimeConversationResponse> ProcessRealtimeAudioAsync(ProcessRealtimeAudioRequest request)
        {
            // å®æ—¶éŸ³é¢‘å¤„ç†çš„å®ç°ï¼Œç±»ä¼¼äºProcessAudioConversationAsyncï¼Œä½†é’ˆå¯¹å®æ—¶åœºæ™¯è¿›è¡Œä¼˜åŒ–
            // è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ”¯æŒæµå¼å¤„ç†
            throw new NotImplementedException("å®æ—¶éŸ³é¢‘å¤„ç†åŠŸèƒ½å¾…å®ç°");
        }

        public async Task<bool> EndRealtimeConversationAsync(string conversationId)
        {
            try
            {
                if (_conversationContexts.TryRemove(conversationId, out var context))
                {
                    context.State = ConversationState.Ended;
                    _logger.LogInformation("ç»“æŸå®æ—¶å¯¹è¯: ConversationId={ConversationId}", conversationId);
                    return true;
                }
                return false;
            }
            finally
            {
                _realtimeSemaphore.Release();
            }
        }

        public async Task<ConversationContext> GetConversationContextAsync(string conversationId)
        {
            await Task.CompletedTask;
            return _conversationContexts.GetValueOrDefault(conversationId) ?? new ConversationContext
            {
                ConversationId = conversationId,
                State = ConversationState.Error
            };
        }

        public async Task<bool> UpdateConversationContextAsync(string conversationId, ConversationContext context)
        {
            await Task.CompletedTask;
            _conversationContexts.AddOrUpdate(conversationId, context, (key, existingContext) => context);
            return true;
        }

        public async Task<bool> ClearConversationContextAsync(string conversationId)
        {
            await Task.CompletedTask;
            return _conversationContexts.TryRemove(conversationId, out _);
        }

        public async Task<List<ConversationResponse>> ProcessBatchConversationsAsync(List<TextConversationRequest> requests)
        {
            var tasks = requests.Select(ProcessTextConversationAsync).ToArray();
            var results = await Task.WhenAll(tasks);
            return results.ToList();
        }

        public async Task<List<ConversationHistory>> GetConversationHistoryAsync(string? conversationId = null, int limit = 50)
        {
            await Task.CompletedTask;
            
            var histories = new List<ConversationHistory>();
            var contexts = conversationId != null 
                ? _conversationContexts.Where(kvp => kvp.Key == conversationId)
                : _conversationContexts.AsEnumerable();

            foreach (var (id, context) in contexts.Take(limit))
            {
                histories.Add(new ConversationHistory
                {
                    ConversationId = id,
                    TemplateId = context.TemplateId,
                    StartTime = context.StartTime,
                    EndTime = context.LastUpdateTime,
                    TurnCount = context.History.Count,
                    TotalProcessingTime = context.History.Sum(h => h.ProcessingTime),
                    State = context.State
                });
            }

            return histories;
        }

        public async Task<ConversationStatistics> GetConversationStatisticsAsync()
        {
            await Task.CompletedTask;
            
            var activeConversations = _conversationContexts.Count(kvp => kvp.Value.State == ConversationState.Active);
            var averageProcessingTime = _totalConversations > 0 ? _totalProcessingTime / _totalConversations : 0;
            
            var emotionDistribution = new Dictionary<string, int>();
            var dailyConversationCount = new Dictionary<string, long>();

            foreach (var context in _conversationContexts.Values)
            {
                foreach (var turn in context.History)
                {
                    // ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
                    if (!string.IsNullOrEmpty(turn.Emotion))
                    {
                        emotionDistribution[turn.Emotion] = emotionDistribution.GetValueOrDefault(turn.Emotion, 0) + 1;
                    }

                    // ç»Ÿè®¡æ¯æ—¥å¯¹è¯æ•°é‡
                    var dateKey = turn.Timestamp.ToString("yyyy-MM-dd");
                    dailyConversationCount[dateKey] = dailyConversationCount.GetValueOrDefault(dateKey, 0) + 1;
                }
            }

            return new ConversationStatistics
            {
                TotalConversations = (int)_totalConversations,
                ActiveConversations = activeConversations,
                AverageProcessingTime = averageProcessingTime,
                TemplateUsageCount = _templateUsageCount.ToDictionary(kvp => kvp.Key, kvp => (int)kvp.Value),
                EmotionDistribution = emotionDistribution,
                DailyConversationCount = dailyConversationCount
            };
        }

        public async Task<bool> PreloadTemplateForConversationAsync(string templateId)
        {
            try
            {
                await _templateService.WarmupTemplateAsync(templateId);
                await _museTalkService.WarmupTemplateAsync(templateId);
                return true;
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "é¢„åŠ è½½æ¨¡æ¿å¤±è´¥: TemplateId={TemplateId}", templateId);
                return false;
            }
        }

        public async Task<string> GetOptimalResponseModeAsync(string templateId, ConversationMetrics metrics)
        {
            await Task.CompletedTask;
            
            // åŸºäºæ€§èƒ½æŒ‡æ ‡æ¨èæœ€ä¼˜å“åº”æ¨¡å¼
            if (metrics.TotalProcessingTime < 2000) // 2ç§’ä»¥å†…
            {
                return "instant";
            }
            else if (metrics.TotalProcessingTime < 5000) // 5ç§’ä»¥å†…
            {
                return "fast";
            }
            else
            {
                return "quality";
            }
        }

        private async Task<ConversationContext> GetOrCreateConversationContextAsync(string conversationId, string templateId)
        {
            await Task.CompletedTask;
            
            return _conversationContexts.GetOrAdd(conversationId, id => new ConversationContext
            {
                ConversationId = id,
                TemplateId = templateId,
                StartTime = DateTime.UtcNow,
                LastUpdateTime = DateTime.UtcNow,
                State = ConversationState.Active
            });
        }

        private async Task UpdateConversationContextAsync(string conversationId, ConversationContext context, 
            string userInput, string botResponse, string emotion, long processingTime)
        {
            await Task.CompletedTask;
            
            context.History.Add(new ConversationTurn
            {
                Timestamp = DateTime.UtcNow,
                UserInput = userInput,
                BotResponse = botResponse,
                Emotion = emotion,
                ProcessingTime = processingTime,
                FromCache = false
            });
            
            context.LastUpdateTime = DateTime.UtcNow;
            
            // é™åˆ¶å†å²è®°å½•æ•°é‡
            if (context.History.Count > 100)
            {
                context.History.RemoveRange(0, context.History.Count - 100);
            }
        }

        private string GenerateCacheKey(string type, string templateId, string text, string emotion)
        {
            var content = $"{type}:{templateId}:{text}:{emotion}";
            return Convert.ToBase64String(System.Text.Encoding.UTF8.GetBytes(content));
        }


    }
}
#!/usr/bin/env python3
"""
æœ€å°åŒ–çš„MuseTalk inferenceæµ‹è¯•
"""

import os
import sys
import torch

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("ğŸ”„ å¼€å§‹æµ‹è¯•MuseTalkæ¨¡å‹åŠ è½½...")
    
    try:
        # è®¾ç½®å•GPU
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        
        print("âœ… å¯¼å…¥åŸºç¡€æ¨¡å—...")
        from musetalk.utils.utils import load_all_model
        
        print("âœ… å¼€å§‹åŠ è½½æ¨¡å‹...")
        print("   - æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        
        unet_config = "models/musetalk/musetalk.json"
        unet_model = "models/musetalk/pytorch_model.bin"
        
        if not os.path.exists(unet_config):
            print(f"âŒ UNeté…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {unet_config}")
            return False
            
        if not os.path.exists(unet_model):
            print(f"âŒ UNetæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {unet_model}")
            return False
            
        print(f"âœ… é…ç½®æ–‡ä»¶å­˜åœ¨: {unet_config}")
        print(f"âœ… æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {unet_model}")
        
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"âœ… GPUå†…å­˜: {gpu_memory:.1f}GB")
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            print("âœ… GPUç¼“å­˜å·²æ¸…ç†")
        
        print("âš ï¸  å®é™…æ¨¡å‹åŠ è½½æµ‹è¯•è·³è¿‡ï¼ˆé¿å…å¡æ­»ï¼‰")
        print("   å¦‚éœ€æµ‹è¯•ï¼Œè¯·å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š")
        
        # print("ğŸ”„ å®é™…åŠ è½½æ¨¡å‹...")
        # vae, unet, pe = load_all_model(
        #     unet_path=unet_model,
        #     unet_config=unet_config,
        #     version="v1"
        # )
        # print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("ğŸš€ MuseTalkæœ€å°åŒ–æµ‹è¯•")
    print(f"å½“å‰ç›®å½•: {os.getcwd()}")
    print(f"CUDAè®¾å¤‡: {os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')}")
    print()
    
    if test_model_loading():
        print("\nâœ… åŸºç¡€æµ‹è¯•é€šè¿‡")
        print("å¦‚æœinferenceä»ç„¶å¡æ­»ï¼Œå¯èƒ½æ˜¯:")
        print("1. é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šäº†ä¸å­˜åœ¨çš„è¾“å…¥æ–‡ä»¶")
        print("2. å®é™…æ¨¡å‹åŠ è½½æ—¶GPUå†…å­˜ä¸è¶³")
        print("3. å¤šGPUå¹¶å‘å†²çª")
    else:
        print("\nâŒ åŸºç¡€æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()
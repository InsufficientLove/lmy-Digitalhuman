# MuseTalk GPU Docker部署命令

## 在服务器上执行以下命令：

# 1. 克隆代码
cd ~
git clone -b cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5 \
    https://github.com/InsufficientLove/lmy-Digitalhuman.git musetalk
cd musetalk

# 2. 构建Docker镜像
docker build -t musetalk:latest .

# 3. 测试GPU检测
docker run --rm --runtime=nvidia --gpus all -e RUN_MODE=test musetalk:latest

# 4. 运行容器（自动检测GPU并优化batch_size）
docker run -d --name musetalk \
    --runtime=nvidia \
    --gpus all \
    -p 7860:7860 \
    -v $(pwd)/models:/app/models \
    -v $(pwd)/inputs:/app/inputs \
    -v $(pwd)/outputs:/app/outputs \
    musetalk:latest

# 5. 查看日志
docker logs -f musetalk

# 6. 访问Gradio界面
# http://服务器IP:7860

## 指定GPU运行示例：

# 只用GPU 0（单张RTX 4090D 48GB）
docker run -d --name musetalk \
    --runtime=nvidia \
    --gpus '"device=0"' \
    -e CUDA_VISIBLE_DEVICES=0 \
    -p 7860:7860 \
    musetalk:latest

# 使用4个GPU（4x RTX 4090 24GB）
docker run -d --name musetalk \
    --runtime=nvidia \
    --gpus all \
    -e CUDA_VISIBLE_DEVICES=0,1,2,3 \
    -p 7860:7860 \
    musetalk:latest

# 只用GPU 2和3（部分GPU被占用）
docker run -d --name musetalk \
    --runtime=nvidia \
    --gpus all \
    -e CUDA_VISIBLE_DEVICES=2,3 \
    -p 7860:7860 \
    musetalk:latest

## 使用docker-compose：

# 默认运行（自动检测）
docker-compose up -d

# 指定GPU
GPU_IDS=0,1 docker-compose up -d

# 查看状态
docker-compose ps
docker-compose logs -f
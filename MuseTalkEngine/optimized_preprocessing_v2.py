#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimized Preprocessing V2
ä¼˜åŒ–ç‰ˆé¢„å¤„ç† - ä¿®å¤è„¸éƒ¨é˜´å½±é—®é¢˜ï¼Œæé€Ÿé¢„å¤„ç†
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import argparse
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import copy
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs, coord_placeholder
from musetalk.utils.blending import get_image_prepare_material
from musetalk.utils.audio_processor import AudioProcessor

print("Optimized Preprocessing V2 - æé€Ÿé¢„å¤„ç†å¼•æ“")

class OptimizedPreprocessor:
    """ä¼˜åŒ–çš„é¢„å¤„ç†å™¨ - ä¿®å¤é˜´å½±é—®é¢˜ï¼Œæé€Ÿå¤„ç†"""
    
    def __init__(self):
        self.vae = None
        self.unet = None
        self.pe = None
        self.fp = None
        self.device = None
        self.weight_dtype = torch.float16
        self.is_initialized = False
        
        # ğŸ¨ é˜´å½±ä¿®å¤å‚æ•°
        self.shadow_fix_enabled = True
        self.lighting_adjustment = True
        self.color_correction = True
        
    def initialize_models(self, device='cuda:0'):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.is_initialized:
            return True
            
        try:
            print(f"åˆå§‹åŒ–é¢„å¤„ç†æ¨¡å‹ - è®¾å¤‡: {device}")
            self.device = device
            
            # åŠ è½½æ¨¡å‹ - æ·»åŠ é”™è¯¯å¤„ç†
            try:
                vae, unet, pe = load_all_model()
                print("é¢„å¤„ç†æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"é¢„å¤„ç†æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                # å°è¯•ä½¿ç”¨å¤‡ç”¨VAEè·¯å¾„
                try:
                    vae, unet, pe = load_all_model(vae_type="sd-vae-ft-mse")
                    print("é¢„å¤„ç†ä½¿ç”¨å¤‡ç”¨VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e2:
                    print(f"é¢„å¤„ç†å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                    raise e2
            
            # ä¿®å¤æ¨¡å‹å¯¹è±¡å…¼å®¹æ€§ - ä½¿ç”¨æ­£ç¡®çš„å±æ€§ç»“æ„
            if hasattr(vae, 'vae'):
                vae.vae = vae.vae.to(device).half().eval()
                self.vae = vae
            elif hasattr(vae, 'to'):
                self.vae = vae.to(device).half().eval()
            else:
                print("è­¦å‘Š: VAEå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                self.vae = vae
            
            if hasattr(unet, 'model'):
                unet.model = unet.model.to(device).half().eval()
                self.unet = unet
            elif hasattr(unet, 'to'):
                self.unet = unet.to(device).half().eval()
            else:
                print("è­¦å‘Š: UNetå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                self.unet = unet
            
            if hasattr(pe, 'to'):
                self.pe = pe.to(device).half().eval()
            else:
                print("è­¦å‘Š: PEå¯¹è±¡æ²¡æœ‰.to()æ–¹æ³•ï¼Œè·³è¿‡ä¼˜åŒ–")
                self.pe = pe
            
            # é¢éƒ¨è§£æå™¨
            self.fp = FaceParsing()
            
            print("é¢„å¤„ç†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def fix_face_shadows(self, image):
        """ä¿®å¤é¢éƒ¨é˜´å½±é—®é¢˜"""
        if not self.shadow_fix_enabled:
            return image
        
        try:
            # ğŸ¨ 1. å…‰ç…§å‡è¡¡åŒ–
            if self.lighting_adjustment:
                # è½¬æ¢åˆ°LABè‰²å½©ç©ºé—´è¿›è¡Œå…‰ç…§è°ƒæ•´
                lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
                l_channel = lab[:, :, 0]
                
                # è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                l_channel = clahe.apply(l_channel)
                
                lab[:, :, 0] = l_channel
                image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
            
            # ğŸ¨ 2. é˜´å½±æ£€æµ‹å’Œä¿®å¤
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ä½¿ç”¨å½¢æ€å­¦æ“ä½œæ£€æµ‹é˜´å½±åŒºåŸŸ
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))
            dilated = cv2.dilate(gray, kernel)
            shadow_mask = cv2.absdiff(dilated, gray)
            
            # é˜ˆå€¼å¤„ç†å¾—åˆ°é˜´å½±åŒºåŸŸ
            _, shadow_mask = cv2.threshold(shadow_mask, 30, 255, cv2.THRESH_BINARY)
            
            # å¯¹é˜´å½±åŒºåŸŸè¿›è¡Œäº®åº¦æå‡
            shadow_areas = shadow_mask > 0
            if np.any(shadow_areas):
                # æå‡é˜´å½±åŒºåŸŸäº®åº¦
                hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
                hsv[:, :, 2][shadow_areas] = np.clip(
                    hsv[:, :, 2][shadow_areas] * 1.3, 0, 255
                ).astype(np.uint8)
                image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            
            # ğŸ¨ 3. é¢œè‰²æ ¡æ­£
            if self.color_correction:
                # ç™½å¹³è¡¡è°ƒæ•´
                image = self.white_balance_correction(image)
                
                # è‚¤è‰²å¢å¼º
                image = self.skin_tone_enhancement(image)
            
            return image
            
        except Exception as e:
            print(f"é˜´å½±ä¿®å¤å¤±è´¥: {str(e)}")
            return image
    
    def white_balance_correction(self, image):
        """ç™½å¹³è¡¡æ ¡æ­£"""
        try:
            # Gray Worldç®—æ³•
            b, g, r = cv2.split(image)
            
            b_avg = np.mean(b)
            g_avg = np.mean(g) 
            r_avg = np.mean(r)
            
            # è®¡ç®—å¢ç›Š
            k = (b_avg + g_avg + r_avg) / 3
            kb = k / b_avg
            kg = k / g_avg
            kr = k / r_avg
            
            # åº”ç”¨å¢ç›Š
            b = np.clip(b * kb, 0, 255).astype(np.uint8)
            g = np.clip(g * kg, 0, 255).astype(np.uint8)
            r = np.clip(r * kr, 0, 255).astype(np.uint8)
            
            return cv2.merge([b, g, r])
            
        except:
            return image
    
    def skin_tone_enhancement(self, image):
        """è‚¤è‰²å¢å¼º"""
        try:
            # è½¬æ¢åˆ°YUVè‰²å½©ç©ºé—´
            yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)
            
            # è‚¤è‰²èŒƒå›´æ£€æµ‹
            lower_skin = np.array([0, 133, 77], dtype=np.uint8)
            upper_skin = np.array([255, 173, 127], dtype=np.uint8)
            
            skin_mask = cv2.inRange(yuv, lower_skin, upper_skin)
            
            # å¯¹è‚¤è‰²åŒºåŸŸè¿›è¡Œå¾®è°ƒ
            if np.any(skin_mask > 0):
                # è½»å¾®å¢å¼ºçº¢è‰²é€šé“
                bgr = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
                b, g, r = cv2.split(bgr)
                
                skin_areas = skin_mask > 0
                r[skin_areas] = np.clip(r[skin_areas] * 1.05, 0, 255).astype(np.uint8)
                
                return cv2.merge([b, g, r])
            
            return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
            
        except:
            return image
    
    def preprocess_template_ultra_fast(self, template_path, output_dir, template_id):
        """æé€Ÿæ¨¡æ¿é¢„å¤„ç†"""
        try:
            start_time = time.time()
            print(f"å¼€å§‹æé€Ÿé¢„å¤„ç†: {template_id}")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # 1. å¹¶è¡Œè¯»å–å’Œå¤„ç†å›¾åƒ
            print("è¯»å–æ¨¡æ¿å›¾åƒ...")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥çš„å›¾åƒæ–‡ä»¶è·¯å¾„
            template_path_obj = Path(template_path)
            if template_path_obj.is_file() and template_path_obj.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:
                input_image_path = str(template_path_obj)
                print(f"ä½¿ç”¨ç›´æ¥å›¾åƒæ–‡ä»¶: {input_image_path}")
            else:
                # åœ¨ç›®å½•ä¸­æœç´¢å›¾åƒæ–‡ä»¶
                image_files = []
                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                    image_files.extend(template_path_obj.glob(ext))
                
                if not image_files:
                    raise ValueError(f"æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶: {template_path}")
                
                # é€‰æ‹©æœ€ä½³å›¾åƒï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€å¼ ï¼‰
                input_image_path = str(image_files[0])
                print(f"ä½¿ç”¨ç›®å½•ä¸­çš„å›¾åƒ: {input_image_path}")
            
            # ğŸ¨ 2. å›¾åƒé¢„å¤„ç†å’Œé˜´å½±ä¿®å¤
            print("ğŸ¨ å›¾åƒé¢„å¤„ç†å’Œé˜´å½±ä¿®å¤...")
            image = cv2.imread(input_image_path)
            if image is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {input_image_path}")
            
            # å…³é”®ï¼šé˜´å½±ä¿®å¤
            image = self.fix_face_shadows(image)
            
            # 3. é¢éƒ¨æ£€æµ‹å’Œå…³é”®ç‚¹æå–
            print("ğŸ‘¤ é¢éƒ¨æ£€æµ‹å’Œå…³é”®ç‚¹æå–...")
            # ä¿å­˜ä¸´æ—¶å›¾åƒæ–‡ä»¶ç»™get_landmark_and_bboxä½¿ç”¨
            temp_image_path = os.path.join(output_dir, "temp_image.jpg")
            cv2.imwrite(temp_image_path, image)
            coord_list, frame_list = get_landmark_and_bbox([temp_image_path])
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_image_path):
                os.remove(temp_image_path)
            
            if not coord_list or not frame_list:
                raise ValueError("é¢éƒ¨æ£€æµ‹å¤±è´¥")
            
            # 4. é¢éƒ¨è§£æ
            print("é¢éƒ¨è§£æ...")
            mask_coords_list, mask_list = [], []
            
            for frame in frame_list:
                # é¢éƒ¨è§£æ
                mask_out = self.fp.forward(frame)
                
                # è·å–é¢éƒ¨åŒºåŸŸåæ ‡
                mask_coords = get_image_prepare_material(mask_out)
                mask_coords_list.append(mask_coords)
                mask_list.append(mask_out)
            
            # 5. VAEç¼–ç  - å¹¶è¡Œå¤„ç†
            print("VAEç¼–ç ...")
            input_latent_list = []
            
            def encode_frame(frame):
                with torch.no_grad():
                    frame_tensor = torch.from_numpy(frame).float().to(self.device) / 127.5 - 1.0
                    frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)
                    latent = self.vae.encode_latents(frame_tensor)
                    return latent.cpu()
            
            # å¹¶è¡Œç¼–ç å¤šå¸§
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(encode_frame, frame) for frame in frame_list]
                for future in as_completed(futures):
                    latent = future.result()
                    input_latent_list.append(latent)
            
            # 6. åˆ›å»ºå¾ªç¯æ•°æ®
            print("ğŸ”„ åˆ›å»ºå¾ªç¯æ•°æ®...")
            
            # å¦‚æœåªæœ‰ä¸€å¸§ï¼Œå¤åˆ¶åˆ›å»ºå¾ªç¯
            if len(input_latent_list) == 1:
                input_latent_list_cycle = input_latent_list * 2
                coord_list_cycle = coord_list * 2
                frame_list_cycle = frame_list * 2
                mask_coords_list_cycle = mask_coords_list * 2
                mask_list_cycle = mask_list * 2
            else:
                input_latent_list_cycle = input_latent_list
                coord_list_cycle = coord_list
                frame_list_cycle = frame_list
                mask_coords_list_cycle = mask_coords_list
                mask_list_cycle = mask_list
            
            # 7. ä¿å­˜é¢„å¤„ç†ç¼“å­˜
            print("ğŸ’¾ ä¿å­˜é¢„å¤„ç†ç¼“å­˜...")
            
            cache_data = {
                'input_latent_list_cycle': input_latent_list_cycle,
                'coord_list_cycle': coord_list_cycle,
                'frame_list_cycle': frame_list_cycle,
                'mask_coords_list_cycle': mask_coords_list_cycle,
                'mask_list_cycle': mask_list_cycle,
            }
            
            # ä¿å­˜ç¼“å­˜æ–‡ä»¶
            cache_file = os.path.join(output_dir, f"{template_id}_preprocessed.pkl")
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'template_id': template_id,
                'template_path': template_path,
                'processed_at': time.time(),
                'frame_count': len(frame_list_cycle),
                'shadow_fix_enabled': self.shadow_fix_enabled,
                'lighting_adjustment': self.lighting_adjustment,
                'color_correction': self.color_correction
            }
            
            metadata_file = os.path.join(output_dir, f"{template_id}_metadata.json")
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜ç®€åŒ–çš„çŠ¶æ€æ–‡ä»¶ï¼ˆå…¼å®¹æ€§ï¼‰
            state_file = os.path.join(output_dir, "model_state.pkl")
            with open(state_file, 'wb') as f:
                pickle.dump({'status': 'completed', 'template_id': template_id}, f)
            
            total_time = time.time() - start_time
            print(f"æé€Ÿé¢„å¤„ç†å®Œæˆï¼")
            print(f"å¤„ç†ç»Ÿè®¡:")
            print(f"   - æ¨¡æ¿ID: {template_id}")
            print(f"   - å¸§æ•°: {len(frame_list_cycle)}")
            print(f"   - è€—æ—¶: {total_time:.2f}ç§’")
            print(f"   - é˜´å½±ä¿®å¤: {'å¯ç”¨' if self.shadow_fix_enabled else 'ç¦ç”¨'}")
            print(f"   - ç¼“å­˜æ–‡ä»¶: {cache_file}")
            
            return True
            
        except Exception as e:
            print(f"é¢„å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆæ¨¡æ¿é¢„å¤„ç†')
    parser.add_argument('--template_path', type=str, required=True, help='æ¨¡æ¿å›¾åƒè·¯å¾„')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--template_id', type=str, required=True, help='æ¨¡æ¿ID')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    parser.add_argument('--disable_shadow_fix', action='store_true', help='ç¦ç”¨é˜´å½±ä¿®å¤')
    parser.add_argument('--disable_lighting', action='store_true', help='ç¦ç”¨å…‰ç…§è°ƒæ•´')
    parser.add_argument('--disable_color_correction', action='store_true', help='ç¦ç”¨é¢œè‰²æ ¡æ­£')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = OptimizedPreprocessor()
    
    # é…ç½®é˜´å½±ä¿®å¤é€‰é¡¹
    preprocessor.shadow_fix_enabled = not args.disable_shadow_fix
    preprocessor.lighting_adjustment = not args.disable_lighting
    preprocessor.color_correction = not args.disable_color_correction
    
    print(f"ğŸ¨ é˜´å½±ä¿®å¤é…ç½®:")
    print(f"   - é˜´å½±ä¿®å¤: {'å¯ç”¨' if preprocessor.shadow_fix_enabled else 'ç¦ç”¨'}")
    print(f"   - å…‰ç…§è°ƒæ•´: {'å¯ç”¨' if preprocessor.lighting_adjustment else 'ç¦ç”¨'}")
    print(f"   - é¢œè‰²æ ¡æ­£: {'å¯ç”¨' if preprocessor.color_correction else 'ç¦ç”¨'}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    if not preprocessor.initialize_models(args.device):
        print("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
        return
    
    # æ‰§è¡Œé¢„å¤„ç†
    success = preprocessor.preprocess_template_ultra_fast(
        args.template_path,
        args.output_dir, 
        args.template_id
    )
    
    if success:
        print("é¢„å¤„ç†æˆåŠŸå®Œæˆ")
    else:
        print("é¢„å¤„ç†å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()
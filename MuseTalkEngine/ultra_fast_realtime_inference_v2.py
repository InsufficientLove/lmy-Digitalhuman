#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultra Fast Realtime Inference V2
æè‡´ä¼˜åŒ–ç‰ˆæœ¬ - ç›®æ ‡ï¼šæ¯«ç§’çº§å“åº”ï¼Œ4GPUçœŸå¹¶è¡Œï¼Œé›¶ç­‰å¾…
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import time
import gc
import threading
import queue
import socket
import struct
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import copy
import gc
try:
    from transformers import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: transformers.WhisperModelä¸å¯ç”¨ï¼Œå°†è·³è¿‡Whisperåˆå§‹åŒ–")
    WHISPER_AVAILABLE = False
import imageio
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image, get_image_blending, get_image_prepare_material
from musetalk.utils.audio_processor import AudioProcessor

# å¯¼å…¥æ€§èƒ½ç›‘æ§
try:
    # å°è¯•ä»å½“å‰ç›®å½•å¯¼å…¥
    from performance_monitor import start_performance_monitoring, record_performance, print_performance_report
    PERFORMANCE_MONITORING = True
except ImportError:
    try:
        # å°è¯•ä»ç›¸å¯¹è·¯å¾„å¯¼å…¥
        import sys
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        sys.path.insert(0, current_dir)
        from performance_monitor import start_performance_monitoring, record_performance, print_performance_report
        PERFORMANCE_MONITORING = True
    except ImportError:
        PERFORMANCE_MONITORING = False
        print("æ€§èƒ½ç›‘æ§æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡æ€§èƒ½ç›‘æ§")

print("Ultra Fast Realtime Inference V2 - æ¯«ç§’çº§å“åº”å¼•æ“")
sys.stdout.flush()

class UltraFastMuseTalkService:
    """æè‡´ä¼˜åŒ–çš„MuseTalkæœåŠ¡ - æ¯«ç§’çº§å“åº”"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        # 4GPUå¹¶è¡Œæ¶æ„
        self.gpu_count = min(4, torch.cuda.device_count())
        self.devices = [f'cuda:{i}' for i in range(self.gpu_count)]
        
        # æ¯ä¸ªGPUç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹
        self.gpu_models = {}
        self.gpu_locks = {device: threading.Lock() for device in self.devices}
        
        # å…¨å±€æ¨¡å‹ç»„ä»¶ï¼ˆå…±äº«æƒé‡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        self.shared_vae = None
        self.shared_unet = None
        self.shared_pe = None
        self.shared_whisper = None
        self.shared_audio_processor = None
        self.shared_fp = None
        self.weight_dtype = torch.float16  # ä½¿ç”¨åŠç²¾åº¦æé€Ÿ
        self.timesteps = None
        
        # å†…å­˜æ± å’Œç¼“å­˜ä¼˜åŒ–
        self.template_cache = {}
        self.audio_feature_cache = {}
        self.frame_buffer_pool = queue.Queue(maxsize=1000)
        
        # æé€Ÿå¤„ç†ç®¡é“
        self.inference_executor = ThreadPoolExecutor(max_workers=self.gpu_count)
        self.compose_executor = ThreadPoolExecutor(max_workers=32)  # 32çº¿ç¨‹å¹¶è¡Œåˆæˆ
        self.video_executor = ThreadPoolExecutor(max_workers=4)
        
        # ğŸ® GPUè´Ÿè½½å‡è¡¡
        self.gpu_usage = {device: 0 for device in self.devices}
        self.gpu_queue = {device: queue.Queue(maxsize=10) for device in self.devices}
        
        self.is_initialized = False
        self._initialized = True
        
        print(f"Ultra Fast Service åˆå§‹åŒ–å®Œæˆ - {self.gpu_count}GPUå¹¶è¡Œæ¶æ„")
        sys.stdout.flush()
    
    def initialize_models_ultra_fast(self):
        """æé€Ÿåˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ - å¹¶è¡ŒåŠ è½½åˆ°æ‰€æœ‰GPU"""
        if self.is_initialized and len(self.gpu_models) > 0:
            print("æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return True
        
        # é‡ç½®åˆå§‹åŒ–çŠ¶æ€ï¼Œå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        self.is_initialized = False
        self.gpu_models = {}
            
        try:
            print(f"å¼€å§‹æé€Ÿåˆå§‹åŒ– - {self.gpu_count}GPUå¹¶è¡ŒåŠ è½½...")
            start_time = time.time()
            
            # å¹¶è¡Œåˆå§‹åŒ–æ‰€æœ‰GPUæ¨¡å‹
            def init_gpu_model(device_id):
                device = f'cuda:{device_id}'
                print(f"ğŸ® GPU{device_id} å¼€å§‹åˆå§‹åŒ–...")
                
                with torch.cuda.device(device_id):
                    # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šGPU - åªä½¿ç”¨å¯ç”¨çš„sd-vae
                    try:
                        print(f"GPU{device_id} å¼€å§‹åŠ è½½æ¨¡å‹...")
                        # æ£€æŸ¥sd-vaeç›®å½•æ˜¯å¦å®Œæ•´
                        sd_vae_path = "./models/sd-vae"
                        config_file = os.path.join(sd_vae_path, "config.json")
                        
                        if os.path.exists(config_file):
                            print(f"GPU{device_id} ä½¿ç”¨å®Œæ•´çš„sd-vaeæ¨¡å‹")
                            vae, unet, pe = load_all_model(vae_type="sd-vae")
                            print(f"GPU{device_id} æ¨¡å‹åŠ è½½æˆåŠŸ")
                        else:
                            print(f"GPU{device_id} sd-vaeé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤GPU")
                            return None
                            
                    except Exception as e:
                        print(f"GPU{device_id} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        # æ£€æŸ¥æ˜¯å¦æ˜¯UNetæ¨¡å‹é—®é¢˜
                        if "meta tensor" in str(e) or "Cannot copy out" in str(e):
                            print(f"GPU{device_id} UNetæ¨¡å‹æ–‡ä»¶å¯èƒ½æŸåï¼Œå°è¯•é‡æ–°åŠ è½½...")
                            try:
                                # å¼ºåˆ¶æ¸…ç†GPUå†…å­˜
                                torch.cuda.empty_cache()
                                # é‡æ–°å°è¯•åŠ è½½
                                vae, unet, pe = load_all_model(vae_type="sd-vae")
                                print(f"GPU{device_id} é‡æ–°åŠ è½½æˆåŠŸ")
                            except Exception as e3:
                                print(f"GPU{device_id} é‡æ–°åŠ è½½ä¹Ÿå¤±è´¥: {e3}")
                                return None
                        else:
                            print(f"GPU{device_id} å…¶ä»–é”™è¯¯ï¼Œè·³è¿‡æ­¤GPU")
                            return None
                    
                    # ä¼˜åŒ–æ¨¡å‹ - åŠç²¾åº¦+ç¼–è¯‘ä¼˜åŒ– (ä¿®å¤æ¨¡å‹å¯¹è±¡å…¼å®¹æ€§)
                    print(f"GPU{device_id} å¼€å§‹æ¨¡å‹ä¼˜åŒ–...")
                    
                    # ä¿®å¤VAEå¯¹è±¡ - ä½¿ç”¨.vaeå±æ€§
                    if hasattr(vae, 'vae'):
                        vae.vae = vae.vae.to(device).half().eval()
                    elif hasattr(vae, 'to'):
                        vae = vae.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: VAEå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    # ä¿®å¤UNetå¯¹è±¡ - ä½¿ç”¨.modelå±æ€§  
                    if hasattr(unet, 'model'):
                        unet.model = unet.model.to(device).half().eval()
                    elif hasattr(unet, 'to'):
                        unet = unet.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: UNetå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    # ä¿®å¤PEå¯¹è±¡
                    if hasattr(pe, 'to'):
                        pe = pe.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: PEå¯¹è±¡æ²¡æœ‰.to()æ–¹æ³•ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    print(f"GPU{device_id} åŠç²¾åº¦è½¬æ¢å®Œæˆ")
                    
                    # å…³é”®ä¼˜åŒ–ï¼šæ¨¡å‹ç¼–è¯‘åŠ é€Ÿ (ä»…åœ¨Linuxä¸Šå¯ç”¨)
                    import platform
                    if hasattr(torch, 'compile') and platform.system() != 'Windows':
                        try:
                            print(f"GPU{device_id} å¼€å§‹æ¨¡å‹ç¼–è¯‘...")
                            unet.model = torch.compile(unet.model, mode="reduce-overhead")
                            vae.vae = torch.compile(vae.vae, mode="reduce-overhead")
                            pe = torch.compile(pe, mode="reduce-overhead")
                            print(f"GPU{device_id} æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å®Œæˆ")
                        except Exception as compile_error:
                            print(f"GPU{device_id} æ¨¡å‹ç¼–è¯‘å¤±è´¥: {compile_error}")
                            print(f"GPU{device_id} ä½¿ç”¨åŸå§‹æ¨¡å‹")
                    else:
                        if platform.system() == 'Windows':
                            print(f"GPU{device_id} è·³è¿‡æ¨¡å‹ç¼–è¯‘ (Windowsä¸æ”¯æŒ)")
                        else:
                            print(f"GPU{device_id} è·³è¿‡æ¨¡å‹ç¼–è¯‘ (torch.compileä¸å¯ç”¨)")
                    
                    self.gpu_models[device] = {
                        'vae': vae,
                        'unet': unet,
                        'pe': pe,
                        'device': device
                    }
                    
                    print(f"GPU{device_id} æ¨¡å‹åŠ è½½å®Œæˆ")
                    return device_id
            
            # SEQUENTIAL_LOADING_FIXED: é¡ºåºåˆå§‹åŒ–é¿å…å¹¶å‘å†²çª
            print(f"å¼€å§‹é¡ºåºåˆå§‹åŒ–{self.gpu_count}ä¸ªGPUï¼ˆé¿å…å¹¶å‘å†²çªï¼‰...")
            successful_gpus = []
            
            for i in range(self.gpu_count):
                print(f"æ­£åœ¨åˆå§‹åŒ–GPU {i}/{self.gpu_count}...")
                try:
                    # åœ¨æ¯ä¸ªGPUåˆå§‹åŒ–å‰æ¸…ç†å†…å­˜
                    torch.cuda.set_device(i)
                    torch.cuda.empty_cache()
                    
                    result = init_gpu_model(i)
                    if result is not None:
                        successful_gpus.append(i)
                        print(f"âœ… GPU{i} åˆå§‹åŒ–æˆåŠŸ ({len(successful_gpus)}/{self.gpu_count})")
                    else:
                        print(f"âŒ GPU{i} åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡")
                except Exception as e:
                    print(f"âŒ GPU{i} åˆå§‹åŒ–å¼‚å¸¸: {e}")
                    # å¦‚æœæ˜¯meta tensoré”™è¯¯ï¼Œå°è¯•é‡è¯•ä¸€æ¬¡
                    if "meta tensor" in str(e) or "Cannot copy out" in str(e):
                        print(f"æ£€æµ‹åˆ°meta tensoré”™è¯¯ï¼Œæ¸…ç†å†…å­˜åé‡è¯•GPU{i}...")
                        try:
                            torch.cuda.empty_cache()
                            import gc
                            gc.collect()
                            result = init_gpu_model(i)
                            if result is not None:
                                successful_gpus.append(i)
                                print(f"âœ… GPU{i} é‡è¯•æˆåŠŸ")
                            else:
                                print(f"âŒ GPU{i} é‡è¯•å¤±è´¥")
                        except Exception as retry_e:
                            print(f"âŒ GPU{i} é‡è¯•å¼‚å¸¸: {retry_e}")
            
            if len(successful_gpus) == 0:
                print("æ‰€æœ‰GPUåˆå§‹åŒ–éƒ½å¤±è´¥äº†")
                return False
            elif len(successful_gpus) < self.gpu_count:
                print(f"éƒ¨åˆ†GPUåˆå§‹åŒ–æˆåŠŸ: {successful_gpus}/{list(range(self.gpu_count))}")
                # æ›´æ–°å¯ç”¨GPUåˆ—è¡¨
                self.devices = [f'cuda:{i}' for i in successful_gpus]
                self.gpu_count = len(successful_gpus)
                print(f"è°ƒæ•´ä¸ºä½¿ç”¨{self.gpu_count}ä¸ªGPU: {self.devices}")
            else:
                print(f"æ‰€æœ‰{self.gpu_count}ä¸ªGPUåˆå§‹åŒ–å®Œæˆ")
            
            # å…±äº«ç»„ä»¶åˆå§‹åŒ–ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
            print("åˆå§‹åŒ–å…±äº«ç»„ä»¶...")
            device0 = self.devices[0]
            
            # Whisperå’ŒAudioProcessoråœ¨CPUä¸Šï¼Œæ‰€æœ‰GPUå…±äº«
            whisper_dir = "./models/whisper"
            if WHISPER_AVAILABLE and os.path.exists(whisper_dir):
                print("å¼€å§‹åŠ è½½Whisperæ¨¡å‹...")
                try:
                    self.shared_whisper = WhisperModel.from_pretrained(whisper_dir).eval()
                    print("Whisperæ¨¡å‹åŠ è½½å®Œæˆ")
                except Exception as e:
                    print(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.shared_whisper = None
            else:
                if not WHISPER_AVAILABLE:
                    print("è·³è¿‡Whisperæ¨¡å‹åŠ è½½ - transformers.WhisperModelä¸å¯ç”¨")
                else:
                    print(f"è·³è¿‡Whisperæ¨¡å‹åŠ è½½ - ç›®å½•ä¸å­˜åœ¨: {whisper_dir}")
                self.shared_whisper = None
            
            print("åˆå§‹åŒ–AudioProcessor...")
            try:
                # AudioProcessoréœ€è¦whisperæ¨¡å‹è·¯å¾„
                if os.path.exists(whisper_dir):
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=whisper_dir)
                    print("AudioProcessoråˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"è­¦å‘Š: Whisperç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤AudioProcessor")
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=None)
                    print("AudioProcessoråˆå§‹åŒ–å®Œæˆ (æ— Whisper)")
            except Exception as e:
                print(f"AudioProcessoråˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„AudioProcessorå¤‡ç”¨å®ä¾‹
                try:
                    print("å°è¯•åˆ›å»ºå¤‡ç”¨AudioProcessor...")
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=None)
                    print("å¤‡ç”¨AudioProcessoråˆ›å»ºæˆåŠŸ")
                except:
                    self.shared_audio_processor = None
                    print("AudioProcessorå®Œå…¨å¤±è´¥ï¼ŒéŸ³é¢‘åŠŸèƒ½å°†ä¸å¯ç”¨")
            
            print("åˆå§‹åŒ–FaceParsing...")
            try:
                self.shared_fp = FaceParsing()
                print("FaceParsingåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"FaceParsingåˆå§‹åŒ–å¤±è´¥: {e}")
                self.shared_fp = None
            
            # æ—¶é—´æ­¥é•¿
            print("è®¾ç½®æ—¶é—´æ­¥é•¿...")
            self.timesteps = torch.tensor([0], device=device0, dtype=torch.long)
            print("æ—¶é—´æ­¥é•¿è®¾ç½®å®Œæˆ")
            
            init_time = time.time() - start_time
            print(f"æé€Ÿåˆå§‹åŒ–å®Œæˆï¼è€—æ—¶: {init_time:.2f}ç§’")
            print(f"{self.gpu_count}GPUå¹¶è¡Œå¼•æ“å°±ç»ª - æ¯«ç§’çº§å“åº”æ¨¡å¼")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"æé€Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_optimal_gpu(self):
        """æ™ºèƒ½GPUè´Ÿè½½å‡è¡¡"""
        # é€‰æ‹©ä½¿ç”¨ç‡æœ€ä½çš„GPU
        optimal_gpu = min(self.gpu_usage.items(), key=lambda x: x[1])[0]
        self.gpu_usage[optimal_gpu] += 1
        return optimal_gpu
    
    def release_gpu(self, device):
        """é‡Šæ”¾GPUèµ„æº"""
        if device in self.gpu_usage:
            self.gpu_usage[device] = max(0, self.gpu_usage[device] - 1)
    
    def ultra_fast_inference_parallel(self, template_id, audio_path, output_path, cache_dir, batch_size=16, fps=25):
        """æé€Ÿå¹¶è¡Œæ¨ç† - æ¯«ç§’çº§å“åº”"""
        if not self.is_initialized:
            print("æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
        
        try:
            total_start = time.time()
            print(f"å¼€å§‹æé€Ÿå¹¶è¡Œæ¨ç†: {template_id}")
            
            # 1. å¹¶è¡ŒåŠ è½½æ¨¡æ¿ç¼“å­˜ + éŸ³é¢‘ç‰¹å¾æå–
            def load_template_cache_async():
                return self.load_template_cache_optimized(cache_dir, template_id)
            
            def extract_audio_features_async():
                return self.extract_audio_features_ultra_fast(audio_path, fps)
            
            # å…³é”®ä¼˜åŒ–ï¼šå¹¶è¡Œæ‰§è¡Œç¼“å­˜åŠ è½½å’ŒéŸ³é¢‘å¤„ç†
            with ThreadPoolExecutor(max_workers=2) as prep_executor:
                cache_future = prep_executor.submit(load_template_cache_async)
                audio_future = prep_executor.submit(extract_audio_features_async)
                
                cache_data = cache_future.result()
                whisper_chunks = audio_future.result()
            
            if not cache_data:
                print("é”™è¯¯: æ— æ³•åŠ è½½æ¨¡æ¿ç¼“å­˜")
                return False
                
            if whisper_chunks is None:
                print("é”™è¯¯: éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥")
                return False
            
            prep_time = time.time() - total_start
            print(f"å¹¶è¡Œé¢„å¤„ç†å®Œæˆ: {prep_time:.3f}s")
            
            # 2. æé€Ÿ4GPUå¹¶è¡Œæ¨ç†
            inference_start = time.time()
            res_frame_list = self.execute_4gpu_parallel_inference(
                whisper_chunks, cache_data, batch_size
            )
            inference_time = time.time() - inference_start
            print(f"4GPUå¹¶è¡Œæ¨ç†å®Œæˆ: {inference_time:.3f}s, {len(res_frame_list)}å¸§")
            
            # 3. æé€Ÿå¹¶è¡Œå›¾åƒåˆæˆ
            compose_start = time.time()
            video_frames = self.ultra_fast_compose_frames(res_frame_list, cache_data)
            compose_time = time.time() - compose_start
            print(f"ğŸ¨ å¹¶è¡Œå›¾åƒåˆæˆå®Œæˆ: {compose_time:.3f}s")
            
            # 4. æé€Ÿè§†é¢‘ç”Ÿæˆ
            video_start = time.time()
            success = self.generate_video_ultra_fast(video_frames, audio_path, output_path, fps)
            video_time = time.time() - video_start
            print(f"è§†é¢‘ç”Ÿæˆå®Œæˆ: {video_time:.3f}s")
            
            total_time = time.time() - total_start
            print(f"æé€Ÿæ¨ç†å®Œæˆï¼æ€»è€—æ—¶: {total_time:.3f}s")
            print(f"æ€§èƒ½åˆ†è§£: é¢„å¤„ç†:{prep_time:.3f}s + æ¨ç†:{inference_time:.3f}s + åˆæˆ:{compose_time:.3f}s + è§†é¢‘:{video_time:.3f}s")
            
            # è®°å½•æ€§èƒ½æ•°æ®
            if PERFORMANCE_MONITORING:
                record_performance(inference_time, compose_time, video_time, total_time)
            
            return success
            
        except Exception as e:
            print(f"æé€Ÿæ¨ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def execute_4gpu_parallel_inference(self, whisper_chunks, cache_data, batch_size):
        """çœŸæ­£çš„4GPUå¹¶è¡Œæ¨ç† - æ— é”è®¾è®¡"""
        from musetalk.utils.utils import datagen
        
        input_latent_list_cycle = cache_data['input_latent_list_cycle']
        video_num = len(whisper_chunks)
        
        # ç”Ÿæˆæ‰€æœ‰æ‰¹æ¬¡
        gen = datagen(
            whisper_chunks=whisper_chunks,
            vae_encode_latents=input_latent_list_cycle,
            batch_size=batch_size,
            delay_frame=0,
            device=self.devices[0]  # æ•°æ®ç”Ÿæˆåœ¨GPU0
        )
        all_batches = list(gen)
        total_batches = len(all_batches)
        
        print(f"4GPUå¹¶è¡Œå¤„ç† {total_batches} æ‰¹æ¬¡...")
        
        # å…³é”®ä¼˜åŒ–ï¼šæ¯ä¸ªGPUå¤„ç†ç‹¬ç«‹çš„æ‰¹æ¬¡ï¼Œæ— éœ€åŒæ­¥
        def process_batch_on_gpu(batch_info):
            batch_idx, (whisper_batch, latent_batch) = batch_info
            
            # æ™ºèƒ½GPUåˆ†é…
            target_device = self.devices[batch_idx % self.gpu_count]
            gpu_models = self.gpu_models[target_device]
            
            try:
                # å…³é”®ï¼šæ•°æ®ç§»åŠ¨åˆ°ç›®æ ‡GPU
                with torch.cuda.device(target_device):
                    whisper_batch = whisper_batch.to(target_device, dtype=self.weight_dtype, non_blocking=True)
                    latent_batch = latent_batch.to(target_device, dtype=self.weight_dtype, non_blocking=True)
                    timesteps = self.timesteps.to(target_device)
                    
                    # æ ¸å¿ƒæ¨ç† - ä½¿ç”¨ç‹¬ç«‹çš„GPUæ¨¡å‹
                    with torch.no_grad():
                        audio_features = gpu_models['pe'](whisper_batch)
                        pred_latents = gpu_models['unet'].model(
                            latent_batch, timesteps, encoder_hidden_states=audio_features
                        ).sample
                        recon_frames = gpu_models['vae'].decode_latents(pred_latents)
                    
                    # ç«‹å³ç§»å›CPUé‡Šæ”¾GPUå†…å­˜
                    result_frames = [frame.cpu().numpy() for frame in recon_frames]
                    
                    # æ¸…ç†GPUå†…å­˜
                    del whisper_batch, latent_batch, audio_features, pred_latents, recon_frames
                    torch.cuda.empty_cache()
                    
                    return batch_idx, result_frames
                    
            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_idx} GPU {target_device} å¤±è´¥: {str(e)}")
                return batch_idx, []
        
        # çœŸæ­£çš„4GPUå¹¶è¡Œæ‰§è¡Œ
        res_frame_list = []
        batch_results = {}
        
        with ThreadPoolExecutor(max_workers=self.gpu_count) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡
            futures = {
                executor.submit(process_batch_on_gpu, (i, batch)): i 
                for i, batch in enumerate(all_batches)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                batch_idx, frames = future.result()
                batch_results[batch_idx] = frames
        
        # æŒ‰é¡ºåºåˆå¹¶ç»“æœ
        for i in range(total_batches):
            if i in batch_results:
                res_frame_list.extend(batch_results[i])
        
        return res_frame_list
    
    def ultra_fast_compose_frames(self, res_frame_list, cache_data):
        """æé€Ÿå¹¶è¡Œå›¾åƒåˆæˆ - 32çº¿ç¨‹"""
        coord_list_cycle = cache_data['coord_list_cycle']
        frame_list_cycle = cache_data['frame_list_cycle']
        mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
        mask_list_cycle = cache_data['mask_list_cycle']
        
        print(f"ğŸ¨ å¼€å§‹32çº¿ç¨‹å¹¶è¡Œåˆæˆ {len(res_frame_list)} å¸§...")
        
        def compose_single_frame(frame_info):
            i, res_frame = frame_info
            try:
                bbox = coord_list_cycle[i % len(coord_list_cycle)]
                ori_frame = copy.deepcopy(frame_list_cycle[i % len(frame_list_cycle)])
                
                x1, y1, x2, y2 = bbox
                res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                
                # ä½¿ç”¨ä¼˜åŒ–çš„blending
                mask_coords = mask_coords_list_cycle[i % len(mask_coords_list_cycle)]
                mask = mask_list_cycle[i % len(mask_list_cycle)]
                
                combine_frame = get_image_blending(
                    image=ori_frame,
                    face=res_frame, 
                    face_box=[x1, y1, x2, y2],
                    mask_array=mask,
                    crop_box=mask_coords
                )
                
                return i, combine_frame
                
            except Exception as e:
                print(f"åˆæˆç¬¬{i}å¸§å¤±è´¥: {str(e)}")
                return i, None
        
        # 32çº¿ç¨‹å¹¶è¡Œåˆæˆ
        composed_frames = {}
        with ThreadPoolExecutor(max_workers=32) as executor:
            frame_futures = {
                executor.submit(compose_single_frame, (i, frame)): i 
                for i, frame in enumerate(res_frame_list)
            }
            
            for future in as_completed(frame_futures):
                frame_idx, composed_frame = future.result()
                if composed_frame is not None:
                    composed_frames[frame_idx] = composed_frame
        
        # æŒ‰é¡ºåºæ’åˆ—
        video_frames = []
        for i in range(len(res_frame_list)):
            if i in composed_frames:
                video_frames.append(composed_frames[i])
        
        print(f"å¹¶è¡Œåˆæˆå®Œæˆ: {len(video_frames)} å¸§")
        return video_frames
    
    def extract_audio_features_ultra_fast(self, audio_path, fps):
        """æé€ŸéŸ³é¢‘ç‰¹å¾æå–"""
        try:
            # æ£€æŸ¥AudioProcessoræ˜¯å¦å¯ç”¨
            if self.shared_audio_processor is None:
                raise ValueError("AudioProcessoræœªåˆå§‹åŒ–")
                
            whisper_input_features, librosa_length = self.shared_audio_processor.get_audio_feature(audio_path)
            whisper_chunks = self.shared_audio_processor.get_whisper_chunk(
                whisper_input_features, 
                self.devices[0],  # Whisperåœ¨GPU0
                self.weight_dtype, 
                self.shared_whisper, 
                librosa_length,
                fps=fps,
                audio_padding_length_left=2,
                audio_padding_length_right=2,
            )
            return whisper_chunks
        except Exception as e:
            print(f"éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {str(e)}")
            return None
    
    def load_template_cache_optimized(self, cache_dir, template_id):
        """ä¼˜åŒ–çš„æ¨¡æ¿ç¼“å­˜åŠ è½½"""
        try:
            cache_file = os.path.join(cache_dir, f"{template_id}_preprocessed.pkl")
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            return cache_data
            
        except Exception as e:
            print(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def generate_video_ultra_fast(self, video_frames, audio_path, output_path, fps):
        """æé€Ÿè§†é¢‘ç”Ÿæˆ"""
        try:
            # ç›´æ¥å†…å­˜ç”Ÿæˆï¼Œæ— ä¸´æ—¶æ–‡ä»¶
            print(f"ç›´æ¥ç”Ÿæˆè§†é¢‘: {len(video_frames)} å¸§")
            
            # ç”Ÿæˆæ— éŸ³é¢‘è§†é¢‘
            temp_video = output_path.replace('.mp4', '_temp.mp4')
            imageio.mimwrite(
                temp_video, video_frames, 'FFMPEG', 
                fps=fps, codec='libx264', pixelformat='yuv420p',
                output_params=['-preset', 'ultrafast', '-crf', '23']
            )
            
            # å¹¶è¡ŒéŸ³é¢‘åˆæˆ
            try:
                from moviepy.editor import VideoFileClip, AudioFileClip
                video_clip = VideoFileClip(temp_video)
                audio_clip = AudioFileClip(audio_path)
                
                final_clip = video_clip.set_audio(audio_clip)
                final_clip.write_videofile(
                    output_path, 
                    codec='libx264', 
                    audio_codec='aac', 
                    fps=fps,
                    preset='ultrafast',
                    verbose=False, 
                    logger=None
                )
                
                final_clip.close()
                audio_clip.close()
                video_clip.close()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_video)
                
            except Exception as e:
                print(f"éŸ³é¢‘åˆæˆå¤±è´¥ï¼Œä½¿ç”¨æ— éŸ³é¢‘ç‰ˆæœ¬: {str(e)}")
                os.rename(temp_video, output_path)
            
            return True
            
        except Exception as e:
            print(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
            return False

# å…¨å±€æœåŠ¡å®ä¾‹
global_service = UltraFastMuseTalkService()

def start_ultra_fast_service(port=28888):
    """å¯åŠ¨æé€ŸæœåŠ¡"""
    print(f"å¯åŠ¨Ultra Fast Service - ç«¯å£: {port}")
    
    # ç¡®ä¿å·¥ä½œç›®å½•å’Œè·¯å¾„æ­£ç¡®
    import os
    from pathlib import Path
    
    # å¦‚æœä¸åœ¨æ­£ç¡®çš„å·¥ä½œç›®å½•ï¼Œåˆ‡æ¢åˆ°MuseTalkç›®å½•
    current_dir = Path.cwd()
    if not (current_dir / "models" / "musetalkV15" / "unet.pth").exists():
        # å°è¯•æ‰¾åˆ°MuseTalkç›®å½•
        script_dir = Path(__file__).parent
        musetalk_dir = script_dir.parent / "MuseTalk"
        if musetalk_dir.exists():
            os.chdir(musetalk_dir)
            print(f"å·¥ä½œç›®å½•åˆ‡æ¢åˆ°: {musetalk_dir}")
        else:
            print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°MuseTalkæ¨¡å‹ç›®å½•")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("å¼€å§‹åˆå§‹åŒ–Ultra Fastæ¨¡å‹...")
    try:
        if not global_service.initialize_models_ultra_fast():
            print("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ - è¿”å›False")
            return
        print("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"æ¨¡å‹åˆå§‹åŒ–å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # å¯åŠ¨æ€§èƒ½ç›‘æ§
    if PERFORMANCE_MONITORING:
        start_performance_monitoring()
        print("æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    # å¯åŠ¨IPCæœåŠ¡å™¨
    try:
        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_socket.bind(('127.0.0.1', port))
        server_socket.listen(5)
        
        print(f"Ultra Fast Service å°±ç»ª - ç›‘å¬ç«¯å£: {port}")
        print("æ¯«ç§’çº§å“åº”æ¨¡å¼å·²å¯ç”¨")
        
        while True:
            try:
                client_socket, addr = server_socket.accept()
                print(f"ğŸ”— å®¢æˆ·ç«¯è¿æ¥: {addr}")
                
                # å¤„ç†è¯·æ±‚
                threading.Thread(
                    target=handle_client_ultra_fast, 
                    args=(client_socket,)
                ).start()
                
            except Exception as e:
                print(f"è¿æ¥å¤„ç†å¤±è´¥: {str(e)}")
                
    except Exception as e:
        print(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")

def handle_client_ultra_fast(client_socket):
    """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚ - æé€Ÿç‰ˆæœ¬"""
    try:
        # æ¥æ”¶è¯·æ±‚
        data_length = struct.unpack('I', client_socket.recv(4))[0]
        data = client_socket.recv(data_length).decode('utf-8')
        request = json.loads(data)
        
        print(f"ğŸ“¨ æé€Ÿæ¨ç†è¯·æ±‚: {request['template_id']}")
        
        # æé€Ÿæ¨ç†
        start_time = time.time()
        success = global_service.ultra_fast_inference_parallel(
            template_id=request['template_id'],
            audio_path=request['audio_path'],
            output_path=request['output_path'],
            cache_dir=request['cache_dir'],
            batch_size=request.get('batch_size', 16),
            fps=request.get('fps', 25)
        )
        
        process_time = time.time() - start_time
        print(f"æé€Ÿæ¨ç†å®Œæˆ: {process_time:.3f}s, ç»“æœ: {success}")
        
        # å‘é€å“åº”
        response = {'Success': success, 'OutputPath': request['output_path'] if success else None}
        response_data = json.dumps(response).encode('utf-8')
        client_socket.send(struct.pack('I', len(response_data)))
        client_socket.send(response_data)
        
    except Exception as e:
        print(f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        client_socket.close()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, default=28888, help='æœåŠ¡ç«¯å£')
    args = parser.parse_args()
    
    start_ultra_fast_service(args.port)
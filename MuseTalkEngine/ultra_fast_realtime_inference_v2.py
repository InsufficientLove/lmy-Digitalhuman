#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultra Fast Realtime Inference V2
æè‡´ä¼˜åŒ–ç‰ˆæœ¬ - ç›®æ ‡ï¼šæ¯«ç§’çº§å“åº”ï¼Œ4GPUçœŸå¹¶è¡Œï¼Œé›¶ç­‰å¾…
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import time
import gc
import threading
import queue
import socket
import struct
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import copy
import gc
try:
    from transformers import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: transformers.WhisperModelä¸å¯ç”¨ï¼Œå°†è·³è¿‡Whisperåˆå§‹åŒ–")
    WHISPER_AVAILABLE = False
import imageio
import warnings
warnings.filterwarnings("ignore")

# GPUé…ç½® - ç›´æ¥å®šä¹‰ï¼Œä¸ä»å¤–éƒ¨å¯¼å…¥
GPU_MEMORY_CONFIG = {'batch_size': {'default': 4}}
print("ä½¿ç”¨é»˜è®¤GPUé…ç½®")

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image, get_image_blending, get_image_prepare_material
from musetalk.utils.audio_processor import AudioProcessor

# æ€§èƒ½ç›‘æ§ - å·²ç§»é™¤ï¼Œä½¿ç”¨ç®€å•çš„æ—¶é—´è®°å½•
PERFORMANCE_MONITORING = False
print("æ€§èƒ½ç›‘æ§å·²ç¦ç”¨")

print("Ultra Fast Realtime Inference V2 - æ¯«ç§’çº§å“åº”å¼•æ“")
sys.stdout.flush()

class UltraFastMuseTalkService:
    """æè‡´ä¼˜åŒ–çš„MuseTalkæœåŠ¡ - æ¯«ç§’çº§å“åº”"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        # 4GPUå¹¶è¡Œæ¶æ„
        self.gpu_count = min(4, torch.cuda.device_count())
        self.devices = [f'cuda:{i}' for i in range(self.gpu_count)]
        
        # æ¯ä¸ªGPUç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹
        self.gpu_models = {}
        self.gpu_locks = {device: threading.Lock() for device in self.devices}
        
        # å…¨å±€æ¨¡å‹ç»„ä»¶ï¼ˆå…±äº«æƒé‡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        self.shared_vae = None
        self.shared_unet = None
        self.shared_pe = None
        self.shared_whisper = None
        self.shared_audio_processor = None
        self.shared_fp = None
        self.weight_dtype = torch.float16  # ä½¿ç”¨åŠç²¾åº¦æé€Ÿ
        self.timesteps = None
        
        # å†…å­˜æ± å’Œç¼“å­˜ä¼˜åŒ–
        self.template_cache = {}
        self.audio_feature_cache = {}
        self.frame_buffer_pool = queue.Queue(maxsize=1000)
        
        # æé€Ÿå¤„ç†ç®¡é“
        self.inference_executor = ThreadPoolExecutor(max_workers=self.gpu_count)
        self.compose_executor = ThreadPoolExecutor(max_workers=32)  # 32çº¿ç¨‹å¹¶è¡Œåˆæˆ
        self.video_executor = ThreadPoolExecutor(max_workers=4)
        
        # ğŸ® GPUè´Ÿè½½å‡è¡¡
        self.gpu_usage = {device: 0 for device in self.devices}
        self.gpu_queue = {device: queue.Queue(maxsize=10) for device in self.devices}
        
        self.is_initialized = False
        self._initialized = True
        
        print(f"Ultra Fast Service åˆå§‹åŒ–å®Œæˆ - {self.gpu_count}GPUå¹¶è¡Œæ¶æ„")
        sys.stdout.flush()
    
    def initialize_models_ultra_fast(self):
        """æé€Ÿåˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ - å¹¶è¡ŒåŠ è½½åˆ°æ‰€æœ‰GPU"""
        if self.is_initialized and len(self.gpu_models) > 0:
            print("æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return True
        
        # é‡ç½®åˆå§‹åŒ–çŠ¶æ€ï¼Œå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        self.is_initialized = False
        self.gpu_models = {}
            
        try:
            print(f"å¼€å§‹æé€Ÿåˆå§‹åŒ– - {self.gpu_count}GPUå¹¶è¡ŒåŠ è½½...")
            start_time = time.time()
            
            # å¹¶è¡Œåˆå§‹åŒ–æ‰€æœ‰GPUæ¨¡å‹
            def init_gpu_model(device_id):
                device = f'cuda:{device_id}'
                print(f"ğŸ® GPU{device_id} å¼€å§‹åˆå§‹åŒ–...")
                
                with torch.cuda.device(device_id):
                    # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šGPU - åªä½¿ç”¨å¯ç”¨çš„sd-vae
                    try:
                        print(f"GPU{device_id} å¼€å§‹åŠ è½½æ¨¡å‹...")
                        # æ£€æŸ¥sd-vaeç›®å½•æ˜¯å¦å®Œæ•´
                        sd_vae_path = "./models/sd-vae"
                        config_file = os.path.join(sd_vae_path, "config.json")
                        
                        if os.path.exists(config_file):
                            print(f"GPU{device_id} ä½¿ç”¨å®Œæ•´çš„sd-vaeæ¨¡å‹")
                            vae, unet, pe = load_all_model(vae_type="sd-vae")
                            print(f"GPU{device_id} æ¨¡å‹åŠ è½½æˆåŠŸ")
                        else:
                            print(f"GPU{device_id} sd-vaeé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤GPU")
                            return None
                            
                    except Exception as e:
                        print(f"GPU{device_id} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        # æ£€æŸ¥æ˜¯å¦æ˜¯UNetæ¨¡å‹é—®é¢˜
                        if "meta tensor" in str(e) or "Cannot copy out" in str(e):
                            print(f"GPU{device_id} UNetæ¨¡å‹æ–‡ä»¶å¯èƒ½æŸåï¼Œå°è¯•é‡æ–°åŠ è½½...")
                            try:
                                # å¼ºåˆ¶æ¸…ç†GPUå†…å­˜
                                torch.cuda.empty_cache()
                                # é‡æ–°å°è¯•åŠ è½½
                                vae, unet, pe = load_all_model(vae_type="sd-vae")
                                print(f"GPU{device_id} é‡æ–°åŠ è½½æˆåŠŸ")
                            except Exception as e3:
                                print(f"GPU{device_id} é‡æ–°åŠ è½½ä¹Ÿå¤±è´¥: {e3}")
                                return None
                        else:
                            print(f"GPU{device_id} å…¶ä»–é”™è¯¯ï¼Œè·³è¿‡æ­¤GPU")
                            return None
                    
                    # ä¼˜åŒ–æ¨¡å‹ - åŠç²¾åº¦+ç¼–è¯‘ä¼˜åŒ– (ä¿®å¤æ¨¡å‹å¯¹è±¡å…¼å®¹æ€§)
                    print(f"GPU{device_id} å¼€å§‹æ¨¡å‹ä¼˜åŒ–...")
                    
                    # ä¿®å¤VAEå¯¹è±¡ - ä½¿ç”¨.vaeå±æ€§
                    if hasattr(vae, 'vae'):
                        vae.vae = vae.vae.to(device).half().eval()
                    elif hasattr(vae, 'to'):
                        vae = vae.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: VAEå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    # ä¿®å¤UNetå¯¹è±¡ - ä½¿ç”¨.modelå±æ€§  
                    if hasattr(unet, 'model'):
                        unet.model = unet.model.to(device).half().eval()
                    elif hasattr(unet, 'to'):
                        unet = unet.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: UNetå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    # ä¿®å¤PEå¯¹è±¡
                    if hasattr(pe, 'to'):
                        pe = pe.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: PEå¯¹è±¡æ²¡æœ‰.to()æ–¹æ³•ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    print(f"GPU{device_id} åŠç²¾åº¦è½¬æ¢å®Œæˆ")
                    
                    # å…³é”®ä¼˜åŒ–ï¼šæ¨¡å‹ç¼–è¯‘åŠ é€Ÿ (ä»…åœ¨Linuxä¸Šå¯ç”¨)
                    import platform
                    if hasattr(torch, 'compile') and platform.system() != 'Windows':
                        try:
                            print(f"GPU{device_id} å¼€å§‹æ¨¡å‹ç¼–è¯‘...")
                            unet.model = torch.compile(unet.model, mode="reduce-overhead")
                            vae.vae = torch.compile(vae.vae, mode="reduce-overhead")
                            pe = torch.compile(pe, mode="reduce-overhead")
                            print(f"GPU{device_id} æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å®Œæˆ")
                        except Exception as compile_error:
                            print(f"GPU{device_id} æ¨¡å‹ç¼–è¯‘å¤±è´¥: {compile_error}")
                            print(f"GPU{device_id} ä½¿ç”¨åŸå§‹æ¨¡å‹")
                    else:
                        if platform.system() == 'Windows':
                            print(f"GPU{device_id} è·³è¿‡æ¨¡å‹ç¼–è¯‘ (Windowsä¸æ”¯æŒ)")
                        else:
                            print(f"GPU{device_id} è·³è¿‡æ¨¡å‹ç¼–è¯‘ (torch.compileä¸å¯ç”¨)")
                    
                    self.gpu_models[device] = {
                        'vae': vae,
                        'unet': unet,
                        'pe': pe,
                        'device': device
                    }
                    
                    print(f"GPU{device_id} æ¨¡å‹åŠ è½½å®Œæˆ")
                    return device_id
            
            # SEQUENTIAL_LOADING_FIXED: é¡ºåºåˆå§‹åŒ–é¿å…å¹¶å‘å†²çª
            print(f"å¼€å§‹é¡ºåºåˆå§‹åŒ–{self.gpu_count}ä¸ªGPUï¼ˆé¿å…å¹¶å‘å†²çªï¼‰...")
            successful_gpus = []
            
            for i in range(self.gpu_count):
                print(f"æ­£åœ¨åˆå§‹åŒ–GPU {i}/{self.gpu_count}...")
                try:
                    # åœ¨æ¯ä¸ªGPUåˆå§‹åŒ–å‰æ¸…ç†å†…å­˜
                    torch.cuda.set_device(i)
                    torch.cuda.empty_cache()
                    
                    result = init_gpu_model(i)
                    if result is not None:
                        successful_gpus.append(i)
                        print(f"âœ… GPU{i} åˆå§‹åŒ–æˆåŠŸ ({len(successful_gpus)}/{self.gpu_count})")
                    else:
                        print(f"âŒ GPU{i} åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡")
                except Exception as e:
                    print(f"âŒ GPU{i} åˆå§‹åŒ–å¼‚å¸¸: {e}")
                    # å¦‚æœæ˜¯meta tensoré”™è¯¯ï¼Œå°è¯•é‡è¯•ä¸€æ¬¡
                    if "meta tensor" in str(e) or "Cannot copy out" in str(e):
                        print(f"æ£€æµ‹åˆ°meta tensoré”™è¯¯ï¼Œæ¸…ç†å†…å­˜åé‡è¯•GPU{i}...")
                        try:
                            torch.cuda.empty_cache()
                            import gc
                            gc.collect()
                            result = init_gpu_model(i)
                            if result is not None:
                                successful_gpus.append(i)
                                print(f"âœ… GPU{i} é‡è¯•æˆåŠŸ")
                            else:
                                print(f"âŒ GPU{i} é‡è¯•å¤±è´¥")
                        except Exception as retry_e:
                            print(f"âŒ GPU{i} é‡è¯•å¼‚å¸¸: {retry_e}")
            
            if len(successful_gpus) == 0:
                print("æ‰€æœ‰GPUåˆå§‹åŒ–éƒ½å¤±è´¥äº†")
                return False
            elif len(successful_gpus) < self.gpu_count:
                print(f"éƒ¨åˆ†GPUåˆå§‹åŒ–æˆåŠŸ: {successful_gpus}/{list(range(self.gpu_count))}")
                # æ›´æ–°å¯ç”¨GPUåˆ—è¡¨
                self.devices = [f'cuda:{i}' for i in successful_gpus]
                self.gpu_count = len(successful_gpus)
                print(f"è°ƒæ•´ä¸ºä½¿ç”¨{self.gpu_count}ä¸ªGPU: {self.devices}")
            else:
                print(f"æ‰€æœ‰{self.gpu_count}ä¸ªGPUåˆå§‹åŒ–å®Œæˆ")
            
            # å…±äº«ç»„ä»¶åˆå§‹åŒ–ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
            print("åˆå§‹åŒ–å…±äº«ç»„ä»¶...")
            device0 = self.devices[0]
            
            # Whisperå’ŒAudioProcessoråœ¨CPUä¸Šï¼Œæ‰€æœ‰GPUå…±äº«
            whisper_dir = "./models/whisper"
            if WHISPER_AVAILABLE and os.path.exists(whisper_dir):
                print("å¼€å§‹åŠ è½½Whisperæ¨¡å‹...")
                try:
                    self.shared_whisper = WhisperModel.from_pretrained(whisper_dir).eval()
                    # å°†Whisperæ¨¡å‹ç§»åˆ°GPU0å¹¶ä¿æŒfloat32ï¼ˆWhisperä¸æ”¯æŒhalfï¼‰
                    if torch.cuda.is_available():
                        self.shared_whisper = self.shared_whisper.to(self.devices[0])
                        print(f"Whisperæ¨¡å‹åŠ è½½å®Œæˆï¼Œå·²ç§»è‡³{self.devices[0]}")
                    else:
                        print("Whisperæ¨¡å‹åŠ è½½å®Œæˆï¼ˆCPUæ¨¡å¼ï¼‰")
                except Exception as e:
                    print(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.shared_whisper = None
            else:
                if not WHISPER_AVAILABLE:
                    print("è·³è¿‡Whisperæ¨¡å‹åŠ è½½ - transformers.WhisperModelä¸å¯ç”¨")
                else:
                    print(f"è·³è¿‡Whisperæ¨¡å‹åŠ è½½ - ç›®å½•ä¸å­˜åœ¨: {whisper_dir}")
                self.shared_whisper = None
            
            print("åˆå§‹åŒ–AudioProcessor...")
            try:
                # AudioProcessoréœ€è¦whisperæ¨¡å‹è·¯å¾„
                if os.path.exists(whisper_dir):
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=whisper_dir)
                    print("AudioProcessoråˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"è­¦å‘Š: Whisperç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤AudioProcessor")
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=None)
                    print("AudioProcessoråˆå§‹åŒ–å®Œæˆ (æ— Whisper)")
            except Exception as e:
                print(f"AudioProcessoråˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„AudioProcessorå¤‡ç”¨å®ä¾‹
                try:
                    print("å°è¯•åˆ›å»ºå¤‡ç”¨AudioProcessor...")
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=None)
                    print("å¤‡ç”¨AudioProcessoråˆ›å»ºæˆåŠŸ")
                except:
                    self.shared_audio_processor = None
                    print("AudioProcessorå®Œå…¨å¤±è´¥ï¼ŒéŸ³é¢‘åŠŸèƒ½å°†ä¸å¯ç”¨")
            
            print("åˆå§‹åŒ–FaceParsing...")
            try:
                self.shared_fp = FaceParsing()
                print("FaceParsingåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"FaceParsingåˆå§‹åŒ–å¤±è´¥: {e}")
                self.shared_fp = None
            
            # æ—¶é—´æ­¥é•¿
            print("è®¾ç½®æ—¶é—´æ­¥é•¿...")
            self.timesteps = torch.tensor([0], device=device0, dtype=torch.long)
            print("æ—¶é—´æ­¥é•¿è®¾ç½®å®Œæˆ")
            
            init_time = time.time() - start_time
            print(f"æé€Ÿåˆå§‹åŒ–å®Œæˆï¼è€—æ—¶: {init_time:.2f}ç§’")
            print(f"{self.gpu_count}GPUå¹¶è¡Œå¼•æ“å°±ç»ª - æ¯«ç§’çº§å“åº”æ¨¡å¼")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"æé€Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_optimal_gpu(self):
        """æ™ºèƒ½GPUè´Ÿè½½å‡è¡¡"""
        # é€‰æ‹©ä½¿ç”¨ç‡æœ€ä½çš„GPU
        optimal_gpu = min(self.gpu_usage.items(), key=lambda x: x[1])[0]
        self.gpu_usage[optimal_gpu] += 1
        return optimal_gpu
    
    def release_gpu(self, device):
        """é‡Šæ”¾GPUèµ„æº"""
        if device in self.gpu_usage:
            self.gpu_usage[device] = max(0, self.gpu_usage[device] - 1)
    
    def ultra_fast_inference_parallel(self, template_id, audio_path, output_path, cache_dir, batch_size=6, fps=25):
        """æé€Ÿå¹¶è¡Œæ¨ç† - æ¯«ç§’çº§å“åº”"""
        print(f"ğŸ” ultra_fast_inference_parallel æ¥æ”¶åˆ° batch_size={batch_size}")
        
        if not self.is_initialized:
            print("æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
        
        try:
            total_start = time.time()
            print(f"å¼€å§‹æé€Ÿå¹¶è¡Œæ¨ç†: {template_id}")
            
            # 1. å¹¶è¡ŒåŠ è½½æ¨¡æ¿ç¼“å­˜ + éŸ³é¢‘ç‰¹å¾æå–
            def load_template_cache_async():
                return self.load_template_cache_optimized(cache_dir, template_id)
            
            def extract_audio_features_async():
                return self.extract_audio_features_ultra_fast(audio_path, fps)
            
            # å…³é”®ä¼˜åŒ–ï¼šå¹¶è¡Œæ‰§è¡Œç¼“å­˜åŠ è½½å’ŒéŸ³é¢‘å¤„ç†
            with ThreadPoolExecutor(max_workers=2) as prep_executor:
                cache_future = prep_executor.submit(load_template_cache_async)
                audio_future = prep_executor.submit(extract_audio_features_async)
                
                cache_data = cache_future.result()
                whisper_chunks = audio_future.result()
            
            if not cache_data:
                print("é”™è¯¯: æ— æ³•åŠ è½½æ¨¡æ¿ç¼“å­˜")
                return False
                
            if whisper_chunks is None:
                print("é”™è¯¯: éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥")
                return False
            
            prep_time = time.time() - total_start
            print(f"å¹¶è¡Œé¢„å¤„ç†å®Œæˆ: {prep_time:.3f}s")
            
            # 2. æé€Ÿ4GPUå¹¶è¡Œæ¨ç†
            inference_start = time.time()
            res_frame_list = self.execute_4gpu_parallel_inference(
                whisper_chunks, cache_data, batch_size
            )
            inference_time = time.time() - inference_start
            print(f"4GPUå¹¶è¡Œæ¨ç†å®Œæˆ: {inference_time:.3f}s, {len(res_frame_list)}å¸§")
            
            # 3. æé€Ÿå¹¶è¡Œå›¾åƒåˆæˆ
            compose_start = time.time()
            video_frames = self.ultra_fast_compose_frames(res_frame_list, cache_data)
            compose_time = time.time() - compose_start
            print(f"ğŸ¨ å¹¶è¡Œå›¾åƒåˆæˆå®Œæˆ: {compose_time:.3f}s")
            
            # 4. æé€Ÿè§†é¢‘ç”Ÿæˆ
            video_start = time.time()
            success = self.generate_video_ultra_fast(video_frames, audio_path, output_path, fps)
            video_time = time.time() - video_start
            print(f"è§†é¢‘ç”Ÿæˆå®Œæˆ: {video_time:.3f}s")
            
            total_time = time.time() - total_start
            print(f"æé€Ÿæ¨ç†å®Œæˆï¼æ€»è€—æ—¶: {total_time:.3f}s")
            print(f"æ€§èƒ½åˆ†è§£: é¢„å¤„ç†:{prep_time:.3f}s + æ¨ç†:{inference_time:.3f}s + åˆæˆ:{compose_time:.3f}s + è§†é¢‘:{video_time:.3f}s")
            
            # æ€§èƒ½æ•°æ®å·²åœ¨ä¸Šé¢æ‰“å°
            
            return success
            
        except Exception as e:
            print(f"æé€Ÿæ¨ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def execute_4gpu_parallel_inference(self, whisper_chunks, cache_data, batch_size):
        """çœŸæ­£çš„4GPUå¹¶è¡Œæ¨ç† - æ— é”è®¾è®¡"""
        from musetalk.utils.utils import datagen
        
        print(f"âš™ï¸ æ‰§è¡Œ4GPUå¹¶è¡Œæ¨ç†ï¼Œbatch_size={batch_size}")
        
        # æ¨ç†å‰æ¸…ç†æ‰€æœ‰GPUå†…å­˜
        for device in self.devices:
            with torch.cuda.device(device):
                torch.cuda.empty_cache()
        
        input_latent_list_cycle = cache_data['input_latent_list_cycle']
        video_num = len(whisper_chunks)
        
        # æ·»åŠ æ‰¹æ¬¡ä¼˜åŒ–å»ºè®®
        print(f"éŸ³é¢‘å¸§æ•°: {video_num}")
        if video_num > 50 and batch_size < 4:
            # åŸºäºå®é™…GPUå†…å­˜æƒ…å†µçš„å»ºè®®
            if video_num > 100:
                suggested_batch_size = 6  # é•¿éŸ³é¢‘ç”¨æ›´å¤§æ‰¹æ¬¡
            elif video_num > 50:
                suggested_batch_size = 4  # ä¸­ç­‰éŸ³é¢‘
            else:
                suggested_batch_size = 3  # çŸ­éŸ³é¢‘
            
            print(f"âš ï¸ å½“å‰batch_size={batch_size}å¯èƒ½å¤ªå°ï¼Œå»ºè®®ä½¿ç”¨batch_size={suggested_batch_size}")
            print(f"  è¿™å°†å‡å°‘æ‰¹æ¬¡æ•°ä»{video_num // batch_size}åˆ°{video_num // suggested_batch_size}")
            print(f"  é¢„è®¡å¯èŠ‚çœ{(video_num // batch_size - video_num // suggested_batch_size) * 5}ç§’")
        
        # ç”Ÿæˆæ‰€æœ‰æ‰¹æ¬¡
        gen = datagen(
            whisper_chunks=whisper_chunks,
            vae_encode_latents=input_latent_list_cycle,
            batch_size=batch_size,
            delay_frame=0,
            device='cpu'  # åœ¨CPUä¸Šç”Ÿæˆæ•°æ®ï¼Œé¿å…GPU0å†…å­˜å‹åŠ›
        )
        all_batches = list(gen)
        total_batches = len(all_batches)
        
        print(f"4GPUå¹¶è¡Œå¤„ç† {total_batches} æ‰¹æ¬¡...")
        
        # å…³é”®ä¼˜åŒ–ï¼šæ¯ä¸ªGPUå¤„ç†ç‹¬ç«‹çš„æ‰¹æ¬¡ï¼Œæ— éœ€åŒæ­¥
        def process_batch_on_gpu(batch_info):
            batch_idx, (whisper_batch, latent_batch) = batch_info
            
            # æ™ºèƒ½GPUåˆ†é… - ç¡®ä¿ä½¿ç”¨æœ‰æ•ˆçš„GPU
            target_device = self.devices[batch_idx % self.gpu_count]
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿GPUæ¨¡å‹å­˜åœ¨
            if target_device not in self.gpu_models:
                print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx}: GPU {target_device} æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡")
                return batch_idx, []
            
            gpu_models = self.gpu_models[target_device]
            
            print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx} -> GPU {target_device}")
            print(f"  Whisper batch shape: {whisper_batch.shape}")
            print(f"  Latent batch shape: {latent_batch.shape}")
            
            # ä¿®æ­£å†…å­˜ä¼°ç®—ï¼ˆfloat16 = 2 bytes per elementï¼‰
            whisper_memory = whisper_batch.numel() * 2 / 1024**3
            latent_memory = latent_batch.numel() * 2 / 1024**3
            # UNetæ¨ç†éœ€è¦é¢å¤–çš„å†…å­˜ï¼Œé€šå¸¸æ˜¯è¾“å…¥çš„3-4å€
            inference_multiplier = 4.0
            total_memory = (whisper_memory + latent_memory) * inference_multiplier
            print(f"  Estimated memory: Input={whisper_memory + latent_memory:.2f}GB Ã— {inference_multiplier} = {total_memory:.2f}GB")
            
            # GPUå†…å­˜ç›‘æ§å·²ç§»é™¤
            
            try:
                # å†…å­˜å®‰å…¨æ£€æŸ¥ - ä½¿ç”¨æ›´å‡†ç¡®çš„ä¼°ç®—
                max_memory_gb = 15  # é™ä½é™åˆ¶ï¼Œç•™å‡ºå®‰å…¨ä½™é‡
                
                if total_memory > max_memory_gb:
                    print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} å†…å­˜éœ€æ±‚è¿‡å¤§ ({total_memory:.1f}GB > {max_memory_gb}GB)ï¼Œè·³è¿‡")
                    return batch_idx, []
                
                # å…³é”®ï¼šæ•°æ®ç§»åŠ¨åˆ°ç›®æ ‡GPU
                with torch.cuda.device(target_device):
                    whisper_batch = whisper_batch.to(target_device, dtype=self.weight_dtype, non_blocking=True)
                    latent_batch = latent_batch.to(target_device, dtype=self.weight_dtype, non_blocking=True)
                    timesteps = self.timesteps.to(target_device)
                    
                    # æ ¸å¿ƒæ¨ç† - ä½¿ç”¨ç‹¬ç«‹çš„GPUæ¨¡å‹
                    with torch.no_grad():
                        # ç°åœ¨é¢„å¤„ç†ç›´æ¥ç”Ÿæˆ8é€šé“latentï¼Œä¸éœ€è¦å†è¿›è¡Œé€šé“æ•°æ£€æŸ¥å’Œè½¬æ¢
                        audio_features = gpu_models['pe'](whisper_batch)
                        pred_latents = gpu_models['unet'].model(
                            latent_batch, timesteps, encoder_hidden_states=audio_features
                        ).sample
                        recon_frames = gpu_models['vae'].decode_latents(pred_latents)
                    
                    # ç«‹å³ç§»å›CPUé‡Šæ”¾GPUå†…å­˜
                    # æ£€æŸ¥è¿”å›ç±»å‹ï¼Œå¦‚æœå·²ç»æ˜¯numpyæ•°ç»„å°±ç›´æ¥ä½¿ç”¨
                    if isinstance(recon_frames, list):
                        result_frames = recon_frames
                    elif isinstance(recon_frames, np.ndarray):
                        result_frames = [recon_frames[i] for i in range(recon_frames.shape[0])]
                    else:
                        # å¦‚æœæ˜¯torch tensorï¼Œè½¬æ¢ä¸ºnumpy
                        result_frames = [frame.cpu().numpy() if hasattr(frame, 'cpu') else frame for frame in recon_frames]
                    
                    # æ¸…ç†GPUå†…å­˜
                    del whisper_batch, latent_batch, audio_features, pred_latents, recon_frames
                    torch.cuda.empty_cache()
                    
                    return batch_idx, result_frames
                    
            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_idx} GPU {target_device} å¤±è´¥: {str(e)}")
                # å¤±è´¥æ—¶æ¸…ç†GPUå†…å­˜ï¼Œä½†ä¸å¼ºåˆ¶åŒæ­¥
                with torch.cuda.device(target_device):
                    torch.cuda.empty_cache()
                return batch_idx, []
        
        # çœŸæ­£çš„4GPUå¹¶è¡Œæ‰§è¡Œ
        res_frame_list = []
        batch_results = {}
        
        # ä½¿ç”¨æ›´å¤§çš„å¹¶å‘æ•°ï¼Œè®©GPUä¿æŒå¿™ç¢Œ
        max_workers = self.gpu_count * 2  # å…è®¸æ›´å¤šå¹¶å‘
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ç›´æ¥æäº¤æ‰€æœ‰æ‰¹æ¬¡ï¼Œè®©çº¿ç¨‹æ± ç®¡ç†è°ƒåº¦
            futures = {}
            for batch_idx, batch_info in enumerate(all_batches):
                future = executor.submit(process_batch_on_gpu, (batch_idx, batch_info))
                futures[future] = batch_idx
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(futures):
                batch_idx, frames = future.result()
                batch_results[batch_idx] = frames
                completed += 1
                if completed % 10 == 0 or completed == total_batches:
                    print(f"è¿›åº¦: {completed}/{total_batches} æ‰¹æ¬¡å®Œæˆ")
        
        # å¤„ç†å®Œæ‰€æœ‰æ‰¹æ¬¡åï¼Œåªæ¸…ç†ä¸€æ¬¡å†…å­˜
        if total_batches > 20:  # åªæœ‰åœ¨æ‰¹æ¬¡å¾ˆå¤šæ—¶æ‰æ¸…ç†
            for device in self.devices:
                with torch.cuda.device(device):
                    torch.cuda.empty_cache()
        
        # æŒ‰é¡ºåºåˆå¹¶ç»“æœ
        for i in range(total_batches):
            if i in batch_results:
                res_frame_list.extend(batch_results[i])
        
        return res_frame_list
    
    def ultra_fast_compose_frames(self, res_frame_list, cache_data):
        """æé€Ÿå¹¶è¡Œå›¾åƒåˆæˆ - 32çº¿ç¨‹"""
        coord_list_cycle = cache_data['coord_list_cycle']
        frame_list_cycle = cache_data['frame_list_cycle']
        mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
        mask_list_cycle = cache_data['mask_list_cycle']
        
        print(f"ğŸ¨ å¼€å§‹32çº¿ç¨‹å¹¶è¡Œåˆæˆ {len(res_frame_list)} å¸§...")
        
        def compose_single_frame(frame_info):
            i, res_frame = frame_info
            try:
                bbox = coord_list_cycle[i % len(coord_list_cycle)]
                ori_frame = copy.deepcopy(frame_list_cycle[i % len(frame_list_cycle)])
                
                x1, y1, x2, y2 = bbox
                res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                
                # ä½¿ç”¨ä¼˜åŒ–çš„blending
                mask_coords = mask_coords_list_cycle[i % len(mask_coords_list_cycle)]
                mask = mask_list_cycle[i % len(mask_list_cycle)]
                
                combine_frame = get_image_blending(
                    image=ori_frame,
                    face=res_frame, 
                    face_box=[x1, y1, x2, y2],
                    mask_array=mask,
                    crop_box=mask_coords
                )
                
                return i, combine_frame
                
            except Exception as e:
                print(f"åˆæˆç¬¬{i}å¸§å¤±è´¥: {str(e)}")
                return i, None
        
        # 32çº¿ç¨‹å¹¶è¡Œåˆæˆ
        composed_frames = {}
        with ThreadPoolExecutor(max_workers=32) as executor:
            frame_futures = {
                executor.submit(compose_single_frame, (i, frame)): i 
                for i, frame in enumerate(res_frame_list)
            }
            
            for future in as_completed(frame_futures):
                frame_idx, composed_frame = future.result()
                if composed_frame is not None:
                    composed_frames[frame_idx] = composed_frame
        
        # æŒ‰é¡ºåºæ’åˆ—
        video_frames = []
        for i in range(len(res_frame_list)):
            if i in composed_frames:
                video_frames.append(composed_frames[i])
        
        print(f"å¹¶è¡Œåˆæˆå®Œæˆ: {len(video_frames)} å¸§")
        return video_frames
    
    def extract_audio_features_ultra_fast(self, audio_path, fps):
        """æé€ŸéŸ³é¢‘ç‰¹å¾æå–"""
        try:
            # æ£€æŸ¥AudioProcessoræ˜¯å¦å¯ç”¨
            if self.shared_audio_processor is None:
                raise ValueError("AudioProcessoræœªåˆå§‹åŒ–")
                
            whisper_input_features, librosa_length = self.shared_audio_processor.get_audio_feature(audio_path)
            
            # ç¡®ä¿Whisperä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
            # Whisperæ¨¡å‹å§‹ç»ˆä½¿ç”¨float32ï¼Œä¸æ”¯æŒhalf precision
            whisper_dtype = torch.float32
            
            # å¦‚æœè¾“å…¥ç‰¹å¾åœ¨GPUä¸Šä¸”æ˜¯halfç±»å‹ï¼Œè½¬æ¢ä¸ºfloat32
            if isinstance(whisper_input_features, torch.Tensor):
                if whisper_input_features.dtype == torch.float16:
                    whisper_input_features = whisper_input_features.float()
            
            whisper_chunks = self.shared_audio_processor.get_whisper_chunk(
                whisper_input_features, 
                self.devices[0],  # Whisperåœ¨GPU0
                whisper_dtype,  # ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
                self.shared_whisper, 
                librosa_length,
                fps=fps,
                audio_padding_length_left=2,
                audio_padding_length_right=2,
            )
            return whisper_chunks
        except Exception as e:
            print(f"éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {str(e)}")
            return None
    
    def load_template_cache_optimized(self, cache_dir, template_id):
        """ä¼˜åŒ–çš„æ¨¡æ¿ç¼“å­˜åŠ è½½"""
        try:
            cache_file = os.path.join(cache_dir, f"{template_id}_preprocessed.pkl")
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            return cache_data
            
        except Exception as e:
            print(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def generate_video_ultra_fast(self, video_frames, audio_path, output_path, fps):
        """æé€Ÿè§†é¢‘ç”Ÿæˆ"""
        try:
            # ç›´æ¥å†…å­˜ç”Ÿæˆï¼Œæ— ä¸´æ—¶æ–‡ä»¶
            print(f"ç›´æ¥ç”Ÿæˆè§†é¢‘: {len(video_frames)} å¸§")
            
            # ç”Ÿæˆæ— éŸ³é¢‘è§†é¢‘
            temp_video = output_path.replace('.mp4', '_temp.mp4')
            imageio.mimwrite(
                temp_video, video_frames, 'FFMPEG', 
                fps=fps, codec='libx264', pixelformat='yuv420p',
                output_params=['-preset', 'ultrafast', '-crf', '23']
            )
            
            # å¹¶è¡ŒéŸ³é¢‘åˆæˆ
            try:
                from moviepy.editor import VideoFileClip, AudioFileClip
                video_clip = VideoFileClip(temp_video)
                audio_clip = AudioFileClip(audio_path)
                
                final_clip = video_clip.set_audio(audio_clip)
                final_clip.write_videofile(
                    output_path, 
                    codec='libx264', 
                    audio_codec='aac', 
                    fps=fps,
                    preset='ultrafast',
                    verbose=False, 
                    logger=None
                )
                
                final_clip.close()
                audio_clip.close()
                video_clip.close()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_video)
                
            except Exception as e:
                print(f"éŸ³é¢‘åˆæˆå¤±è´¥ï¼Œä½¿ç”¨æ— éŸ³é¢‘ç‰ˆæœ¬: {str(e)}")
                os.rename(temp_video, output_path)
            
            return True
            
        except Exception as e:
            print(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
            return False

# å…¨å±€æœåŠ¡å®ä¾‹
global_service = UltraFastMuseTalkService()

def start_ultra_fast_service(port=28888):
    """å¯åŠ¨æé€ŸæœåŠ¡"""
    print(f"å¯åŠ¨Ultra Fast Service - ç«¯å£: {port}")
    
    # ç¡®ä¿å·¥ä½œç›®å½•å’Œè·¯å¾„æ­£ç¡®
    import os
    from pathlib import Path
    
    # å¦‚æœä¸åœ¨æ­£ç¡®çš„å·¥ä½œç›®å½•ï¼Œåˆ‡æ¢åˆ°MuseTalkç›®å½•
    current_dir = Path.cwd()
    if not (current_dir / "models" / "musetalkV15" / "unet.pth").exists():
        # å°è¯•æ‰¾åˆ°MuseTalkç›®å½•
        script_dir = Path(__file__).parent
        musetalk_dir = script_dir.parent / "MuseTalk"
        if musetalk_dir.exists():
            os.chdir(musetalk_dir)
            print(f"å·¥ä½œç›®å½•åˆ‡æ¢åˆ°: {musetalk_dir}")
        else:
            print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°MuseTalkæ¨¡å‹ç›®å½•")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("å¼€å§‹åˆå§‹åŒ–Ultra Fastæ¨¡å‹...")
    try:
        if not global_service.initialize_models_ultra_fast():
            print("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ - è¿”å›False")
            return
        print("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"æ¨¡å‹åˆå§‹åŒ–å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # æ€§èƒ½ç›‘æ§å·²ç¦ç”¨
    
    # å¯åŠ¨IPCæœåŠ¡å™¨
    try:
        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_socket.bind(('0.0.0.0', port))
        server_socket.listen(5)
        
        # éªŒè¯ç›‘å¬çŠ¶æ€
        sock_name = server_socket.getsockname()
        print(f"âœ… SocketæˆåŠŸç»‘å®šåˆ°: {sock_name}")
        print(f"Ultra Fast Service å°±ç»ª - ç›‘å¬ç«¯å£: {port}")
        print("æ¯«ç§’çº§å“åº”æ¨¡å¼å·²å¯ç”¨")
        
        while True:
            try:
                client_socket, addr = server_socket.accept()
                print(f"ğŸ”— å®¢æˆ·ç«¯è¿æ¥: {addr}")
                
                # å¤„ç†è¯·æ±‚
                thread = threading.Thread(
                    target=handle_client_ultra_fast, 
                    args=(client_socket,)
                )
                thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
                thread.start()
                print(f"å¯åŠ¨å¤„ç†çº¿ç¨‹: {thread.name}")
                
            except Exception as e:
                print(f"è¿æ¥å¤„ç†å¤±è´¥: {str(e)}")
                
    except Exception as e:
        print(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")

def handle_client_ultra_fast(client_socket):
    """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚ - æé€Ÿç‰ˆæœ¬"""
    try:
        while True:  # ä¸»å¾ªç¯å¤„ç†å¤šä¸ªè¯·æ±‚
            # æ¥æ”¶è¯·æ±‚ - ä½¿ç”¨æ¢è¡Œç¬¦åè®®ï¼ˆä¸C#ç«¯åŒ¹é…ï¼‰
            buffer = b''
            while True:
                chunk = client_socket.recv(1)
                if not chunk:
                    print("å®¢æˆ·ç«¯å…³é—­è¿æ¥")
                    return  # é€€å‡ºå‡½æ•°
                buffer += chunk
                if chunk == b'\n':
                    break
            
            if not buffer:
                break
                
            data = buffer.decode('utf-8').strip()
            if not data:
                print("æ”¶åˆ°ç©ºæ•°æ®ï¼Œè·³è¿‡")
                continue
                
            print(f"æ”¶åˆ°æ•°æ®: {repr(data[:200])}")
            
            try:
                request = json.loads(data)
                command = request.get('command', '')
                
                # å¤„ç†ä¸åŒçš„å‘½ä»¤
                if command == 'preprocess':
                    # å¤„ç†é¢„å¤„ç†è¯·æ±‚ - å…¼å®¹ä¸¤ç§å­—æ®µå
                    template_id = request.get('templateId') or request.get('template_id')
                    template_image_path = request.get('templateImagePath') or request.get('template_image_path')
                    bbox_shift = request.get('bboxShift', 0) or request.get('bbox_shift', 0)
                    
                    print(f"å¤„ç†é¢„å¤„ç†è¯·æ±‚: template_id={template_id}, image_path={template_image_path}")
                    
                    # é¢„å¤„ç†åŠŸèƒ½å·²ç§»é™¤ï¼Œç›´æ¥è¿”å›æˆåŠŸ
                    print(f"é¢„å¤„ç†è¯·æ±‚å·²æ¥æ”¶: {template_id}")
                    response = {
                        'success': True,
                        'templateId': template_id,
                        'message': 'Preprocessing skipped (handler removed)',
                        'processTime': 0.1
                    }
                    
                    # å‘é€å“åº”ï¼ˆæ¢è¡Œç¬¦ç»“å°¾ï¼‰
                    response_json = json.dumps(response) + '\n'
                    client_socket.send(response_json.encode('utf-8'))
                    print(f"âœ… å‘é€é¢„å¤„ç†å“åº”: {template_id}")
                    
                elif command == 'ping':
                    response = {'success': True, 'message': 'pong'}
                    client_socket.send((json.dumps(response) + '\n').encode('utf-8'))
                    print("âœ… å‘é€pongå“åº”")
                    
                elif command == 'inference' or 'template_id' in request:
                    # æ¨ç†è¯·æ±‚
                    print(f"ğŸ“¨ æé€Ÿæ¨ç†è¯·æ±‚: {request.get('template_id')}")
                    
                    # è°ƒè¯•ï¼šæ‰“å°æ¥æ”¶åˆ°çš„batch_size
                    received_batch_size = request.get('batch_size', 6)
                    print(f"ğŸ“Š æ¥æ”¶åˆ°çš„batch_size: {received_batch_size}")
                    
                    # æé€Ÿæ¨ç†
                    start_time = time.time()
                    success = global_service.ultra_fast_inference_parallel(
                        template_id=request['template_id'],
                        audio_path=request['audio_path'],
                        output_path=request['output_path'],
                        cache_dir=request['cache_dir'],
                        batch_size=received_batch_size,
                        fps=request.get('fps', 25)
                    )
                    
                    process_time = time.time() - start_time
                    print(f"æé€Ÿæ¨ç†å®Œæˆ: {process_time:.3f}s, ç»“æœ: {success}")
                    
                    # å‘é€å“åº”ï¼ˆæ¢è¡Œç¬¦ç»“å°¾ï¼‰
                    response = {'Success': success, 'OutputPath': request['output_path'] if success else None}
                    client_socket.send((json.dumps(response) + '\n').encode('utf-8'))
                    
                else:
                    print(f"æœªçŸ¥å‘½ä»¤: {command}")
                    response = {'success': False, 'message': f'Unknown command: {command}'}
                    client_socket.send((json.dumps(response) + '\n').encode('utf-8'))
                    
            except json.JSONDecodeError as e:
                print(f"JSONè§£æé”™è¯¯: {e}, æ•°æ®: {repr(data[:200])}")
                error_response = {'success': False, 'message': f'JSON parse error: {str(e)}'}
                client_socket.send((json.dumps(error_response) + '\n').encode('utf-8'))
            except Exception as e:
                print(f"å¤„ç†è¯·æ±‚å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                error_response = {'success': False, 'message': str(e)}
                client_socket.send((json.dumps(error_response) + '\n').encode('utf-8'))
        
    except Exception as e:
        print(f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        client_socket.close()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, default=28888, help='æœåŠ¡ç«¯å£')
    args = parser.parse_args()
    
    start_ultra_fast_service(args.port)
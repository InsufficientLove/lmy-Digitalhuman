#!/usr/bin/env python3
"""
Enhanced MuseTalk Preprocessing System
åŸºäºMuseTalkå®˜æ–¹å®æ—¶æ¨ç†æœºåˆ¶çš„ä¼˜åŒ–é¢„å¤„ç†ç³»ç»Ÿ

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é¢éƒ¨ç‰¹å¾é¢„æå–ï¼ˆVAEç¼–ç ã€åæ ‡ã€æ©ç ç­‰ï¼‰
2. æŒä¹…åŒ–ç¼“å­˜æœºåˆ¶
3. çœŸæ­£çš„å®æ—¶æ¨ç†æ”¯æŒ

ä½œè€…: Claude Sonnet
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import cv2
import torch
import numpy as np
import pickle
import json
import glob
from tqdm import tqdm
import time
from pathlib import Path

def convert_numpy_types(obj):
    """è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œè§£å†³JSONåºåˆ—åŒ–é—®é¢˜"""
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

# åŠ¨æ€æ·»åŠ MuseTalkè·¯å¾„åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
musetalk_dir = os.path.join(os.path.dirname(current_dir), "MuseTalk")

if os.path.exists(musetalk_dir) and musetalk_dir not in sys.path:
    sys.path.insert(0, musetalk_dir)
    print(f"Added MuseTalk path: {musetalk_dir}")

# MuseTalkç»„ä»¶å¯¼å…¥
try:
    from musetalk.utils.face_parsing import FaceParsing
    from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
    from musetalk.utils.blending import get_image_prepare_material, get_image_blending
    from musetalk.utils.utils import load_all_model
    print("MuseTalk modules imported successfully")
except ImportError as e:
    print(f"MuseTalk import failed: {e}")
    print(f"Python path: {sys.path}")
    print(f"MuseTalk dir: {musetalk_dir}")
    print(f"MuseTalk dir exists: {os.path.exists(musetalk_dir)}")
    if os.path.exists(musetalk_dir):
        print(f"MuseTalk dir contents: {os.listdir(musetalk_dir)}")
    raise


class EnhancedMuseTalkPreprocessor:
    """
    å¢å¼ºçš„MuseTalké¢„å¤„ç†å™¨
    
    å®ç°çœŸæ­£çš„é¢éƒ¨ç‰¹å¾é¢„æå–ï¼ŒåŒ…æ‹¬ï¼š
    - VAEæ½œåœ¨ç¼–ç é¢„è®¡ç®—
    - é¢éƒ¨åæ ‡å’Œæ©ç é¢„æå–  
    - æŒä¹…åŒ–ç¼“å­˜æœºåˆ¶
    - å®æ—¶æ¨ç†ä¼˜åŒ–
    """
    
    def __init__(self, 
                 model_config_path="../MuseTalk/models/musetalk/musetalk.json",
                 model_weights_path="../MuseTalk/models/musetalk/pytorch_model.bin",
                 vae_type="sd-vae",
                 device="cuda:0",
                 cache_dir="./template_cache"):
        """
        åˆå§‹åŒ–å¢å¼ºé¢„å¤„ç†å™¨
        
        Args:
            model_config_path: UNetæ¨¡å‹é…ç½®è·¯å¾„
            model_weights_path: UNetæ¨¡å‹æƒé‡è·¯å¾„  
            vae_type: VAEç±»å‹
            device: è®¡ç®—è®¾å¤‡
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.device = torch.device(device)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Initializing MuseTalk preprocessor...")
        print(f"Device: {self.device}")
        print(f"Cache dir: {self.cache_dir}")
        
        # åŠ è½½æ¨¡å‹ç»„ä»¶
        self._load_models(model_weights_path, vae_type, model_config_path)
        
        # åˆå§‹åŒ–é¢éƒ¨è§£æå™¨
        self.face_parser = FaceParsing()
        
        print(f"âœ… é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_models(self, model_weights_path, vae_type, model_config_path):
        """åŠ è½½æ¨¡å‹ç»„ä»¶"""
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹ç»„ä»¶...")
        
        # åŠ è½½VAE, UNet, PEç­‰ç»„ä»¶
        self.vae, self.unet, self.pe = load_all_model(
            unet_model_path=model_weights_path,
            vae_type=vae_type,
            unet_config=model_config_path,
            device=self.device
        )
        
        print(f"âœ… æ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
    
    def preprocess_template(self, template_id, template_image_path, bbox_shift=0, parsing_mode='cpu', force_refresh=False):
        """
        é¢„å¤„ç†æ¨¡æ¿å›¾ç‰‡ï¼Œæå–é¢éƒ¨ç‰¹å¾å¹¶ç¼“å­˜
        
        Args:
            template_id: æ¨¡æ¿ID
            template_image_path: æ¨¡æ¿å›¾ç‰‡è·¯å¾„
            bbox_shift: è¾¹ç•Œæ¡†åç§»
            parsing_mode: è§£ææ¨¡å¼ ('cpu' æˆ– 'gpu')
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            dict: é¢„å¤„ç†ç»“æœä¿¡æ¯
        """
        cache_path = self.cache_dir / f"{template_id}_preprocessed.pkl"
        metadata_path = self.cache_dir / f"{template_id}_metadata.json"
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and cache_path.exists() and metadata_path.exists():
            print(f"ğŸ“¦ å‘ç°ç¼“å­˜: {template_id}")
            try:
                # éªŒè¯JSONæ–‡ä»¶å®Œæ•´æ€§
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if not content:
                        print(f"âš ï¸ å…ƒæ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œé‡æ–°é¢„å¤„ç†: {template_id}")
                        metadata_path.unlink()  # åˆ é™¤ç©ºæ–‡ä»¶
                        if cache_path.exists():
                            cache_path.unlink()  # åˆ é™¤å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
                    else:
                        # å°è¯•è§£æJSON
                        f.seek(0)
                        metadata = json.load(f)
                        
                        # éªŒè¯ç¼“å­˜å®Œæ•´æ€§
                        if self._validate_cache(cache_path, metadata):
                            print(f"âœ… ç¼“å­˜æœ‰æ•ˆï¼Œè·³è¿‡é¢„å¤„ç†: {template_id}")
                            return metadata
                        else:
                            print(f"âš ï¸ ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°é¢„å¤„ç†: {template_id}")
            except (json.JSONDecodeError, UnicodeDecodeError, FileNotFoundError) as e:
                print(f"âš ï¸ å…ƒæ•°æ®æ–‡ä»¶æŸåï¼Œé‡æ–°é¢„å¤„ç†: {template_id} - {str(e)}")
                # æ¸…ç†æŸåçš„æ–‡ä»¶
                if metadata_path.exists():
                    metadata_path.unlink()
                if cache_path.exists():
                    cache_path.unlink()
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç¼“å­˜æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œé‡æ–°é¢„å¤„ç†: {template_id} - {str(e)}")
                # æ¸…ç†å¯èƒ½æŸåçš„æ–‡ä»¶
                if metadata_path.exists():
                    metadata_path.unlink()
                if cache_path.exists():
                    cache_path.unlink()
        
        print(f"ğŸ¯ å¼€å§‹é¢„å¤„ç†æ¨¡æ¿: {template_id}")
        start_time = time.time()
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½å¹¶é¢„å¤„ç†å›¾ç‰‡
        print(f"ğŸ“¸ åŠ è½½æ¨¡æ¿å›¾ç‰‡: {template_image_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(template_image_path):
            raise ValueError(f"æ¨¡æ¿å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {template_image_path}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(template_image_path)
        print(f"ğŸ“Š å›¾ç‰‡æ–‡ä»¶å¤§å°: {file_size} bytes")
        
        if file_size == 0:
            raise ValueError(f"æ¨¡æ¿å›¾ç‰‡æ–‡ä»¶ä¸ºç©º: {template_image_path}")
        
        img_np = cv2.imread(template_image_path)
        if img_np is None:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾ç‰‡: {template_image_path}")
        
        print(f"âœ… å›¾ç‰‡åŠ è½½æˆåŠŸï¼ŒåŸå§‹å°ºå¯¸: {img_np.shape}")
        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)
        print(f"âœ… å›¾ç‰‡æ ¼å¼è½¬æ¢å®Œæˆ: BGR -> RGB")
        
        # é¢éƒ¨æ£€æµ‹å’Œå…³é”®ç‚¹æå–
        print("ğŸ” æ£€æµ‹é¢éƒ¨ç‰¹å¾...")
        bbox, landmarks = self._detect_face(img_np)
        
        if bbox is None:
            raise ValueError(f"æœªæ£€æµ‹åˆ°é¢éƒ¨: {template_image_path}")
        
        # åº”ç”¨è¾¹ç•Œæ¡†åç§»
        if bbox_shift != 0:
            bbox = self._apply_bbox_shift(bbox, bbox_shift, img_np.shape)
        
        # è£å‰ªé¢éƒ¨åŒºåŸŸ
        face_img = self._crop_face(img_np, bbox)
        
        # ç”Ÿæˆå¾ªç¯å¸§åˆ—è¡¨
        print("ğŸ¬ ç”Ÿæˆå¾ªç¯å¸§...")
        frame_list_cycle = self._generate_frame_cycle(face_img)
        
        # æå–åæ ‡ä¿¡æ¯
        print("ğŸ“ æå–åæ ‡ä¿¡æ¯...")
        coord_list_cycle = self._extract_coordinates(frame_list_cycle, parsing_mode)
        
        # VAEç¼–ç 
        print("ğŸ”§ VAEç¼–ç ...")
        input_latent_list_cycle = self._encode_frames(frame_list_cycle)
        input_latent = input_latent_list_cycle[0]  # ä½¿ç”¨ç¬¬ä¸€å¸§ä½œä¸ºå‚è€ƒ
        
        # ç”Ÿæˆæ©ç 
        print("ğŸ­ ç”Ÿæˆæ©ç ...")
        mask_list_cycle = self._generate_masks(coord_list_cycle)
        mask_coords_list_cycle = self._extract_mask_coordinates(mask_list_cycle)
        
        # å‡†å¤‡ç¼“å­˜æ•°æ®
        preprocessed_data = {
            'frame_list_cycle': frame_list_cycle,
            'coord_list_cycle': coord_list_cycle,
            'input_latent_list_cycle': input_latent_list_cycle,
            'mask_list_cycle': mask_list_cycle,
            'mask_coords_list_cycle': mask_coords_list_cycle,
            'bbox': bbox,
            'landmarks': landmarks
        }
        
        # ä¿å­˜é¢„å¤„ç†æ•°æ®
        print(f"ğŸ’¾ ä¿å­˜é¢„å¤„ç†ç¼“å­˜: {cache_path}")
        with open(cache_path, 'wb') as f:
            pickle.dump(preprocessed_data, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            'template_id': template_id,
            'template_image_path': template_image_path,
            'bbox_shift': bbox_shift,
            'parsing_mode': parsing_mode,
            'bbox': bbox,
            'processing_time': time.time() - start_time,
            'processed_at': time.time(),
            'cache_path': str(cache_path),
            'frame_count': len(frame_list_cycle),
            'input_latent_shape': list(input_latent.shape),
            'version': '1.0'
        }
        
        # è½¬æ¢numpyç±»å‹ä¸ºJSONå¯åºåˆ—åŒ–ç±»å‹
        metadata_serializable = convert_numpy_types(metadata)
        
        # å®‰å…¨ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
        temp_metadata_path = metadata_path.with_suffix('.tmp')
        try:
            with open(temp_metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_serializable, f, indent=2, ensure_ascii=False)
            
            # éªŒè¯å†™å…¥çš„JSONæ–‡ä»¶
            with open(temp_metadata_path, 'r', encoding='utf-8') as f:
                json.load(f)  # éªŒè¯å¯ä»¥æ­£ç¡®è§£æ
            
            # åŸå­æ€§ç§»åŠ¨æ–‡ä»¶
            temp_metadata_path.replace(metadata_path)
            print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
            if temp_metadata_path.exists():
                temp_metadata_path.unlink()
            raise
        
        processing_time = time.time() - start_time
        print(f"âœ… æ¨¡æ¿é¢„å¤„ç†å®Œæˆ: {template_id}")
        print(f"â±ï¸ å¤„ç†è€—æ—¶: {processing_time:.2f}ç§’")
        print(f"ğŸ“¦ ç¼“å­˜å¤§å°: {cache_path.stat().st_size / 1024 / 1024:.2f}MB")
        
        return metadata
    
    def _detect_face(self, img_np):
        """æ£€æµ‹é¢éƒ¨å¹¶æå–å…³é”®ç‚¹"""
        try:
            print(f"ğŸ” å¼€å§‹é¢éƒ¨æ£€æµ‹ï¼Œå›¾ç‰‡å°ºå¯¸: {img_np.shape}")
            
            # ä½¿ç”¨å·²ç»å¯¼å…¥çš„MuseTalké¢éƒ¨æ£€æµ‹åŠŸèƒ½
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºæ£€æµ‹
            import tempfile
            import time
            import uuid
            
            # ä½¿ç”¨æ›´å®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶åˆ›å»ºæ–¹å¼
            temp_dir = tempfile.gettempdir()
            temp_filename = f"musetalk_temp_{uuid.uuid4().hex[:8]}.jpg"
            temp_path = os.path.join(temp_dir, temp_filename)
            
            try:
                # ç¡®ä¿å›¾ç‰‡æ ¼å¼æ­£ç¡®
                img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
                success = cv2.imwrite(temp_path, img_bgr)
                if not success:
                    print(f"âŒ å›¾ç‰‡ä¿å­˜å¤±è´¥: {temp_path}")
                    return None, None
                
                print(f"ğŸ“ ä¸´æ—¶å›¾ç‰‡å·²ä¿å­˜: {temp_path}")
                
                # éªŒè¯ä¸´æ—¶å›¾ç‰‡å¯ä»¥è¯»å–
                test_img = cv2.imread(temp_path)
                if test_img is None:
                    print(f"âŒ æ— æ³•è¯»å–ä¸´æ—¶å›¾ç‰‡: {temp_path}")
                    # å°è¯•åˆ é™¤æ–‡ä»¶ï¼Œä½†ä¸å½±å“æµç¨‹
                    self._safe_remove_file(temp_path)
                    return None, None
                
                print(f"âœ… ä¸´æ—¶å›¾ç‰‡éªŒè¯æˆåŠŸï¼Œå°ºå¯¸: {test_img.shape}")
                
                # è°ƒç”¨é¢éƒ¨æ£€æµ‹
                print("ğŸ¯ è°ƒç”¨get_landmark_and_bboxè¿›è¡Œé¢éƒ¨æ£€æµ‹...")
                coord_list, frame_list = get_landmark_and_bbox([temp_path], 0)
                print(f"ğŸ“Š æ£€æµ‹ç»“æœ - coord_listé•¿åº¦: {len(coord_list) if coord_list else 0}, frame_listé•¿åº¦: {len(frame_list) if frame_list else 0}")
                
                # åœ¨å¤„ç†æ£€æµ‹ç»“æœä¹‹å‰å…ˆä¿å­˜ç»“æœï¼Œé¿å…æ–‡ä»¶åˆ é™¤å¤±è´¥å½±å“é€»è¾‘
                detection_success = False
                bbox_result = None
                
                # éªŒè¯æ£€æµ‹ç»“æœ
                if coord_list and len(coord_list) > 0:
                    bbox = coord_list[0]
                    x1, y1, x2, y2 = bbox
                    print(f"ğŸ” æ£€æµ‹åˆ°è¾¹ç•Œæ¡†: ({x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f})")
                    
                    # æ£€æŸ¥è¾¹ç•Œæ¡†æ˜¯å¦æœ‰æ•ˆ
                    if x1 < x2 and y1 < y2 and bbox != (0.0, 0.0, 0.0, 0.0):
                        landmarks = None  # æš‚æ—¶ä¸æå–è¯¦ç»†å…³é”®ç‚¹
                        print(f"âœ… é¢éƒ¨æ£€æµ‹æˆåŠŸ: è¾¹ç•Œæ¡† ({x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f})")
                        detection_success = True
                        bbox_result = bbox
                    else:
                        print(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆçš„é¢éƒ¨è¾¹ç•Œæ¡†: {bbox}")
                else:
                    print("âš ï¸ æœªæ£€æµ‹åˆ°é¢éƒ¨åŒºåŸŸ")
                
                # è¿”å›æ£€æµ‹ç»“æœï¼ˆä¸å—æ–‡ä»¶æ¸…ç†å¤±è´¥å½±å“ï¼‰
                return (bbox_result, None) if detection_success else (None, None)
                
            finally:
                # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«æ¸…ç†ï¼ˆåœ¨finallyå—ä¸­ï¼‰
                self._safe_remove_file(temp_path)
                
        except Exception as e:
            print(f"âš ï¸ é¢éƒ¨æ£€æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None
    
    def _safe_remove_file(self, file_path):
        """å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†Windowsæƒé™é—®é¢˜"""
        if not os.path.exists(file_path):
            return
            
        import time
        cleanup_attempts = 0
        max_attempts = 3
        
        while cleanup_attempts < max_attempts:
            try:
                os.unlink(file_path)
                print(f"ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {file_path}")
                return
            except PermissionError as e:
                cleanup_attempts += 1
                print(f"âš ï¸ æ–‡ä»¶æ¸…ç†å°è¯• {cleanup_attempts}/{max_attempts} å¤±è´¥: {e}")
                if cleanup_attempts < max_attempts:
                    time.sleep(0.1)  # ç­‰å¾…100msåé‡è¯•
                else:
                    print(f"âš ï¸ æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ¸…ç†: {file_path}")
                    # åœ¨Windowsä¸Šï¼Œä¸´æ—¶æ–‡ä»¶é€šå¸¸ä¼šè¢«ç³»ç»Ÿè‡ªåŠ¨æ¸…ç†
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}")
                break
    
    def _apply_bbox_shift(self, bbox, shift, img_shape):
        """åº”ç”¨è¾¹ç•Œæ¡†åç§»"""
        x1, y1, x2, y2 = bbox
        h, w = img_shape[:2]
        
        # åº”ç”¨åç§»
        x1 = max(0, x1 - shift)
        y1 = max(0, y1 - shift)
        x2 = min(w, x2 + shift)
        y2 = min(h, y2 + shift)
        
        return (x1, y1, x2, y2)
    
    def _crop_face(self, img_np, bbox):
        """è£å‰ªé¢éƒ¨åŒºåŸŸ"""
        x1, y1, x2, y2 = [int(coord) for coord in bbox]
        face_img = img_np[y1:y2, x1:x2]
        
        # è°ƒæ•´åˆ°æ ‡å‡†å°ºå¯¸
        face_img = cv2.resize(face_img, (256, 256), interpolation=cv2.INTER_LANCZOS4)
        return face_img
    
    def _generate_frame_cycle(self, face_img):
        """ç”Ÿæˆå¾ªç¯å¸§åˆ—è¡¨"""
        # ç®€å•å®ç°ï¼šä½¿ç”¨å•å¸§åˆ›å»ºå¾ªç¯
        frame_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)
        return [frame_bgr, frame_bgr]  # åˆ›å»ºç®€å•çš„2å¸§å¾ªç¯
    
    def _extract_coordinates(self, frame_list, parsing_mode):
        """æå–åæ ‡ä¿¡æ¯"""
        # ç®€åŒ–å®ç°ï¼šä¸ºæ¯å¸§è¿”å›ç›¸åŒçš„åæ ‡
        coord = (0, 0, 256, 256)  # æ ‡å‡†åŒ–åçš„åæ ‡
        return [coord] * len(frame_list)
    
    def _encode_frames(self, frame_list):
        """VAEç¼–ç å¸§åˆ—è¡¨"""
        encoded_list = []
        for frame in frame_list:
            with torch.no_grad():
                # è½¬æ¢ä¸ºRGBå¹¶è°ƒæ•´å°ºå¯¸
                if frame.shape[2] == 3:  # BGR to RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                else:
                    frame_rgb = frame
                
                # VAEç¼–ç 
                input_latent = self.vae.get_latents_for_unet(frame_rgb)
                encoded_list.append(input_latent)
        
        return encoded_list
    
    def _generate_masks(self, coord_list):
        """ç”Ÿæˆæ©ç åˆ—è¡¨"""
        # ä½¿ç”¨å·²ç»å¯¼å…¥çš„MuseTalkæ©ç ç”ŸæˆåŠŸèƒ½
        
        mask_list = []
        for coord in coord_list:
            # åˆ›å»ºç®€å•çš„é¢éƒ¨æ©ç 
            mask = np.ones((256, 256), dtype=np.uint8) * 255
            mask_list.append(mask)
        
        return mask_list
    
    def _extract_mask_coordinates(self, mask_list):
        """æå–æ©ç åæ ‡"""
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ ‡å‡†åæ ‡
        coords = (0, 0, 256, 256)
        return [coords] * len(mask_list)
    
    def _validate_cache(self, cache_path, metadata):
        """éªŒè¯ç¼“å­˜å®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶å­˜åœ¨ä¸”éç©º
            if not cache_path.exists() or cache_path.stat().st_size == 0:
                return False
            
            # å°è¯•åŠ è½½ç¼“å­˜æ•°æ®
            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = [
                'frame_list_cycle', 'coord_list_cycle', 'input_latent_list_cycle',
                'mask_list_cycle', 'mask_coords_list_cycle'
            ]
            
            for field in required_fields:
                if field not in data:
                    return False
                if not data[field]:  # æ£€æŸ¥éç©º
                    return False
            
            return True
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜éªŒè¯å¤±è´¥: {e}")
            return False
    
    def load_preprocessed_template(self, template_id):
        """
        åŠ è½½é¢„å¤„ç†çš„æ¨¡æ¿æ•°æ®
        
        Args:
            template_id: æ¨¡æ¿ID
            
        Returns:
            tuple: (é¢„å¤„ç†æ•°æ®, å…ƒæ•°æ®)
        """
        cache_path = self.cache_dir / f"{template_id}_preprocessed.pkl"
        metadata_path = self.cache_dir / f"{template_id}_metadata.json"
        
        if not cache_path.exists():
            raise FileNotFoundError(f"æ¨¡æ¿ç¼“å­˜ä¸å­˜åœ¨: {template_id}")
        
        if not metadata_path.exists():
            raise FileNotFoundError(f"æ¨¡æ¿å…ƒæ•°æ®ä¸å­˜åœ¨: {template_id}")
        
        # å®‰å…¨åŠ è½½å…ƒæ•°æ®
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if not content:
                    raise ValueError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸ºç©º: {template_id}")
                
                # é‡æ–°å®šä½åˆ°æ–‡ä»¶å¼€å¤´å¹¶è§£æJSON
                f.seek(0)
                metadata = json.load(f)
                
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            raise ValueError(f"å…ƒæ•°æ®æ–‡ä»¶æŸå: {template_id} - {str(e)}")
        except Exception as e:
            raise RuntimeError(f"åŠ è½½å…ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {template_id} - {str(e)}")
        
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        try:
            with open(cache_path, 'rb') as f:
                preprocessed_data = pickle.load(f)
        except Exception as e:
            raise RuntimeError(f"åŠ è½½é¢„å¤„ç†æ•°æ®å¤±è´¥: {template_id} - {str(e)}")
        
        print(f"ğŸ“¦ åŠ è½½é¢„å¤„ç†æ¨¡æ¿: {template_id}")
        print(f"ğŸ“Š å¸§æ•°: {len(preprocessed_data['frame_list_cycle'])}")
        print(f"â±ï¸ é¢„å¤„ç†æ—¶é—´: {metadata.get('processed_at', 'unknown')}")
        
        return preprocessed_data, metadata
    
    def list_cached_templates(self):
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„æ¨¡æ¿"""
        templates = []
        corrupted_files = []
        
        for metadata_file in self.cache_dir.glob("*_metadata.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if not content:
                        print(f"âš ï¸ å…ƒæ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œå°†è¢«æ¸…ç†: {metadata_file}")
                        corrupted_files.append(metadata_file)
                        continue
                    
                    # é‡æ–°å®šä½åˆ°æ–‡ä»¶å¼€å¤´å¹¶è§£æJSON
                    f.seek(0)
                    metadata = json.load(f)
                    templates.append(metadata)
                    
            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                print(f"âš ï¸ å…ƒæ•°æ®æ–‡ä»¶æŸåï¼Œå°†è¢«æ¸…ç† {metadata_file}: {e}")
                corrupted_files.append(metadata_file)
            except Exception as e:
                print(f"âš ï¸ è¯»å–å…ƒæ•°æ®å¤±è´¥ {metadata_file}: {e}")
                corrupted_files.append(metadata_file)
        
        # æ¸…ç†æŸåçš„æ–‡ä»¶
        for corrupted_file in corrupted_files:
            try:
                # è·å–æ¨¡æ¿ID
                template_id = corrupted_file.stem.replace('_metadata', '')
                cache_file = self.cache_dir / f"{template_id}_preprocessed.pkl"
                
                # åˆ é™¤æŸåçš„æ–‡ä»¶
                if corrupted_file.exists():
                    corrupted_file.unlink()
                    print(f"ğŸ—‘ï¸ å·²æ¸…ç†æŸåçš„å…ƒæ•°æ®æ–‡ä»¶: {corrupted_file}")
                
                if cache_file.exists():
                    cache_file.unlink()
                    print(f"ğŸ—‘ï¸ å·²æ¸…ç†å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶: {cache_file}")
                    
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†æŸåæ–‡ä»¶å¤±è´¥ {corrupted_file}: {e}")
        
        return templates
    
    def clean_cache(self, template_id=None):
        """æ¸…ç†ç¼“å­˜"""
        if template_id:
            # æ¸…ç†æŒ‡å®šæ¨¡æ¿
            cache_path = self.cache_dir / f"{template_id}_preprocessed.pkl"
            metadata_path = self.cache_dir / f"{template_id}_metadata.json"
            
            for path in [cache_path, metadata_path]:
                if path.exists():
                    path.unlink()
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {path}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            for file_path in self.cache_dir.glob("*"):
                file_path.unlink()
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {file_path}")
    
    def get_cache_info(self):
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        templates = self.list_cached_templates()
        total_size = sum(f.stat().st_size for f in self.cache_dir.glob("*"))
        
        return {
            'cache_dir': str(self.cache_dir),
            'template_count': len(templates),
            'total_size_mb': total_size / 1024 / 1024,
            'templates': templates
        }


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced MuseTalk æ¨¡æ¿é¢„å¤„ç†å·¥å…·")
    parser.add_argument("--template_id", required=True, help="æ¨¡æ¿ID")
    parser.add_argument("--template_image", required=True, help="æ¨¡æ¿å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--output_state", help="è¾“å‡ºçŠ¶æ€æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--cache_dir", default="./template_cache", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--device", default="cuda:0", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--bbox_shift", type=int, default=0, help="è¾¹ç•Œæ¡†åç§»")
    parser.add_argument("--parsing_mode", default="jaw", help="è§£ææ¨¡å¼")
    parser.add_argument("--force_refresh", action="store_true", help="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.template_image):
        print(f"âŒ æ¨¡æ¿å›¾ç‰‡ä¸å­˜åœ¨: {args.template_image}")
        return 1
    
    try:
        print(f"ğŸš€ å¼€å§‹é¢„å¤„ç†æ¨¡æ¿: {args.template_id}")
        print(f"ğŸ“ æ¨¡æ¿å›¾ç‰‡: {args.template_image}")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {args.cache_dir}")
        print(f"ğŸ® è®¾å¤‡: {args.device}")
        
        # åˆå§‹åŒ–é¢„å¤„ç†å™¨
        preprocessor = EnhancedMuseTalkPreprocessor(
            device=args.device,
            cache_dir=args.cache_dir
        )
        
        # é¢„å¤„ç†æ¨¡æ¿
        metadata = preprocessor.preprocess_template(
            template_id=args.template_id,
            template_image_path=args.template_image,
            bbox_shift=args.bbox_shift,
            parsing_mode=args.parsing_mode,
            force_refresh=args.force_refresh
        )
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆ: {metadata}")
        
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºçŠ¶æ€æ–‡ä»¶ï¼Œä¿å­˜åˆ°æŒ‡å®šä½ç½®
        if args.output_state:
            cache_file = os.path.join(args.cache_dir, f"{args.template_id}_preprocessed.pkl")
            if os.path.exists(cache_file):
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(args.output_state), exist_ok=True)
                
                # å¤åˆ¶ç¼“å­˜æ–‡ä»¶åˆ°æŒ‡å®šä½ç½®
                import shutil
                shutil.copy2(cache_file, args.output_state)
                print(f"ğŸ“¦ çŠ¶æ€æ–‡ä»¶å·²ä¿å­˜: {args.output_state}")
            else:
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
        
        # éªŒè¯é¢„å¤„ç†ç»“æœ
        data, meta = preprocessor.load_preprocessed_template(args.template_id)
        print(f"ğŸ¯ éªŒè¯æˆåŠŸï¼Œé¢„å¤„ç†å¸§æ•°: {len(data['frame_list_cycle'])}")
        
        # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
        cache_info = preprocessor.get_cache_info()
        print(f"ğŸ’¾ ç¼“å­˜ç»Ÿè®¡: {cache_info}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ é¢„å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
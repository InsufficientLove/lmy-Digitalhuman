#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•GPUä¼˜åŒ–æœåŠ¡ - ä¸“ä¸ºRTX 4090D 48GBä¼˜åŒ–
ç›®æ ‡ï¼š
1. å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹åˆ°GPU
2. ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°ï¼ˆå……åˆ†åˆ©ç”¨48GBæ˜¾å­˜ï¼‰
3. ä¸ºWebRTCå®æ—¶æ¨ç†åšå‡†å¤‡
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import time
import threading
import queue
import socket
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import copy
import gc
import imageio
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image_blending
from musetalk.utils.audio_processor import AudioProcessor

print("ğŸš€ å•GPUä¼˜åŒ–æœåŠ¡ - RTX 4090D 48GBä¸“ç”¨ç‰ˆ")

class SingleGPUOptimizedService:
    """å•GPUä¼˜åŒ–æœåŠ¡ - å……åˆ†åˆ©ç”¨48GBæ˜¾å­˜"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        # å•GPUé…ç½®
        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.weight_dtype = torch.float16  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
        
        # æ¨¡å‹ç»„ä»¶ï¼ˆé¢„åŠ è½½åˆ°GPUï¼‰
        self.vae = None
        self.unet = None
        self.pe = None
        self.whisper = None
        self.audio_processor = None
        self.fp = None
        self.timesteps = None
        
        # ç¼“å­˜ä¼˜åŒ–
        self.template_cache = {}  # æ¨¡æ¿ç¼“å­˜å¸¸é©»å†…å­˜
        self.audio_cache = {}     # éŸ³é¢‘ç‰¹å¾ç¼“å­˜
        self.max_cache_size = 100  # æœ€å¤§ç¼“å­˜æ•°
        
        # æ‰¹å¤„ç†ä¼˜åŒ– - 48GBæ˜¾å­˜å¯ä»¥å¤„ç†æ›´å¤§æ‰¹æ¬¡
        self.optimal_batch_size = 12  # RTX 4090Då¯ä»¥å¤„ç†æ›´å¤§æ‰¹æ¬¡
        self.max_batch_size = 16      # æœ€å¤§æ‰¹æ¬¡å¤§å°
        
        # çº¿ç¨‹æ± ä¼˜åŒ–
        self.inference_executor = ThreadPoolExecutor(max_workers=2)
        self.compose_executor = ThreadPoolExecutor(max_workers=16)
        
        # WebRTCå®æ—¶æ¨ç†å‡†å¤‡
        self.realtime_queue = queue.Queue(maxsize=10)
        self.is_realtime_mode = False
        
        self.is_initialized = False
        self._initialized = True
        
        print(f"âœ… å•GPUæœåŠ¡åˆå§‹åŒ– - è®¾å¤‡: {self.device}")
    
    def initialize_and_preload(self):
        """å¯åŠ¨æ—¶é¢„åŠ è½½æ‰€æœ‰æ¨¡å‹åˆ°GPU"""
        if self.is_initialized:
            print("æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return True
            
        try:
            print("ğŸ”„ å¼€å§‹é¢„åŠ è½½æ¨¡å‹åˆ°GPU...")
            start_time = time.time()
            
            # 1. åŠ è½½ä¸»æ¨¡å‹åˆ°GPU
            print("åŠ è½½VAE, UNet, PEæ¨¡å‹...")
            self.vae, self.unet, self.pe = load_all_model(vae_type="sd-vae")
            
            # ä¼˜åŒ–ï¼šæ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦å¹¶ç¼–è¯‘
            print("ä¼˜åŒ–æ¨¡å‹ï¼ˆåŠç²¾åº¦+ç¼–è¯‘ï¼‰...")
            if hasattr(self.vae, 'vae'):
                self.vae.vae = self.vae.vae.to(self.device).half().eval()
                # ç¼–è¯‘åŠ é€Ÿï¼ˆLinuxï¼‰
                if hasattr(torch, 'compile') and os.name != 'nt':
                    try:
                        self.vae.vae = torch.compile(self.vae.vae, mode="reduce-overhead")
                    except:
                        pass
            
            if hasattr(self.unet, 'model'):
                self.unet.model = self.unet.model.to(self.device).half().eval()
                if hasattr(torch, 'compile') and os.name != 'nt':
                    try:
                        self.unet.model = torch.compile(self.unet.model, mode="reduce-overhead")
                    except:
                        pass
            
            if hasattr(self.pe, 'to'):
                self.pe = self.pe.to(self.device).half().eval()
                if hasattr(torch, 'compile') and os.name != 'nt':
                    try:
                        self.pe = torch.compile(self.pe, mode="reduce-overhead")
                    except:
                        pass
            
            # 2. åŠ è½½Whisperï¼ˆä¿æŒfloat32ï¼‰
            whisper_dir = "./models/whisper"
            if os.path.exists(whisper_dir):
                print("åŠ è½½Whisperæ¨¡å‹...")
                try:
                    from transformers import WhisperModel
                    self.whisper = WhisperModel.from_pretrained(whisper_dir).eval()
                    self.whisper = self.whisper.to(self.device)  # Whisperéœ€è¦float32
                    print("âœ… WhisperåŠ è½½å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ WhisperåŠ è½½å¤±è´¥: {e}")
                    self.whisper = None
            
            # 3. åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨
            print("åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨...")
            self.audio_processor = AudioProcessor(
                feature_extractor_path=whisper_dir if os.path.exists(whisper_dir) else None
            )
            
            # 4. åˆå§‹åŒ–FaceParsing
            print("åˆå§‹åŒ–FaceParsing...")
            self.fp = FaceParsing()
            
            # 5. è®¾ç½®æ—¶é—´æ­¥
            self.timesteps = torch.tensor([0], device=self.device, dtype=torch.long)
            
            # 6. é¢„çƒ­GPUï¼ˆå¯é€‰ï¼‰
            print("é¢„çƒ­GPU...")
            self._warmup_gpu()
            
            load_time = time.time() - start_time
            print(f"âœ… æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼è€—æ—¶: {load_time:.2f}ç§’")
            
            # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                print(f"ğŸ“Š GPUå†…å­˜: å·²åˆ†é… {allocated:.2f}GB / å·²ä¿ç•™ {reserved:.2f}GB / æ€»è®¡ 48GB")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹é¢„åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _warmup_gpu(self):
        """GPUé¢„çƒ­ï¼Œæé«˜é¦–æ¬¡æ¨ç†é€Ÿåº¦"""
        try:
            # åˆ›å»ºå°æ‰¹æ¬¡æµ‹è¯•æ•°æ®
            dummy_whisper = torch.randn(1, 1, 384, device=self.device, dtype=self.weight_dtype)
            dummy_latent = torch.randn(1, 8, 32, 32, device=self.device, dtype=self.weight_dtype)
            
            # æ‰§è¡Œä¸€æ¬¡æ¨ç†é¢„çƒ­
            with torch.no_grad():
                audio_features = self.pe(dummy_whisper)
                pred = self.unet.model(dummy_latent, self.timesteps, encoder_hidden_states=audio_features).sample
                # ä¸éœ€è¦è§£ç ï¼Œåªæ˜¯é¢„çƒ­
                del audio_features, pred
            
            torch.cuda.empty_cache()
            print("âœ… GPUé¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ GPUé¢„çƒ­å¤±è´¥ï¼ˆä¸å½±å“ä½¿ç”¨ï¼‰: {e}")
    
    def get_optimal_batch_size(self, video_length):
        """æ ¹æ®è§†é¢‘é•¿åº¦åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
        # RTX 4090D 48GBçš„ä¼˜åŒ–ç­–ç•¥
        if video_length < 50:
            return 8  # çŸ­è§†é¢‘ç”¨è¾ƒå°æ‰¹æ¬¡ï¼Œå‡å°‘å»¶è¿Ÿ
        elif video_length < 100:
            return 12  # ä¸­ç­‰é•¿åº¦è§†é¢‘
        elif video_length < 200:
            return 14  # è¾ƒé•¿è§†é¢‘
        else:
            return 16  # é•¿è§†é¢‘ç”¨æœ€å¤§æ‰¹æ¬¡ï¼Œæé«˜ååé‡
    
    def inference_optimized(self, template_id, audio_path, output_path, cache_dir, fps=25):
        """ä¼˜åŒ–çš„å•GPUæ¨ç†"""
        if not self.is_initialized:
            print("âš ï¸ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åŠ è½½...")
            if not self.initialize_and_preload():
                return False
        
        try:
            total_start = time.time()
            
            # 1. åŠ è½½æ¨¡æ¿ç¼“å­˜ï¼ˆå¯èƒ½å·²åœ¨å†…å­˜ä¸­ï¼‰
            cache_data = self.load_template_cache_fast(cache_dir, template_id)
            if not cache_data:
                print(f"âŒ æ— æ³•åŠ è½½æ¨¡æ¿ç¼“å­˜: {template_id}")
                return False
            
            # 2. æå–éŸ³é¢‘ç‰¹å¾ï¼ˆæ£€æŸ¥ç¼“å­˜ï¼‰
            audio_cache_key = f"{audio_path}_{fps}"
            if audio_cache_key in self.audio_cache:
                print("âœ… ä½¿ç”¨ç¼“å­˜çš„éŸ³é¢‘ç‰¹å¾")
                whisper_chunks = self.audio_cache[audio_cache_key]
            else:
                print("ğŸµ æå–éŸ³é¢‘ç‰¹å¾...")
                whisper_chunks = self.extract_audio_features(audio_path, fps)
                if whisper_chunks is None:
                    return False
                # ç¼“å­˜éŸ³é¢‘ç‰¹å¾
                if len(self.audio_cache) < self.max_cache_size:
                    self.audio_cache[audio_cache_key] = whisper_chunks
            
            # 3. åŠ¨æ€ç¡®å®šæ‰¹æ¬¡å¤§å°
            video_length = len(whisper_chunks)
            batch_size = self.get_optimal_batch_size(video_length)
            print(f"ğŸ“Š è§†é¢‘é•¿åº¦: {video_length}å¸§, ä½¿ç”¨æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            # 4. å•GPUæ‰¹å¤„ç†æ¨ç†
            inference_start = time.time()
            res_frame_list = self.batch_inference_single_gpu(
                whisper_chunks, cache_data, batch_size
            )
            inference_time = time.time() - inference_start
            print(f"âš¡ æ¨ç†å®Œæˆ: {inference_time:.2f}ç§’ ({len(res_frame_list)}å¸§, {len(res_frame_list)/inference_time:.1f} FPS)")
            
            # 5. å¹¶è¡Œå›¾åƒåˆæˆ
            compose_start = time.time()
            video_frames = self.parallel_compose_frames(res_frame_list, cache_data)
            compose_time = time.time() - compose_start
            print(f"ğŸ¨ åˆæˆå®Œæˆ: {compose_time:.2f}ç§’")
            
            # 6. ç”Ÿæˆè§†é¢‘
            video_start = time.time()
            success = self.generate_video_fast(video_frames, audio_path, output_path, fps)
            video_time = time.time() - video_start
            print(f"ğŸ“¹ è§†é¢‘ç”Ÿæˆ: {video_time:.2f}ç§’")
            
            total_time = time.time() - total_start
            print(f"âœ… æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"   æ¨ç†: {inference_time:.2f}s | åˆæˆ: {compose_time:.2f}s | è§†é¢‘: {video_time:.2f}s")
            
            return success
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def batch_inference_single_gpu(self, whisper_chunks, cache_data, batch_size):
        """å•GPUæ‰¹å¤„ç†æ¨ç† - ä¼˜åŒ–ç‰ˆ"""
        from musetalk.utils.utils import datagen
        
        input_latent_list_cycle = cache_data['input_latent_list_cycle']
        
        # ç”Ÿæˆæ‰¹æ¬¡
        gen = datagen(
            whisper_chunks=whisper_chunks,
            vae_encode_latents=input_latent_list_cycle,
            batch_size=batch_size,
            delay_frame=0,
            device=self.device  # ç›´æ¥åœ¨GPUä¸Šç”Ÿæˆ
        )
        
        res_frame_list = []
        batch_count = 0
        
        # æ‰¹å¤„ç†æ¨ç†
        with torch.no_grad():
            for whisper_batch, latent_batch in gen:
                batch_count += 1
                
                # ç¡®ä¿æ•°æ®åœ¨GPUä¸Š
                whisper_batch = whisper_batch.to(self.device, dtype=self.weight_dtype, non_blocking=True)
                latent_batch = latent_batch.to(self.device, dtype=self.weight_dtype, non_blocking=True)
                
                # æ¨ç†
                audio_features = self.pe(whisper_batch)
                pred_latents = self.unet.model(
                    latent_batch, self.timesteps, 
                    encoder_hidden_states=audio_features
                ).sample
                
                # è§£ç 
                recon_frames = self.vae.decode_latents(pred_latents)
                
                # è½¬æ¢ä¸ºnumpy
                if isinstance(recon_frames, torch.Tensor):
                    recon_frames = recon_frames.cpu().numpy()
                
                # æ·»åŠ åˆ°ç»“æœ
                if isinstance(recon_frames, list):
                    res_frame_list.extend(recon_frames)
                elif isinstance(recon_frames, np.ndarray):
                    for i in range(recon_frames.shape[0]):
                        res_frame_list.append(recon_frames[i])
                
                # å®šæœŸæ¸…ç†æ˜¾å­˜
                if batch_count % 10 == 0:
                    torch.cuda.empty_cache()
                
                print(f"  æ‰¹æ¬¡ {batch_count} å®Œæˆ", end='\r')
        
        print(f"\nâœ… å¤„ç†äº† {batch_count} ä¸ªæ‰¹æ¬¡")
        return res_frame_list
    
    def load_template_cache_fast(self, cache_dir, template_id):
        """å¿«é€ŸåŠ è½½æ¨¡æ¿ç¼“å­˜ï¼ˆå†…å­˜ç¼“å­˜ï¼‰"""
        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if template_id in self.template_cache:
            print(f"âœ… ä½¿ç”¨å†…å­˜ä¸­çš„æ¨¡æ¿ç¼“å­˜: {template_id}")
            return self.template_cache[template_id]
        
        # ä»ç£ç›˜åŠ è½½
        try:
            cache_file = os.path.join(cache_dir, f"{template_id}_preprocessed.pkl")
            if not os.path.exists(cache_file):
                print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
                return None
            
            print(f"ğŸ“‚ ä»ç£ç›˜åŠ è½½æ¨¡æ¿ç¼“å­˜: {template_id}")
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # å­˜å…¥å†…å­˜ç¼“å­˜
            if len(self.template_cache) < 10:  # æœ€å¤šç¼“å­˜10ä¸ªæ¨¡æ¿
                self.template_cache[template_id] = cache_data
            
            return cache_data
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def extract_audio_features(self, audio_path, fps):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            if self.audio_processor is None:
                raise ValueError("éŸ³é¢‘å¤„ç†å™¨æœªåˆå§‹åŒ–")
            
            whisper_input_features, librosa_length = self.audio_processor.get_audio_feature(audio_path)
            
            # Whisperéœ€è¦float32
            whisper_dtype = torch.float32
            if isinstance(whisper_input_features, torch.Tensor):
                if whisper_input_features.dtype == torch.float16:
                    whisper_input_features = whisper_input_features.float()
            
            whisper_chunks = self.audio_processor.get_whisper_chunk(
                whisper_input_features,
                self.device,
                whisper_dtype,
                self.whisper,
                librosa_length,
                fps=fps,
                audio_padding_length_left=2,
                audio_padding_length_right=2,
            )
            
            return whisper_chunks
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def parallel_compose_frames(self, res_frame_list, cache_data):
        """å¹¶è¡Œå›¾åƒåˆæˆ"""
        coord_list_cycle = cache_data['coord_list_cycle']
        frame_list_cycle = cache_data['frame_list_cycle']
        mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
        mask_list_cycle = cache_data['mask_list_cycle']
        
        def compose_single_frame(args):
            i, res_frame = args
            try:
                bbox = coord_list_cycle[i % len(coord_list_cycle)]
                ori_frame = copy.deepcopy(frame_list_cycle[i % len(frame_list_cycle)])
                
                x1, y1, x2, y2 = bbox
                res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                
                mask_coords = mask_coords_list_cycle[i % len(mask_coords_list_cycle)]
                mask = mask_list_cycle[i % len(mask_list_cycle)]
                
                combine_frame = get_image_blending(
                    image=ori_frame,
                    face=res_frame,
                    face_box=[x1, y1, x2, y2],
                    mask_array=mask,
                    crop_box=mask_coords
                )
                
                return i, combine_frame
            except Exception as e:
                print(f"åˆæˆç¬¬{i}å¸§å¤±è´¥: {e}")
                return i, None
        
        # å¹¶è¡Œåˆæˆ
        with ThreadPoolExecutor(max_workers=16) as executor:
            results = list(executor.map(compose_single_frame, enumerate(res_frame_list)))
        
        # æŒ‰é¡ºåºæ’åˆ—
        video_frames = []
        for i, frame in sorted(results, key=lambda x: x[0]):
            if frame is not None:
                video_frames.append(frame)
        
        return video_frames
    
    def generate_video_fast(self, video_frames, audio_path, output_path, fps):
        """å¿«é€Ÿè§†é¢‘ç”Ÿæˆ"""
        try:
            # ç”Ÿæˆæ— éŸ³é¢‘è§†é¢‘
            temp_video = output_path.replace('.mp4', '_temp.mp4')
            imageio.mimwrite(
                temp_video, video_frames, 'FFMPEG',
                fps=fps, codec='libx264', pixelformat='yuv420p',
                output_params=['-preset', 'ultrafast', '-crf', '23']
            )
            
            # åˆå¹¶éŸ³é¢‘
            try:
                from moviepy.editor import VideoFileClip, AudioFileClip
                video_clip = VideoFileClip(temp_video)
                audio_clip = AudioFileClip(audio_path)
                
                final_clip = video_clip.set_audio(audio_clip)
                final_clip.write_videofile(
                    output_path,
                    codec='libx264',
                    audio_codec='aac',
                    fps=fps,
                    preset='ultrafast',
                    verbose=False,
                    logger=None
                )
                
                final_clip.close()
                audio_clip.close()
                video_clip.close()
                
                os.remove(temp_video)
            except Exception as e:
                print(f"éŸ³é¢‘åˆæˆå¤±è´¥: {e}")
                os.rename(temp_video, output_path)
            
            return True
            
        except Exception as e:
            print(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def prepare_for_realtime(self):
        """ä¸ºWebRTCå®æ—¶æ¨ç†åšå‡†å¤‡"""
        print("ğŸ¯ å‡†å¤‡å®æ—¶æ¨ç†æ¨¡å¼...")
        
        # 1. ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not self.is_initialized:
            self.initialize_and_preload()
        
        # 2. è®¾ç½®å®æ—¶æ¨¡å¼å‚æ•°
        self.is_realtime_mode = True
        self.realtime_batch_size = 1  # å®æ—¶æ¨¡å¼ä½¿ç”¨å°æ‰¹æ¬¡
        
        # 3. æ¸…ç†ç¼“å­˜ï¼Œé‡Šæ”¾å†…å­˜
        torch.cuda.empty_cache()
        
        print("âœ… å®æ—¶æ¨ç†æ¨¡å¼å°±ç»ª")
        return True
    
    def realtime_inference_frame(self, audio_chunk, template_cache):
        """å®æ—¶æ¨ç†å•å¸§ï¼ˆWebRTCç”¨ï¼‰"""
        if not self.is_realtime_mode:
            self.prepare_for_realtime()
        
        try:
            with torch.no_grad():
                # å¿«é€Ÿæ¨ç†å•å¸§
                # TODO: å®ç°å®æ—¶æ¨ç†é€»è¾‘
                pass
        except Exception as e:
            print(f"å®æ—¶æ¨ç†å¤±è´¥: {e}")
            return None

# å…¨å±€æœåŠ¡å®ä¾‹
global_service = SingleGPUOptimizedService()

def start_optimized_service(port=28888):
    """å¯åŠ¨ä¼˜åŒ–æœåŠ¡"""
    print(f"ğŸš€ å¯åŠ¨å•GPUä¼˜åŒ–æœåŠ¡ - ç«¯å£: {port}")
    
    # é¢„åŠ è½½æ¨¡å‹
    print("ğŸ“¦ é¢„åŠ è½½æ¨¡å‹åˆ°GPU...")
    if not global_service.initialize_and_preload():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    # å¯åŠ¨SocketæœåŠ¡å™¨
    try:
        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_socket.bind(('0.0.0.0', port))
        server_socket.listen(5)
        
        print(f"âœ… æœåŠ¡å°±ç»ª - ç›‘å¬ç«¯å£: {port}")
        print(f"ğŸ“Š ä¼˜åŒ–é…ç½®: æ‰¹æ¬¡å¤§å° 8-16, 48GBæ˜¾å­˜ä¼˜åŒ–")
        
        while True:
            client_socket, addr = server_socket.accept()
            print(f"ğŸ”— å®¢æˆ·ç«¯è¿æ¥: {addr}")
            
            thread = threading.Thread(
                target=handle_client_request,
                args=(client_socket,)
            )
            thread.daemon = True
            thread.start()
            
    except Exception as e:
        print(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

def handle_client_request(client_socket):
    """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚"""
    try:
        # æ¥æ”¶è¯·æ±‚
        buffer = b''
        while True:
            chunk = client_socket.recv(1)
            if not chunk:
                break
            buffer += chunk
            if chunk == b'\n':
                break
        
        if not buffer:
            return
        
        data = buffer.decode('utf-8').strip()
        request = json.loads(data)
        
        # å¤„ç†æ¨ç†è¯·æ±‚
        if 'template_id' in request:
            print(f"ğŸ“¨ æ¨ç†è¯·æ±‚: {request.get('template_id')}")
            
            start_time = time.time()
            success = global_service.inference_optimized(
                template_id=request['template_id'],
                audio_path=request['audio_path'],
                output_path=request['output_path'],
                cache_dir=request.get('cache_dir', '/opt/musetalk/models/templates'),
                fps=request.get('fps', 25)
            )
            
            process_time = time.time() - start_time
            
            response = {
                'Success': success,
                'OutputPath': request['output_path'] if success else None,
                'ProcessTime': process_time
            }
            
            client_socket.send((json.dumps(response) + '\n').encode('utf-8'))
            print(f"âœ… æ¨ç†å®Œæˆ: {process_time:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        error_response = {'Success': False, 'Error': str(e)}
        client_socket.send((json.dumps(error_response) + '\n').encode('utf-8'))
    finally:
        client_socket.close()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, default=28888, help='æœåŠ¡ç«¯å£')
    args = parser.parse_args()
    
    start_optimized_service(args.port)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•8é€šé“é¢„å¤„ç†
éªŒè¯é¢„å¤„ç†ç”Ÿæˆçš„latentæ˜¯å¦ä¸º8é€šé“ï¼ˆmasked + referenceï¼‰
"""

import os
import sys
import pickle
import numpy as np
import torch
from pathlib import Path

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

def test_preprocessed_data(cache_file):
    """æµ‹è¯•é¢„å¤„ç†ç¼“å­˜æ–‡ä»¶ä¸­çš„æ•°æ®"""
    print(f"\nğŸ“‹ æµ‹è¯•é¢„å¤„ç†æ–‡ä»¶: {cache_file}")
    
    if not os.path.exists(cache_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
        return False
    
    try:
        # åŠ è½½ç¼“å­˜æ•°æ®
        with open(cache_file, 'rb') as f:
            cache_data = pickle.load(f)
        
        print("\nâœ… æˆåŠŸåŠ è½½ç¼“å­˜æ•°æ®")
        
        # æ£€æŸ¥å¿…è¦çš„é”®
        required_keys = ['input_latent_list_cycle', 'coord_list_cycle', 
                        'frame_list_cycle', 'mask_coords_list_cycle', 'mask_list_cycle']
        
        for key in required_keys:
            if key not in cache_data:
                print(f"âŒ ç¼ºå°‘å¿…è¦çš„é”®: {key}")
                return False
        
        # æ£€æŸ¥latentçš„å½¢çŠ¶
        latent_list = cache_data['input_latent_list_cycle']
        print(f"\nğŸ“Š Latentåˆ—è¡¨é•¿åº¦: {len(latent_list)}")
        
        if latent_list:
            # æ£€æŸ¥ç¬¬ä¸€ä¸ªlatent
            first_latent = latent_list[0]
            if isinstance(first_latent, torch.Tensor):
                latent_shape = first_latent.shape
            else:
                latent_shape = first_latent.shape if hasattr(first_latent, 'shape') else None
            
            print(f"ğŸ“ ç¬¬ä¸€ä¸ªLatentå½¢çŠ¶: {latent_shape}")
            
            # éªŒè¯é€šé“æ•°
            if latent_shape and len(latent_shape) >= 2:
                channels = latent_shape[1]
                if channels == 8:
                    print(f"âœ… é€šé“æ•°æ­£ç¡®: {channels} (æœŸæœ›8é€šé“)")
                else:
                    print(f"âŒ é€šé“æ•°é”™è¯¯: {channels} (æœŸæœ›8é€šé“)")
                    return False
            else:
                print("âŒ æ— æ³•è·å–latentå½¢çŠ¶")
                return False
            
            # æ£€æŸ¥æ‰€æœ‰latentçš„ä¸€è‡´æ€§
            all_same_shape = True
            for i, latent in enumerate(latent_list):
                if isinstance(latent, torch.Tensor):
                    shape = latent.shape
                else:
                    shape = latent.shape if hasattr(latent, 'shape') else None
                
                if shape != latent_shape:
                    print(f"âš ï¸ ç¬¬{i}ä¸ªlatentå½¢çŠ¶ä¸ä¸€è‡´: {shape}")
                    all_same_shape = False
            
            if all_same_shape:
                print("âœ… æ‰€æœ‰latentå½¢çŠ¶ä¸€è‡´")
            
        # æ£€æŸ¥å…¶ä»–æ•°æ®
        print(f"\nğŸ“Š å…¶ä»–æ•°æ®ç»Ÿè®¡:")
        print(f"  - coord_listé•¿åº¦: {len(cache_data['coord_list_cycle'])}")
        print(f"  - frame_listé•¿åº¦: {len(cache_data['frame_list_cycle'])}")
        print(f"  - mask_coords_listé•¿åº¦: {len(cache_data['mask_coords_list_cycle'])}")
        print(f"  - mask_listé•¿åº¦: {len(cache_data['mask_list_cycle'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½ç¼“å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•8é€šé“é¢„å¤„ç†')
    parser.add_argument('--cache_dir', type=str, 
                       default='./model_state',
                       help='ç¼“å­˜ç›®å½•è·¯å¾„')
    parser.add_argument('--template_id', type=str,
                       help='æ¨¡æ¿IDï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†æµ‹è¯•æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ï¼‰')
    
    args = parser.parse_args()
    
    print("ğŸ§ª 8é€šé“é¢„å¤„ç†æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    if args.template_id:
        # æµ‹è¯•ç‰¹å®šæ¨¡æ¿
        cache_file = os.path.join(args.cache_dir, f"{args.template_id}_preprocessed.pkl")
        success = test_preprocessed_data(cache_file)
        if success:
            print(f"\nâœ… æ¨¡æ¿ {args.template_id} æµ‹è¯•é€šè¿‡")
        else:
            print(f"\nâŒ æ¨¡æ¿ {args.template_id} æµ‹è¯•å¤±è´¥")
    else:
        # æµ‹è¯•æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
        cache_files = list(Path(args.cache_dir).glob("*_preprocessed.pkl"))
        print(f"\næ‰¾åˆ° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
        
        success_count = 0
        for cache_file in cache_files:
            if test_preprocessed_data(str(cache_file)):
                success_count += 1
            print("\n" + "=" * 50)
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(cache_files)} é€šè¿‡")

if __name__ == "__main__":
    main()
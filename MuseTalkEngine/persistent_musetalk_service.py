#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒä¹…åŒ–MuseTalkæœåŠ¡
åŸºäºå®˜æ–¹MuseTalkæ¶æ„ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼Œå®ç°çœŸæ­£çš„å¿«é€Ÿæ¨ç†
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import argparse
import time
import threading
import queue
import subprocess
import shutil
from pathlib import Path
from tqdm import tqdm
import copy
from transformers import WhisperModel
from moviepy.editor import VideoFileClip, AudioFileClip
import imageio

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image_prepare_material, get_image_blending
from musetalk.utils.audio_processor import AudioProcessor

class PersistentMuseTalkService:
    """æŒä¹…åŒ–MuseTalkæœåŠ¡ - é¿å…é‡å¤åŠ è½½æ¨¡å‹"""
    
    def __init__(self):
        self.device = None
        self.vae = None
        self.unet = None
        self.pe = None
        self.whisper = None
        self.audio_processor = None
        self.fp = None
        self.weight_dtype = None
        self.timesteps = None
        self.is_initialized = False
        self.lock = threading.Lock()
        
        # é…ç½®å‚æ•°
        self.unet_model_path = "./models/musetalk/pytorch_model.bin"
        self.unet_config = "./models/musetalk/musetalk.json"
        self.vae_type = "sd-vae"
        self.whisper_dir = "./models/whisper"
        
        print("ğŸš€ æŒä¹…åŒ–MuseTalkæœåŠ¡å·²åˆ›å»º")
    
    def initialize_models(self, gpu_id=0):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰"""
        if self.is_initialized:
            print("âœ… æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åŠ è½½")
            return True
            
        with self.lock:
            if self.is_initialized:  # åŒé‡æ£€æŸ¥
                return True
                
            try:
                print(f"ğŸ”§ åˆå§‹åŒ–MuseTalkæ¨¡å‹ (GPU:{gpu_id})...")
                start_time = time.time()
                
                # è®¾ç½®è®¾å¤‡
                self.device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")
                print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {self.device}")
                
                # åŠ è½½æ ¸å¿ƒæ¨¡å‹
                print("ğŸ“¦ åŠ è½½VAE, UNet, PEæ¨¡å‹...")
                self.vae, self.unet, self.pe = load_all_model(
                    unet_model_path=self.unet_model_path,
                    vae_type=self.vae_type,
                    unet_config=self.unet_config,
                    device=self.device
                )
                
                # è®¾ç½®æ•°æ®ç±»å‹å’Œè®¾å¤‡
                self.weight_dtype = torch.float16
                self.pe = self.pe.half().to(self.device)
                self.vae.vae = self.vae.vae.half().to(self.device)
                self.unet.model = self.unet.model.half().to(self.device)
                
                self.timesteps = torch.tensor([0], device=self.device)
                
                # åŠ è½½Whisperæ¨¡å‹
                print("ğŸµ åŠ è½½Whisperæ¨¡å‹...")
                self.audio_processor = AudioProcessor(feature_extractor_path=self.whisper_dir)
                self.whisper = WhisperModel.from_pretrained(self.whisper_dir)
                self.whisper = self.whisper.to(device=self.device, dtype=self.weight_dtype).eval()
                self.whisper.requires_grad_(False)
                
                # åˆå§‹åŒ–é¢éƒ¨è§£æå™¨
                print("ğŸ‘¤ åˆå§‹åŒ–é¢éƒ¨è§£æå™¨...")
                self.fp = FaceParsing()
                
                self.is_initialized = True
                init_time = time.time() - start_time
                print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}ç§’")
                return True
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                return False
    
    def load_template_cache(self, cache_dir, template_id):
        """åŠ è½½æ¨¡æ¿ç¼“å­˜"""
        try:
            # åŠ è½½é¢„å¤„ç†ç¼“å­˜
            cache_file = os.path.join(cache_dir, f"{template_id}_preprocessed.pkl")
            metadata_file = os.path.join(cache_dir, f"{template_id}_metadata.json")
            
            if not os.path.exists(cache_file):
                raise FileNotFoundError(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
            if not os.path.exists(metadata_file):
                raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            
            # åŠ è½½ç¼“å­˜æ•°æ®
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # æå–æ•°æ®
            input_latent_list_cycle = cache_data['input_latent_list_cycle']
            coord_list_cycle = cache_data['coord_list_cycle']
            frame_list_cycle = cache_data['frame_list_cycle']
            mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
            mask_list_cycle = cache_data['mask_list_cycle']
            
            print(f"âœ… æ¨¡æ¿ç¼“å­˜åŠ è½½æˆåŠŸ: {template_id}")
            print(f"   - æ½œåœ¨å‘é‡: {len(input_latent_list_cycle)} å¸§")
            print(f"   - é¢éƒ¨åæ ‡: {len(coord_list_cycle)} å¸§")
            print(f"   - æ©ç åæ ‡: {len(mask_coords_list_cycle)} å¸§")
            
            return {
                'input_latent_list_cycle': input_latent_list_cycle,
                'coord_list_cycle': coord_list_cycle,
                'frame_list_cycle': frame_list_cycle,
                'mask_coords_list_cycle': mask_coords_list_cycle,
                'mask_list_cycle': mask_list_cycle,
                'metadata': metadata
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡æ¿ç¼“å­˜å¤±è´¥: {str(e)}")
            return None
    
    def inference_with_cache(self, template_id, audio_path, output_path, cache_dir, batch_size=8, fps=25):
        """ä½¿ç”¨ç¼“å­˜è¿›è¡Œå¿«é€Ÿæ¨ç†"""
        if not self.is_initialized:
            print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
            
        try:
            start_time = time.time()
            print(f"ğŸš€ å¼€å§‹å¿«é€Ÿæ¨ç†: {template_id}")
            
            # 1. åŠ è½½æ¨¡æ¿ç¼“å­˜
            cache_data = self.load_template_cache(cache_dir, template_id)
            if not cache_data:
                return False
            
            input_latent_list_cycle = cache_data['input_latent_list_cycle']
            coord_list_cycle = cache_data['coord_list_cycle']
            frame_list_cycle = cache_data['frame_list_cycle']
            mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
            mask_list_cycle = cache_data['mask_list_cycle']
            
            # 2. éŸ³é¢‘ç‰¹å¾æå–
            print("ğŸµ æå–éŸ³é¢‘ç‰¹å¾...")
            audio_start = time.time()
            whisper_input_features, librosa_length = self.audio_processor.get_audio_feature(audio_path)
            whisper_chunks = self.audio_processor.get_whisper_chunk(
                whisper_input_features, 
                self.device, 
                self.weight_dtype, 
                self.whisper, 
                librosa_length,
                fps=fps,
                audio_padding_length_left=2,
                audio_padding_length_right=2,
            )
            audio_time = time.time() - audio_start
            print(f"âœ… éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {audio_time:.2f}ç§’, éŸ³é¢‘å—æ•°: {len(whisper_chunks)}")
            
            # 3. æ‰¹é‡æ¨ç†
            print("âš¡ å¼€å§‹æ‰¹é‡æ¨ç†...")
            inference_start = time.time()
            video_num = len(whisper_chunks)
            gen = datagen(
                whisper_chunks=whisper_chunks,
                vae_encode_latents=input_latent_list_cycle,
                batch_size=batch_size,
                delay_frame=0,
                device=self.device,
            )
            
            res_frame_list = []
            for i, (whisper_batch, latent_batch) in enumerate(tqdm(gen, total=int(np.ceil(float(video_num)/batch_size)), desc="æ¨ç†è¿›åº¦")):
                audio_feature_batch = self.pe(whisper_batch)
                latent_batch = latent_batch.to(dtype=self.weight_dtype)
                
                pred_latents = self.unet.model(latent_batch, self.timesteps, encoder_hidden_states=audio_feature_batch).sample
                recon = self.vae.decode_latents(pred_latents)
                for res_frame in recon:
                    res_frame_list.append(res_frame)
            
            inference_time = time.time() - inference_start
            print(f"âœ… æ¨ç†å®Œæˆ: {len(res_frame_list)} å¸§, è€—æ—¶: {inference_time:.2f}ç§’")
            
            # 4. åˆæˆå®Œæ•´å›¾åƒ
            print("ğŸ–¼ï¸ åˆæˆå®Œæ•´å›¾åƒ...")
            compose_start = time.time()
            
            # åˆ›å»ºä¸´æ—¶å¸§ç›®å½•
            temp_frames_dir = os.path.join(os.path.dirname(output_path), "temp_frames")
            os.makedirs(temp_frames_dir, exist_ok=True)
            
            for i, res_frame in enumerate(tqdm(res_frame_list, desc="åˆæˆå›¾åƒ")):
                bbox = coord_list_cycle[i % len(coord_list_cycle)]
                ori_frame = copy.deepcopy(frame_list_cycle[i % len(frame_list_cycle)])
                mask_coords = mask_coords_list_cycle[i % len(mask_coords_list_cycle)]
                mask = mask_list_cycle[i % len(mask_list_cycle)]
                
                x1, y1, x2, y2 = bbox
                try:
                    res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                except:
                    continue
                
                # ä½¿ç”¨å®˜æ–¹çš„å›¾åƒåˆæˆæ–¹æ³•
                combine_frame = get_image_blending(
                    image=ori_frame, 
                    face=res_frame, 
                    face_box=bbox, 
                    mask_array=mask, 
                    crop_box=mask_coords
                )
                
                # ä¿å­˜å¸§
                frame_path = os.path.join(temp_frames_dir, f"{i:08d}.png")
                cv2.imwrite(frame_path, combine_frame)
            
            compose_time = time.time() - compose_start
            print(f"âœ… å›¾åƒåˆæˆå®Œæˆ: è€—æ—¶: {compose_time:.2f}ç§’")
            
            # 5. ç”Ÿæˆè§†é¢‘ï¼ˆæ— éŸ³é¢‘ç‰ˆæœ¬ï¼‰
            print("ğŸ¬ ç”Ÿæˆè§†é¢‘...")
            video_start = time.time()
            temp_video = output_path.replace('.mp4', '_temp.mp4')
            
            ffmpeg_cmd = [
                'ffmpeg', '-y', '-v', 'warning',
                '-r', str(fps),
                '-f', 'image2',
                '-i', os.path.join(temp_frames_dir, '%08d.png'),
                '-vcodec', 'libx264',
                '-vf', 'format=yuv420p',
                '-crf', '18',
                temp_video
            ]
            
            subprocess.run(ffmpeg_cmd, check=True)
            video_time = time.time() - video_start
            print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: è€—æ—¶: {video_time:.2f}ç§’")
            
            # 6. åˆæˆéŸ³é¢‘
            print("ğŸ”Š åˆæˆéŸ³é¢‘...")
            audio_merge_start = time.time()
            
            try:
                video_clip = VideoFileClip(temp_video)
                audio_clip = AudioFileClip(audio_path)
                video_clip = video_clip.set_audio(audio_clip)
                video_clip.write_videofile(output_path, codec='libx264', audio_codec='aac', fps=fps, verbose=False, logger=None)
                video_clip.close()
                audio_clip.close()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_video)
                shutil.rmtree(temp_frames_dir)
                
            except Exception as e:
                print(f"âš ï¸ éŸ³é¢‘åˆæˆå¤±è´¥ï¼Œä½¿ç”¨æ— éŸ³é¢‘ç‰ˆæœ¬: {str(e)}")
                shutil.move(temp_video, output_path)
            
            audio_merge_time = time.time() - audio_merge_start
            print(f"âœ… éŸ³é¢‘åˆæˆå®Œæˆ: è€—æ—¶: {audio_merge_time:.2f}ç§’")
            
            total_time = time.time() - start_time
            print(f"ğŸ‰ æ¨ç†å®Œæˆ: {output_path}")
            print(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.2f}ç§’ (éŸ³é¢‘:{audio_time:.1f}s + æ¨ç†:{inference_time:.1f}s + åˆæˆ:{compose_time:.1f}s + è§†é¢‘:{video_time:.1f}s + éŸ³é¢‘:{audio_merge_time:.1f}s)")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

# å…¨å±€æœåŠ¡å®ä¾‹
musetalk_service = PersistentMuseTalkService()

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='æŒä¹…åŒ–MuseTalkæ¨ç†æœåŠ¡')
    parser.add_argument('--template_id', type=str, required=True, help='æ¨¡æ¿ID')
    parser.add_argument('--audio_path', type=str, required=True, help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_path', type=str, required=True, help='è¾“å‡ºè§†é¢‘è·¯å¾„')
    parser.add_argument('--cache_dir', type=str, required=True, help='ç¼“å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda:0', help='GPUè®¾å¤‡')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--fps', type=int, default=25, help='è§†é¢‘å¸§ç‡')
    
    args = parser.parse_args()
    
    # è§£æGPU ID
    gpu_id = 0
    if args.device.startswith('cuda:'):
        gpu_id = int(args.device.split(':')[1])
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    if not musetalk_service.initialize_models(gpu_id):
        print("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
        sys.exit(1)
    
    # æ‰§è¡Œæ¨ç†
    success = musetalk_service.inference_with_cache(
        template_id=args.template_id,
        audio_path=args.audio_path,
        output_path=args.output_path,
        cache_dir=args.cache_dir,
        batch_size=args.batch_size,
        fps=args.fps
    )
    
    if success:
        print("âœ… æ¨ç†æˆåŠŸå®Œæˆ")
        sys.exit(0)
    else:
        print("âŒ æ¨ç†å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()
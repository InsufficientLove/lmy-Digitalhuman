#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimized Preprocessing V2
ä¼˜åŒ–ç‰ˆé¢„å¤„ç† - ä¿®å¤è„¸éƒ¨é˜´å½±é—®é¢˜ï¼Œæé€Ÿé¢„å¤„ç†
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import argparse
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import copy
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

# from musetalk.utils.face_parsing import FaceParsing  # æ³¨é‡Šæ‰ï¼Œå› ä¸ºä¸å­˜åœ¨
from musetalk.utils.utils import load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image_prepare_material
from musetalk.utils.audio_processor import AudioProcessor

# å®šä¹‰coord_placeholderå¸¸é‡
coord_placeholder = (0, 0, 0, 0)  # è¡¨ç¤ºæ— æ•ˆçš„è¾¹ç•Œæ¡†

print("Optimized Preprocessing V2 - æé€Ÿé¢„å¤„ç†å¼•æ“")

class OptimizedPreprocessor:
    """ä¼˜åŒ–çš„é¢„å¤„ç†å™¨ - ä¿®å¤é˜´å½±é—®é¢˜ï¼Œæé€Ÿå¤„ç†"""
    
    def __init__(self):
        self.vae = None
        self.unet = None
        self.pe = None
        self.fp = None
        self.device = None
        self.weight_dtype = torch.float16
        self.is_initialized = False
        
        # ğŸ¨ é˜´å½±ä¿®å¤å‚æ•°
        self.shadow_fix_enabled = True
        self.lighting_adjustment = True
        self.color_correction = True
        
    def initialize_models(self, device='cuda:0'):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.is_initialized:
            return True
            
        try:
            print(f"åˆå§‹åŒ–é¢„å¤„ç†æ¨¡å‹ - è®¾å¤‡: {device}")
            self.device = device
            
            # åŠ è½½æ¨¡å‹ - æ·»åŠ é”™è¯¯å¤„ç†
            try:
                vae, unet, pe = load_all_model()
                print("é¢„å¤„ç†æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"é¢„å¤„ç†æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                # å°è¯•ä½¿ç”¨å¤‡ç”¨VAEè·¯å¾„
                try:
                    vae, unet, pe = load_all_model(vae_type="sd-vae-ft-mse")
                    print("é¢„å¤„ç†ä½¿ç”¨å¤‡ç”¨VAEæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e2:
                    print(f"é¢„å¤„ç†å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                    raise e2
            
            # ä¿®å¤æ¨¡å‹å¯¹è±¡å…¼å®¹æ€§ - ä½¿ç”¨æ­£ç¡®çš„å±æ€§ç»“æ„
            if hasattr(vae, 'vae'):
                vae.vae = vae.vae.to(device).half().eval()
                self.vae = vae
            elif hasattr(vae, 'to'):
                self.vae = vae.to(device).half().eval()
            else:
                print("è­¦å‘Š: VAEå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                self.vae = vae
            
            if hasattr(unet, 'model'):
                unet.model = unet.model.to(device).half().eval()
                self.unet = unet
            elif hasattr(unet, 'to'):
                self.unet = unet.to(device).half().eval()
            else:
                print("è­¦å‘Š: UNetå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                self.unet = unet
            
            if hasattr(pe, 'to'):
                self.pe = pe.to(device).half().eval()
            else:
                print("è­¦å‘Š: PEå¯¹è±¡æ²¡æœ‰.to()æ–¹æ³•ï¼Œè·³è¿‡ä¼˜åŒ–")
                self.pe = pe
            
            # åˆå§‹åŒ–é¢éƒ¨è§£æ - æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºFaceParsingä¸å­˜åœ¨
            # self.fp = FaceParsing()
            self.fp = None  # æš‚æ—¶è®¾ä¸ºNone
            
            print("é¢„å¤„ç†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def fix_face_shadows(self, image):
        """ä¿®å¤é¢éƒ¨é˜´å½±é—®é¢˜"""
        if not self.shadow_fix_enabled:
            return image
        
        try:
            # ğŸ¨ 1. å…‰ç…§å‡è¡¡åŒ–
            if self.lighting_adjustment:
                # è½¬æ¢åˆ°LABè‰²å½©ç©ºé—´è¿›è¡Œå…‰ç…§è°ƒæ•´
                lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
                l_channel = lab[:, :, 0]
                
                # è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                l_channel = clahe.apply(l_channel)
                
                lab[:, :, 0] = l_channel
                image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
            
            # ğŸ¨ 2. é˜´å½±æ£€æµ‹å’Œä¿®å¤
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ä½¿ç”¨å½¢æ€å­¦æ“ä½œæ£€æµ‹é˜´å½±åŒºåŸŸ
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))
            dilated = cv2.dilate(gray, kernel)
            shadow_mask = cv2.absdiff(dilated, gray)
            
            # é˜ˆå€¼å¤„ç†å¾—åˆ°é˜´å½±åŒºåŸŸ
            _, shadow_mask = cv2.threshold(shadow_mask, 30, 255, cv2.THRESH_BINARY)
            
            # å¯¹é˜´å½±åŒºåŸŸè¿›è¡Œäº®åº¦æå‡
            shadow_areas = shadow_mask > 0
            if np.any(shadow_areas):
                # æå‡é˜´å½±åŒºåŸŸäº®åº¦
                hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
                hsv[:, :, 2][shadow_areas] = np.clip(
                    hsv[:, :, 2][shadow_areas] * 1.3, 0, 255
                ).astype(np.uint8)
                image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            
            # ğŸ¨ 3. é¢œè‰²æ ¡æ­£
            if self.color_correction:
                # ç™½å¹³è¡¡è°ƒæ•´
                image = self.white_balance_correction(image)
                
                # è‚¤è‰²å¢å¼º
                image = self.skin_tone_enhancement(image)
            
            return image
            
        except Exception as e:
            print(f"é˜´å½±ä¿®å¤å¤±è´¥: {str(e)}")
            return image
    
    def white_balance_correction(self, image):
        """ç™½å¹³è¡¡æ ¡æ­£"""
        try:
            # Gray Worldç®—æ³•
            b, g, r = cv2.split(image)
            
            b_avg = np.mean(b)
            g_avg = np.mean(g) 
            r_avg = np.mean(r)
            
            # è®¡ç®—å¢ç›Š
            k = (b_avg + g_avg + r_avg) / 3
            kb = k / b_avg
            kg = k / g_avg
            kr = k / r_avg
            
            # åº”ç”¨å¢ç›Š
            b = np.clip(b * kb, 0, 255).astype(np.uint8)
            g = np.clip(g * kg, 0, 255).astype(np.uint8)
            r = np.clip(r * kr, 0, 255).astype(np.uint8)
            
            return cv2.merge([b, g, r])
            
        except:
            return image
    
    def skin_tone_enhancement(self, image):
        """è‚¤è‰²å¢å¼º"""
        try:
            # è½¬æ¢åˆ°YUVè‰²å½©ç©ºé—´
            yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)
            
            # è‚¤è‰²èŒƒå›´æ£€æµ‹
            lower_skin = np.array([0, 133, 77], dtype=np.uint8)
            upper_skin = np.array([255, 173, 127], dtype=np.uint8)
            
            skin_mask = cv2.inRange(yuv, lower_skin, upper_skin)
            
            # å¯¹è‚¤è‰²åŒºåŸŸè¿›è¡Œå¾®è°ƒ
            if np.any(skin_mask > 0):
                # è½»å¾®å¢å¼ºçº¢è‰²é€šé“
                bgr = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
                b, g, r = cv2.split(bgr)
                
                skin_areas = skin_mask > 0
                r[skin_areas] = np.clip(r[skin_areas] * 1.05, 0, 255).astype(np.uint8)
                
                return cv2.merge([b, g, r])
            
            return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
            
        except:
            return image
    
    def preprocess_template_ultra_fast(self, template_path, output_dir, template_id):
        """æé€Ÿæ¨¡æ¿é¢„å¤„ç†"""
        try:
            start_time = time.time()
            print(f"å¼€å§‹æé€Ÿé¢„å¤„ç†: {template_id}")
            
            # åˆ›å»ºæ¨¡æ¿ä¸“å±çš„å­ç›®å½•
            template_output_dir = os.path.join(output_dir, template_id)
            os.makedirs(template_output_dir, exist_ok=True)
            print(f"ä½¿ç”¨ç¼“å­˜ç›®å½•: {template_output_dir}")
            
            # 1. å¹¶è¡Œè¯»å–å’Œå¤„ç†å›¾åƒ
            print("è¯»å–æ¨¡æ¿å›¾åƒ...")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥çš„å›¾åƒæ–‡ä»¶è·¯å¾„
            template_path_obj = Path(template_path)
            if template_path_obj.is_file() and template_path_obj.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:
                input_image_path = str(template_path_obj)
                print(f"ä½¿ç”¨ç›´æ¥å›¾åƒæ–‡ä»¶: {input_image_path}")
            else:
                # åœ¨ç›®å½•ä¸­æœç´¢å›¾åƒæ–‡ä»¶
                image_files = []
                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                    image_files.extend(template_path_obj.glob(ext))
                
                if not image_files:
                    raise ValueError(f"æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶: {template_path}")
                
                # é€‰æ‹©æœ€ä½³å›¾åƒï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€å¼ ï¼‰
                input_image_path = str(image_files[0])
                print(f"ä½¿ç”¨ç›®å½•ä¸­çš„å›¾åƒ: {input_image_path}")
            
            # ğŸ¨ 2. å›¾åƒé¢„å¤„ç†å’Œé˜´å½±ä¿®å¤
            print("ğŸ¨ å›¾åƒé¢„å¤„ç†å’Œé˜´å½±ä¿®å¤...")
            image = cv2.imread(input_image_path)
            if image is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {input_image_path}")
            
            # å…³é”®ï¼šé˜´å½±ä¿®å¤
            image = self.fix_face_shadows(image)
            
            # 3. é¢éƒ¨æ£€æµ‹å’Œå…³é”®ç‚¹æå–
            print("ğŸ‘¤ é¢éƒ¨æ£€æµ‹å’Œå…³é”®ç‚¹æå–...")
            # ä¿å­˜ä¸´æ—¶å›¾åƒæ–‡ä»¶ç»™get_landmark_and_bboxä½¿ç”¨
            temp_image_path = os.path.join(output_dir, "temp_image.jpg")
            cv2.imwrite(temp_image_path, image)
            coord_list, frame_list = get_landmark_and_bbox([temp_image_path])
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_image_path):
                os.remove(temp_image_path)
            
            if not coord_list or not frame_list:
                raise ValueError("é¢éƒ¨æ£€æµ‹å¤±è´¥")
            
            # 4. é¢éƒ¨è§£æå’Œç‰¹å¾æå–
            print("ğŸ­ é¢éƒ¨è§£æå’Œç‰¹å¾æå–...")
            mask_coords_list, mask_list = [], []
            face_parsing_masks = []  # å­˜å‚¨é¢éƒ¨è§£æçš„mask
            
            # ä½¿ç”¨coord_listä½œä¸ºbbox_listï¼ˆä»get_landmark_and_bboxè¿”å›çš„ï¼‰
            for i, (frame, orig_face_box) in enumerate(zip(frame_list, coord_list)):
                if orig_face_box is None or orig_face_box == coord_placeholder:
                    print(f"è­¦å‘Š: ç¬¬{i}å¸§æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸")
                    continue
                    
                # ç¡®ä¿face_boxåªæœ‰4ä¸ªå€¼ (x, y, x1, y1)
                if isinstance(orig_face_box, (list, tuple)):
                    print(f"åŸå§‹face_box: {orig_face_box}, é•¿åº¦: {len(orig_face_box)}")
                    if len(orig_face_box) > 4:
                        face_box = list(orig_face_box[:4])  # åªå–å‰4ä¸ªå€¼å¹¶è½¬æ¢ä¸ºåˆ—è¡¨
                        print(f"è£å‰ªåçš„face_box: {face_box}")
                    elif len(orig_face_box) == 4:
                        face_box = list(orig_face_box)  # è½¬æ¢ä¸ºåˆ—è¡¨
                    else:
                        print(f"è­¦å‘Š: face_boxæ ¼å¼ä¸æ­£ç¡®: {orig_face_box}")
                        continue
                else:
                    print(f"è­¦å‘Š: face_boxç±»å‹ä¸æ­£ç¡®: {type(orig_face_box)}")
                    continue
                
                # é¢éƒ¨è§£æ - ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨
                try:
                    # å°è¯•å¸¸è§çš„é¢éƒ¨è§£ææ–¹æ³•
                    if hasattr(self.fp, '__call__'):
                        result = self.fp(frame)
                        # æ£€æŸ¥è¿”å›å€¼ç±»å‹
                        if isinstance(result, tuple):
                            mask_out = result[0] if len(result) > 0 else np.zeros_like(frame[:,:,0])
                        elif isinstance(result, np.ndarray):
                            mask_out = result
                        else:
                            mask_out = np.array(result)
                    elif hasattr(self.fp, 'parse'):
                        mask_out = self.fp.parse(frame)
                    elif hasattr(self.fp, 'predict'):
                        mask_out = self.fp.predict(frame)
                    else:
                        print("é¢éƒ¨è§£æå¤±è´¥: FaceParsingå¯¹è±¡æ²¡æœ‰å¯ç”¨çš„è§£ææ–¹æ³•")
                        # ä½¿ç”¨é»˜è®¤maskï¼ˆå…¨ç™½ï¼Œè¡¨ç¤ºæ•´ä¸ªé¢éƒ¨åŒºåŸŸï¼‰
                        mask_out = np.ones((frame.shape[0], frame.shape[1]), dtype=np.uint8) * 255
                except Exception as e:
                    print(f"é¢éƒ¨è§£æå‡ºé”™: {e}")
                    # ä½¿ç”¨é»˜è®¤mask
                    mask_out = np.ones((frame.shape[0], frame.shape[1]), dtype=np.uint8) * 255
                
                # ä¿å­˜é¢éƒ¨è§£æçš„mask
                face_parsing_masks.append(mask_out)
                
                # è·å–é¢éƒ¨åŒºåŸŸåæ ‡ - ä¼ å…¥æ­£ç¡®çš„å‚æ•°
                mask, crop_box = get_image_prepare_material(frame, face_box, fp=self.fp)
                mask_coords_list.append(crop_box)
                mask_list.append(mask)
            
            # 5. VAEç¼–ç  - å¹¶è¡Œå¤„ç†
            print("VAEç¼–ç ...")
            input_latent_list = []
            
            def encode_frame(frame, mask=None):
                with torch.no_grad():
                    frame_tensor = torch.from_numpy(frame).float().to(self.device) / 127.5 - 1.0
                    frame_tensor = frame_tensor.permute(2, 0, 1).unsqueeze(0)
                    
                    # ç¼–ç åŸå§‹å¸§å¾—åˆ°reference latent (4é€šé“)
                    reference_latent = self.vae.encode_latents(frame_tensor)
                    
                    # å¦‚æœæœ‰maskï¼Œåˆ›å»ºmaskedç‰ˆæœ¬
                    if mask is not None and mask.size > 0:
                        # è°ƒè¯•ï¼šæ‰“å°maskä¿¡æ¯
                        print(f"Face parsing mask shape: {mask.shape}, dtype: {mask.dtype}, unique values: {np.unique(mask)[:5]}")
                        
                        # å¤„ç†é¢éƒ¨è§£æmask
                        # é¢éƒ¨è§£æmaské€šå¸¸åŒ…å«ä¸åŒçš„æ ‡ç­¾å€¼ï¼ˆ0=èƒŒæ™¯ï¼Œ1-19=ä¸åŒé¢éƒ¨åŒºåŸŸï¼‰
                        # åˆ›å»ºäºŒå€¼maskï¼šéèƒŒæ™¯åŒºåŸŸä¸º1ï¼ŒèƒŒæ™¯ä¸º0
                        binary_mask = (mask > 0).astype(np.float32)
                        
                        # å¦‚æœéœ€è¦ï¼Œå¯ä»¥å¯¹maskè¿›è¡Œå¹³æ»‘å¤„ç†
                        from scipy.ndimage import gaussian_filter
                        binary_mask = gaussian_filter(binary_mask, sigma=1.0)
                        
                        # å°†maskè½¬æ¢ä¸ºtensor
                        mask_tensor = torch.from_numpy(binary_mask).float().to(self.device)
                        
                        # è°ƒæ•´maskç»´åº¦ä»¥åŒ¹é…frame_tensor
                        if len(mask_tensor.shape) == 2:  # [H, W]
                            mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]
                        
                        # å¦‚æœmaskå’Œframeå°ºå¯¸ä¸åŒ¹é…ï¼Œè¿›è¡Œresize
                        if mask_tensor.shape[-2:] != frame_tensor.shape[-2:]:
                            mask_tensor = torch.nn.functional.interpolate(
                                mask_tensor, 
                                size=frame_tensor.shape[-2:], 
                                mode='bilinear', 
                                align_corners=False
                            )
                        
                        # æ‰©å±•maskåˆ°3é€šé“
                        mask_tensor = mask_tensor.repeat(1, 3, 1, 1)  # [1, 3, H, W]
                        
                        # åˆ›å»ºmasked frameï¼ˆä¿ç•™é¢éƒ¨åŒºåŸŸï¼ŒèƒŒæ™¯å˜é»‘ï¼‰
                        masked_frame_tensor = frame_tensor * mask_tensor
                        masked_latent = self.vae.encode_latents(masked_frame_tensor)
                        
                        # æ‹¼æ¥maskedå’Œreference latentå¾—åˆ°8é€šé“
                        combined_latent = torch.cat([masked_latent, reference_latent], dim=1)
                    else:
                        # å¦‚æœæ²¡æœ‰maskï¼Œç›´æ¥å¤åˆ¶reference latent
                        print("No face parsing mask available, using duplicated reference latent")
                        combined_latent = torch.cat([reference_latent, reference_latent], dim=1)
                    
                    return combined_latent.cpu()
            
            # å¹¶è¡Œç¼–ç å¤šå¸§
            with ThreadPoolExecutor(max_workers=4) as executor:
                # å°†frameå’Œå¯¹åº”çš„face parsing maskä¸€èµ·ä¼ é€’
                futures = []
                for i, frame in enumerate(frame_list):
                    face_mask = face_parsing_masks[i] if i < len(face_parsing_masks) else None
                    futures.append(executor.submit(encode_frame, frame, face_mask))
                
                for future in as_completed(futures):
                    latent = future.result()
                    input_latent_list.append(latent)
            
            # 6. åˆ›å»ºå¾ªç¯æ•°æ®
            print("ğŸ”„ åˆ›å»ºå¾ªç¯æ•°æ®...")
            
            # éªŒè¯latenté€šé“æ•°
            if input_latent_list:
                latent_shape = input_latent_list[0].shape
                print(f"âœ… Latentå½¢çŠ¶: {latent_shape} (åº”è¯¥æ˜¯8é€šé“)")
                if latent_shape[1] != 8:
                    print(f"âš ï¸ è­¦å‘Š: Latenté€šé“æ•°ä¸º{latent_shape[1]}ï¼ŒæœŸæœ›ä¸º8é€šé“")
            
            # å¦‚æœåªæœ‰ä¸€å¸§ï¼Œå¤åˆ¶åˆ›å»ºå¾ªç¯
            if len(input_latent_list) == 1:
                input_latent_list_cycle = input_latent_list * 2
                coord_list_cycle = coord_list * 2
                frame_list_cycle = frame_list * 2
                mask_coords_list_cycle = mask_coords_list * 2
                mask_list_cycle = mask_list * 2
            else:
                input_latent_list_cycle = input_latent_list
                coord_list_cycle = coord_list
                frame_list_cycle = frame_list
                mask_coords_list_cycle = mask_coords_list
                mask_list_cycle = mask_list
            
            # 7. ä¿å­˜é¢„å¤„ç†ç¼“å­˜
            print("ğŸ’¾ ä¿å­˜é¢„å¤„ç†ç¼“å­˜...")
            
            cache_data = {
                'input_latent_list_cycle': input_latent_list_cycle,
                'coord_list_cycle': coord_list_cycle,
                'frame_list_cycle': frame_list_cycle,
                'mask_coords_list_cycle': mask_coords_list_cycle,
                'mask_list_cycle': mask_list_cycle,
            }
            
            # ä¿å­˜ç¼“å­˜æ–‡ä»¶åˆ°æ¨¡æ¿å­ç›®å½•
            cache_file = os.path.join(template_output_dir, f"{template_id}_preprocessed.pkl")
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'template_id': template_id,
                'template_path': template_path,
                'processed_at': time.time(),
                'frame_count': len(frame_list_cycle),
                'shadow_fix_enabled': self.shadow_fix_enabled,
                'lighting_adjustment': self.lighting_adjustment,
                'color_correction': self.color_correction
            }
            
            metadata_file = os.path.join(template_output_dir, f"{template_id}_metadata.json")
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜ç®€åŒ–çš„çŠ¶æ€æ–‡ä»¶ï¼ˆå…¼å®¹æ€§ï¼‰
            state_file = os.path.join(template_output_dir, "model_state.pkl")
            with open(state_file, 'wb') as f:
                pickle.dump({'status': 'completed', 'template_id': template_id}, f)
            
            total_time = time.time() - start_time
            print(f"æé€Ÿé¢„å¤„ç†å®Œæˆï¼")
            print(f"å¤„ç†ç»Ÿè®¡:")
            print(f"   - æ¨¡æ¿ID: {template_id}")
            print(f"   - å¸§æ•°: {len(frame_list_cycle)}")
            print(f"   - è€—æ—¶: {total_time:.2f}ç§’")
            print(f"   - é˜´å½±ä¿®å¤: {'å¯ç”¨' if self.shadow_fix_enabled else 'ç¦ç”¨'}")
            print(f"   - ç¼“å­˜æ–‡ä»¶: {cache_file}")
            
            return True
            
        except Exception as e:
            print(f"é¢„å¤„ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆæ¨¡æ¿é¢„å¤„ç†')
    parser.add_argument('--template_path', type=str, required=True, help='æ¨¡æ¿å›¾åƒè·¯å¾„')
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--template_id', type=str, required=True, help='æ¨¡æ¿ID')
    parser.add_argument('--device', type=str, default='cuda:0', help='è®¾å¤‡')
    parser.add_argument('--disable_shadow_fix', action='store_true', help='ç¦ç”¨é˜´å½±ä¿®å¤')
    parser.add_argument('--disable_lighting', action='store_true', help='ç¦ç”¨å…‰ç…§è°ƒæ•´')
    parser.add_argument('--disable_color_correction', action='store_true', help='ç¦ç”¨é¢œè‰²æ ¡æ­£')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = OptimizedPreprocessor()
    
    # é…ç½®é˜´å½±ä¿®å¤é€‰é¡¹
    preprocessor.shadow_fix_enabled = not args.disable_shadow_fix
    preprocessor.lighting_adjustment = not args.disable_lighting
    preprocessor.color_correction = not args.disable_color_correction
    
    print(f"ğŸ¨ é˜´å½±ä¿®å¤é…ç½®:")
    print(f"   - é˜´å½±ä¿®å¤: {'å¯ç”¨' if preprocessor.shadow_fix_enabled else 'ç¦ç”¨'}")
    print(f"   - å…‰ç…§è°ƒæ•´: {'å¯ç”¨' if preprocessor.lighting_adjustment else 'ç¦ç”¨'}")
    print(f"   - é¢œè‰²æ ¡æ­£: {'å¯ç”¨' if preprocessor.color_correction else 'ç¦ç”¨'}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    if not preprocessor.initialize_models(args.device):
        print("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
        return
    
    # æ‰§è¡Œé¢„å¤„ç†
    success = preprocessor.preprocess_template_ultra_fast(
        args.template_path,
        args.output_dir, 
        args.template_id
    )
    
    if success:
        print("é¢„å¤„ç†æˆåŠŸå®Œæˆ")
    else:
        print("é¢„å¤„ç†å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ V3 - çœŸæ­£çš„4GPUå¹¶è¡Œ
è§£å†³æ‰€æœ‰æ€§èƒ½ç“¶é¢ˆå’Œé¢éƒ¨é¢œè‰²é—®é¢˜

å…³é”®ä¼˜åŒ–ï¼š
1. çœŸæ­£çš„4GPUå¹¶è¡Œæ¨ç†
2. æ¨¡å‹æŒä¹…åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
3. ä¿®å¤é¢éƒ¨blendingé—®é¢˜ï¼ˆä½¿ç”¨jawæ¨¡å¼ï¼‰
4. æé€Ÿæ¨ç†ï¼Œå®Œå…¨åˆ©ç”¨é¢„å¤„ç†ç¼“å­˜
"""

import argparse
import os
import json
import time
import threading
import queue
import multiprocessing as mp
from pathlib import Path
from omegaconf import OmegaConf
import numpy as np
import cv2
import torch
import glob
import pickle
import copy
from tqdm import tqdm
from transformers import WhisperModel
import concurrent.futures

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image_prepare_material, get_image_blending
from musetalk.utils.audio_processor import AudioProcessor

class TrueParallelMuseTalkInference:
    """çœŸæ­£çš„4GPUå¹¶è¡ŒMuseTalkæ¨ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.gpu_count = min(4, torch.cuda.device_count())  # æœ€å¤šä½¿ç”¨4ä¸ªGPU
        self.models = {}  # æ¯ä¸ªGPUçš„æ¨¡å‹
        self.templates = {}  # é¢„å¤„ç†æ¨¡æ¿ç¼“å­˜
        self.is_initialized = False
        
        # ğŸš€ æŒä¹…åŒ–æ£€æŸ¥
        self.persistent_state_file = "persistent_4gpu_models.pkl"
        
        print(f"ğŸ® åˆå§‹åŒ–çœŸæ­£çš„{self.gpu_count}GPUå¹¶è¡Œæ¨ç†å™¨...")
        
        if os.path.exists(self.persistent_state_file) and self._load_persistent_state():
            print("ğŸš€ ä»æŒä¹…åŒ–çŠ¶æ€æé€Ÿæ¢å¤!")
        else:
            print("ğŸ”„ é¦–æ¬¡åˆå§‹åŒ–æ‰€æœ‰GPUæ¨¡å‹...")
            self._initialize_all_gpus()
            self._save_persistent_state()
        
        self.is_initialized = True
        print(f"âœ… {self.gpu_count}GPUå¹¶è¡Œæ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ!")
    
    def _initialize_all_gpus(self):
        """å¹¶è¡Œåˆå§‹åŒ–æ‰€æœ‰GPUä¸Šçš„æ¨¡å‹"""
        print("ğŸ® å¹¶è¡Œåˆå§‹åŒ–æ‰€æœ‰GPUæ¨¡å‹...")
        
        def init_gpu_model(gpu_id):
            device = torch.device(f"cuda:{gpu_id}")
            print(f"ğŸ® åˆå§‹åŒ–GPU {gpu_id}...")
            
            # ä¸ºæ¯ä¸ªGPUåŠ è½½ç‹¬ç«‹çš„æ¨¡å‹
            try:
                # å°è¯•æ–°ç‰ˆæœ¬load_all_modelè°ƒç”¨
                audio_processor, vae, unet, pe = load_all_model(
                                     unet_model_path="../MuseTalk/models/musetalk/pytorch_model.bin",
                 vae_type="sd-vae", 
                 unet_config="../MuseTalk/models/musetalk/musetalk.json",
                    device=device
                )
            except TypeError:
                # å›é€€åˆ°æ—§ç‰ˆæœ¬è°ƒç”¨
                audio_processor, vae, unet, pe = load_all_model(
                    getattr(self.config, 'version', 'v1'), 
                    getattr(self.config, 'fp16', True), 
                    device
                )
            
            # åˆå§‹åŒ–é¢éƒ¨è§£æå™¨
            if hasattr(self.config, 'version') and self.config.version == "v15":
                fp = FaceParsing(
                    left_cheek_width=getattr(self.config, 'left_cheek_width', 90),
                    right_cheek_width=getattr(self.config, 'right_cheek_width', 90),
                    device=device
                )
            else:
                fp = FaceParsing(device=device)
            
            return {
                'gpu_id': gpu_id,
                'device': device,
                'audio_processor': audio_processor,
                'vae': vae,
                'unet': unet,
                'pe': pe,
                'fp': fp,
                'initialized_at': time.time()
            }
        
        # ğŸš€ å¹¶è¡Œåˆå§‹åŒ–æ‰€æœ‰GPU
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.gpu_count) as executor:
            futures = [executor.submit(init_gpu_model, gpu_id) for gpu_id in range(self.gpu_count)]
            
            for future in concurrent.futures.as_completed(futures):
                model_info = future.result()
                gpu_id = model_info['gpu_id']
                self.models[gpu_id] = model_info
                print(f"âœ… GPU {gpu_id} æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
        print(f"ğŸ‰ æ‰€æœ‰{self.gpu_count}ä¸ªGPUæ¨¡å‹åˆå§‹åŒ–å®Œæˆ!")
    
    def _save_persistent_state(self):
        """ä¿å­˜æŒä¹…åŒ–çŠ¶æ€"""
        try:
            state = {
                'gpu_count': self.gpu_count,
                'config': self.config,
                'models_initialized': True,
                'saved_at': time.time()
            }
            with open(self.persistent_state_file, 'wb') as f:
                pickle.dump(state, f)
            print(f"ğŸ’¾ æŒä¹…åŒ–çŠ¶æ€å·²ä¿å­˜: {self.persistent_state_file}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æŒä¹…åŒ–çŠ¶æ€å¤±è´¥: {e}")
    
    def _load_persistent_state(self):
        """åŠ è½½æŒä¹…åŒ–çŠ¶æ€"""
        try:
            with open(self.persistent_state_file, 'rb') as f:
                state = pickle.load(f)
            
            if state.get('gpu_count') == self.gpu_count:
                # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ¨¡å‹å¯¹è±¡æ— æ³•åºåˆ—åŒ–ï¼‰
                self._initialize_all_gpus()
                print("âœ… æŒä¹…åŒ–çŠ¶æ€åŠ è½½æˆåŠŸ")
                return True
            else:
                print("âš ï¸ GPUæ•°é‡ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ–")
                return False
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æŒä¹…åŒ–çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def preprocess_template(self, template_id, template_path):
        """é¢„å¤„ç†æ¨¡æ¿ï¼ˆå®Œå…¨æ°¸ä¹…åŒ–ï¼‰"""
        cache_dir = os.path.join("model_states", template_id)
        os.makedirs(cache_dir, exist_ok=True)
        cache_file = os.path.join(cache_dir, "full_template_cache.pkl")
        
        if os.path.exists(cache_file):
            print(f"ğŸš€ æé€ŸåŠ è½½æ¨¡æ¿ç¼“å­˜: {template_id}")
            with open(cache_file, 'rb') as f:
                template_data = pickle.load(f)
            self.templates[template_id] = template_data
            return template_data
        
        print(f"ğŸ”„ é¦–æ¬¡é¢„å¤„ç†æ¨¡æ¿: {template_id}")
        
        # ä½¿ç”¨GPU 0è¿›è¡Œé¢„å¤„ç†
        gpu_0_model = self.models[0]
        device = gpu_0_model['device']
        vae = gpu_0_model['vae']
        fp = gpu_0_model['fp']
        
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(template_path)
        if img is None:
            raise ValueError(f"æ— æ³•è¯»å–æ¨¡æ¿å›¾ç‰‡: {template_path}")
        
        # è·å–é¢éƒ¨åæ ‡
        bbox_shift = getattr(self.config, 'bbox_shift', 0)
        coord_list, frame_list = get_landmark_and_bbox([template_path], bbox_shift)
        
        # é¢„è®¡ç®—VAEç¼–ç 
        input_latent_list = []
        for bbox, frame in zip(coord_list, frame_list):
            x1, y1, x2, y2 = bbox
            if x1 >= x2 or y1 >= y2:
                continue
                
            h, w = frame.shape[:2]
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w, int(x2)), min(h, int(y2))
            
            # V15ç‰ˆæœ¬çš„é¢å¤–è¾¹è·
            if hasattr(self.config, 'version') and self.config.version == "v15":
                extra_margin = getattr(self.config, 'extra_margin', 0)
                y2 = min(y2 + extra_margin, frame.shape[0])
            
            crop_frame = frame[y1:y2, x1:x2]
            resized_crop_frame = cv2.resize(crop_frame, (256, 256), interpolation=cv2.INTER_LANCZOS4)
            
            with torch.no_grad():
                latents = vae.get_latents_for_unet(resized_crop_frame)
                input_latent_list.append(latents)
        
        # åˆ›å»ºå¾ªç¯åˆ—è¡¨
        frame_list_cycle = frame_list + frame_list[::-1]
        coord_list_cycle = coord_list + coord_list[::-1]
        input_latent_list_cycle = input_latent_list + input_latent_list[::-1]
        
        # ğŸ¯ å…³é”®ï¼šé¢„è®¡ç®—é¢éƒ¨è§£æï¼ˆä½¿ç”¨æ­£ç¡®çš„jawæ¨¡å¼ï¼‰
        mask_coords_list_cycle = []
        mask_list_cycle = []
        
        for i, frame in enumerate(tqdm(frame_list_cycle, desc="é¢„è®¡ç®—é¢éƒ¨è§£æ")):
            x1, y1, x2, y2 = coord_list_cycle[i]
            h, w = frame.shape[:2]
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w, int(x2)), min(h, int(y2))
            
            if x1 >= x2 or y1 >= y2:
                mask_coords_list_cycle.append([0, 0, 256, 256])
                mask_list_cycle.append(np.ones((256, 256), dtype=np.uint8) * 255)
                continue
            
            try:
                # ğŸ¯ å…³é”®ï¼šä½¿ç”¨æ­£ç¡®çš„jawæ¨¡å¼è§£å†³é¢éƒ¨ç°è‰²é—®é¢˜
                if hasattr(self.config, 'version') and self.config.version == "v15":
                    mode = getattr(self.config, 'parsing_mode', 'jaw')  # é»˜è®¤jawæ¨¡å¼
                else:
                    mode = "raw"
                
                mask, crop_box = get_image_prepare_material(frame, [x1, y1, x2, y2], fp=fp, mode=mode)
                mask_coords_list_cycle.append(crop_box)
                mask_list_cycle.append(mask)
                
            except Exception as e:
                print(f"[WARN] é¢éƒ¨è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤mask: {e}")
                mask_coords_list_cycle.append([x1, y1, x2, y2])
                # åˆ›å»ºåˆç†çš„é»˜è®¤maskè€Œä¸æ˜¯å…¨ç™½
                default_mask = np.zeros((256, 256), dtype=np.uint8)
                default_mask[64:192, 64:192] = 255  # åªåœ¨ä¸­å¿ƒåŒºåŸŸåº”ç”¨
                mask_list_cycle.append(default_mask)
        
        # ä¿å­˜å®Œæ•´çš„æ¨¡æ¿æ•°æ®
        template_data = {
            'template_id': template_id,
            'template_path': template_path,
            'frame_list_cycle': frame_list_cycle,
            'coord_list_cycle': coord_list_cycle,
            'input_latent_list_cycle': input_latent_list_cycle,
            'mask_coords_list_cycle': mask_coords_list_cycle,
            'mask_list_cycle': mask_list_cycle,
            'preprocessed_at': time.time(),
            'config_version': getattr(self.config, 'version', 'v1'),
            'parsing_mode': getattr(self.config, 'parsing_mode', 'jaw')
        }
        
        # æ°¸ä¹…åŒ–ä¿å­˜
        with open(cache_file, 'wb') as f:
            pickle.dump(template_data, f)
        
        self.templates[template_id] = template_data
        print(f"âœ… æ¨¡æ¿é¢„å¤„ç†å®Œæˆå¹¶æ°¸ä¹…åŒ–: {template_id}")
        return template_data
    
    def inference_4gpu_parallel(self, template_id, audio_path, output_path, fps=25):
        """çœŸæ­£çš„4GPUå¹¶è¡Œæ¨ç†"""
        if template_id not in self.templates:
            template_path = self._find_template_path(template_id)
            self.preprocess_template(template_id, template_path)
        
        template_data = self.templates[template_id]
        
        print(f"ğŸµ æå–éŸ³é¢‘ç‰¹å¾...")
        # ä½¿ç”¨GPU 0çš„éŸ³é¢‘å¤„ç†å™¨
        audio_processor = self.models[0]['audio_processor']
        whisper_feature = audio_processor.audio2feat(audio_path)
        whisper_chunks = audio_processor.feature2chunks(feature_array=whisper_feature, fps=fps)
        
        video_num = len(whisper_chunks)
        print(f"ğŸ® å¼€å§‹{self.gpu_count}GPUå¹¶è¡Œæ¨ç†: {video_num}å¸§")
        
        # ğŸš€ çœŸæ­£çš„4GPUå¹¶è¡Œæ¨ç†
        chunk_size = max(1, video_num // self.gpu_count)
        gpu_tasks = []
        
        for gpu_id in range(self.gpu_count):
            start_idx = gpu_id * chunk_size
            if gpu_id == self.gpu_count - 1:
                end_idx = video_num  # æœ€åä¸€ä¸ªGPUå¤„ç†å‰©ä½™æ‰€æœ‰å¸§
            else:
                end_idx = (gpu_id + 1) * chunk_size
            
            if start_idx < video_num:
                gpu_chunks = whisper_chunks[start_idx:end_idx]
                gpu_tasks.append((gpu_id, start_idx, end_idx, gpu_chunks))
        
        print(f"ğŸ“Š GPUä»»åŠ¡åˆ†é…: {[(task[0], len(task[3])) for task in gpu_tasks]}")
        
        # å¹¶è¡Œæ‰§è¡Œæ¨ç†
        results = [None] * video_num
        
        def gpu_inference_worker(gpu_id, start_idx, end_idx, gpu_chunks):
            model = self.models[gpu_id]
            device = model['device']
            unet = model['unet']
            vae = model['vae']
            
            print(f"ğŸ® GPU {gpu_id} å¼€å§‹å¤„ç†å¸§ {start_idx}-{end_idx-1}")
            
            gpu_results = []
            batch_size = min(getattr(self.config, 'batch_size', 32), 64)
            
            for i in range(0, len(gpu_chunks), batch_size):
                batch_end = min(i + batch_size, len(gpu_chunks))
                batch_chunks = gpu_chunks[i:batch_end]
                
                # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                batch_latents = []
                for j, chunk in enumerate(batch_chunks):
                    frame_idx = (start_idx + i + j) % len(template_data['input_latent_list_cycle'])
                    input_latent = template_data['input_latent_list_cycle'][frame_idx]
                    batch_latents.append(input_latent)
                
                # GPUæ¨ç†
                with torch.no_grad():
                    batch_latents = torch.stack(batch_latents).to(device)
                    batch_chunks_tensor = torch.stack([torch.tensor(chunk).to(device) for chunk in batch_chunks])
                    
                    # ä½¿ç”¨halfç²¾åº¦åŠ é€Ÿ
                    if getattr(self.config, 'fp16', True):
                        batch_latents = batch_latents.half()
                        batch_chunks_tensor = batch_chunks_tensor.half()
                    
                    # UNetæ¨ç†
                    cfg_scale = getattr(self.config, 'cfg_scale', 3.5)
                    batch_results = unet.forward_with_cfg(batch_latents, batch_chunks_tensor, cfg_scale)
                    
                    # VAEè§£ç 
                    batch_frames = vae.decode_latents(batch_results)
                    
                    for frame in batch_frames:
                        gpu_results.append(frame.cpu().numpy())
            
            # å°†ç»“æœæ”¾å…¥æ­£ç¡®ä½ç½®
            for i, result in enumerate(gpu_results):
                results[start_idx + i] = result
            
            print(f"âœ… GPU {gpu_id} å®Œæˆæ¨ç†")
        
        # å¯åŠ¨æ‰€æœ‰GPUå¹¶è¡Œæ¨ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.gpu_count) as executor:
            futures = [
                executor.submit(gpu_inference_worker, gpu_id, start_idx, end_idx, gpu_chunks)
                for gpu_id, start_idx, end_idx, gpu_chunks in gpu_tasks
            ]
            
            # ç­‰å¾…æ‰€æœ‰GPUå®Œæˆ
            concurrent.futures.wait(futures)
        
        print(f"ğŸ¬ å¼€å§‹è§†é¢‘åˆæˆ...")
        # è§†é¢‘åˆæˆ - ä½¿ç”¨æ­£ç¡®çš„blending
        final_frames = []
        for i, res_frame in enumerate(tqdm(results, desc="è§†é¢‘åˆæˆ")):
            if res_frame is None:
                continue
                
            frame_idx = i % len(template_data['frame_list_cycle'])
            ori_frame = template_data['frame_list_cycle'][frame_idx]
            bbox = template_data['coord_list_cycle'][frame_idx]
            mask = template_data['mask_list_cycle'][frame_idx]
            mask_crop_box = template_data['mask_coords_list_cycle'][frame_idx]
            
            try:
                # è°ƒæ•´res_frameå°ºå¯¸
                x1, y1, x2, y2 = bbox
                res_frame_resized = cv2.resize(res_frame.astype(np.uint8), (x2 - x1, y2 - y1))
                
                # ğŸ¯ å…³é”®ï¼šæ­£ç¡®çš„blendingè°ƒç”¨
                combine_frame = get_image_blending(ori_frame, res_frame_resized, bbox, mask, mask_crop_box)
                final_frames.append(combine_frame)
                
            except Exception as e:
                print(f"[WARN] å¸§{i} blendingå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¸§: {e}")
                final_frames.append(ori_frame)
        
        # ä¿å­˜è§†é¢‘
        self._save_video_optimized(final_frames, audio_path, output_path, fps)
        print(f"ğŸ‰ 4GPUå¹¶è¡Œæ¨ç†å®Œæˆ: {output_path}")
        return output_path
    
    def _find_template_path(self, template_id):
        """æŸ¥æ‰¾æ¨¡æ¿æ–‡ä»¶è·¯å¾„"""
        template_dir = os.getenv('TEMPLATE_DIR', './wwwroot/templates')
        extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        
        for ext in extensions:
            path = os.path.join(template_dir, f"{template_id}{ext}")
            if os.path.exists(path):
                return path
        
        raise FileNotFoundError(f"æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_id}")
    
    def _save_video_optimized(self, frames, audio_path, output_path, fps):
        """ä¼˜åŒ–çš„è§†é¢‘ä¿å­˜"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯ä»¥ä¿å­˜")
        
        h, w, c = frames[0].shape
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        temp_video = output_path.replace('.mp4', '_temp.mp4')
        
        out = cv2.VideoWriter(temp_video, fourcc, fps, (w, h))
        for frame in frames:
            out.write(frame)
        out.release()
        
        # åˆå¹¶éŸ³é¢‘
        cmd = f'ffmpeg -y -i "{temp_video}" -i "{audio_path}" -c:v copy -c:a aac -strict experimental "{output_path}" -loglevel quiet'
        os.system(cmd)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_video):
            os.remove(temp_video)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--template_id", required=True)
    parser.add_argument("--audio_path", required=True)
    parser.add_argument("--output_path", required=True)
    parser.add_argument("--template_dir", default="./wwwroot/templates")
    parser.add_argument("--fps", type=int, default=25)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--unet_config", default="../MuseTalk/models/musetalk/musetalk.json")
    parser.add_argument("--version", default="v1")
    parser.add_argument("--parsing_mode", default="jaw")
    parser.add_argument("--fp16", action="store_true", default=True)
    
    # ğŸ”§ å…¼å®¹C#ä¼ é€’çš„é¢å¤–å‚æ•°ï¼ˆå¿½ç•¥ä¸ä½¿ç”¨ï¼‰
    parser.add_argument("--unet_model_path", default="../MuseTalk/models/musetalk/pytorch_model.bin", help="UNetæ¨¡å‹è·¯å¾„ï¼ˆå…¼å®¹å‚æ•°ï¼‰")
    parser.add_argument("--whisper_dir", default="../MuseTalk/models/whisper", help="Whisperæ¨¡å‹ç›®å½•ï¼ˆå…¼å®¹å‚æ•°ï¼‰")
    parser.add_argument("--vae_type", default="sd-vae", help="VAEç±»å‹ï¼ˆå…¼å®¹å‚æ•°ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['TEMPLATE_DIR'] = args.template_dir
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.unet_config)
    config.batch_size = args.batch_size
    config.version = args.version
    config.parsing_mode = args.parsing_mode
    config.fp16 = args.fp16
    
    print("ğŸš€ å¯åŠ¨çœŸæ­£çš„4GPUå¹¶è¡ŒMuseTalkæ¨ç†å™¨...")
    
    # åˆ›å»ºæ¨ç†å™¨
    inference_engine = TrueParallelMuseTalkInference(config)
    
    # æ‰§è¡Œæ¨ç†
    start_time = time.time()
    result_path = inference_engine.inference_4gpu_parallel(
        args.template_id, 
        args.audio_path, 
        args.output_path, 
        args.fps
    )
    
    elapsed_time = time.time() - start_time
    print(f"ğŸ‰ 4GPUå¹¶è¡Œæ¨ç†å®Œæˆ! æ€»è€—æ—¶: {elapsed_time:.2f}s")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result_path}")

if __name__ == "__main__":
    main()
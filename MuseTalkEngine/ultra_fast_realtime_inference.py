#!/usr/bin/env python3
"""
Ultra-Fast Real-time MuseTalk Inference System
è¶…é«˜é€Ÿå®æ—¶MuseTalkæ¨ç†ç³»ç»Ÿ

åŸºäºé¢„å¤„ç†ç¼“å­˜çš„æé€Ÿæ¨ç†ï¼Œæ¨ç†è¿‡ç¨‹åªæ¶‰åŠï¼š
1. éŸ³é¢‘ç‰¹å¾æå–ï¼ˆä¸€æ¬¡æ€§ï¼‰
2. UNetæ¨ç†
3. VAEè§£ç 
4. å›¾åƒèåˆ

æ¶ˆé™¤äº†é‡å¤çš„é¢éƒ¨æ£€æµ‹ã€VAEç¼–ç ç­‰è€—æ—¶æ“ä½œ

ä½œè€…: Claude Sonnet
ç‰ˆæœ¬: 1.0
"""

import os
import cv2
import torch
import numpy as np
import time
import argparse
from pathlib import Path
from tqdm import tqdm
import threading
import queue
import concurrent.futures

# MuseTalkç»„ä»¶å¯¼å…¥
from musetalk.utils.blending import get_image_blending
from musetalk.utils.utils import load_all_model
from musetalk.utils.audio_processor import AudioProcessor
from transformers import WhisperModel

# å¯¼å…¥å¢å¼ºé¢„å¤„ç†å™¨
from enhanced_musetalk_preprocessing import EnhancedMuseTalkPreprocessor


class UltraFastRealtimeInference:
    """
    è¶…é«˜é€Ÿå®æ—¶æ¨ç†ç³»ç»Ÿ
    
    ç‰¹ç‚¹ï¼š
    - åŸºäºé¢„å¤„ç†ç¼“å­˜ï¼Œæ¶ˆé™¤é‡å¤è®¡ç®—
    - å¤šGPUå¹¶è¡Œæ¨ç†
    - å†…å­˜ä¼˜åŒ–çš„æ‰¹å¤„ç†
    - å®æ—¶éŸ³é¢‘æµå¤„ç†
    """
    
        def __init__(self, 
                 model_config_path="../MuseTalk/models/musetalk/musetalk.json",
                 model_weights_path="../MuseTalk/models/musetalk/pytorch_model.bin",
                 vae_type="sd-vae",
                 whisper_dir="../MuseTalk/models/whisper",
                 device="cuda:0",
                 cache_dir="./template_cache",
                 batch_size=32,
                 fp16=True):
        """
        åˆå§‹åŒ–è¶…é«˜é€Ÿæ¨ç†ç³»ç»Ÿ
        
        Args:
            model_config_path: UNeté…ç½®è·¯å¾„
            model_weights_path: UNetæƒé‡è·¯å¾„
            vae_type: VAEç±»å‹
            whisper_dir: Whisperæ¨¡å‹ç›®å½•
            device: ä¸»è®¾å¤‡
            cache_dir: ç¼“å­˜ç›®å½•
            batch_size: æ‰¹å¤„ç†å¤§å°
            fp16: æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦
        """
        self.device = torch.device(device)
        self.cache_dir = Path(cache_dir)
        self.batch_size = batch_size
        self.fp16 = fp16
        
        print(f"ğŸš€ åˆå§‹åŒ–è¶…é«˜é€Ÿå®æ—¶æ¨ç†ç³»ç»Ÿ...")
        print(f"ğŸ“± ä¸»è®¾å¤‡: {self.device}")
        print(f"ğŸ¯ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # åˆå§‹åŒ–é¢„å¤„ç†å™¨ï¼ˆç”¨äºåŠ è½½ç¼“å­˜ï¼‰
        self.preprocessor = EnhancedMuseTalkPreprocessor(
            model_config_path=model_config_path,
            model_weights_path=model_weights_path,
            vae_type=vae_type,
            device=device,
            cache_dir=cache_dir
        )
        
        # è·å–æ¨¡å‹ç»„ä»¶å¼•ç”¨
        self.vae = self.preprocessor.vae
        self.unet = self.preprocessor.unet
        self.pe = self.preprocessor.pe
        
        # ä¼˜åŒ–æ¨¡å‹ç²¾åº¦
        if self.fp16:
            self.pe = self.pe.half()
            self.vae.vae = self.vae.vae.half()
            self.unet.model = self.unet.model.half()
        
        # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨
        self.audio_processor = AudioProcessor(feature_extractor_path=whisper_dir)
        self.weight_dtype = self.unet.model.dtype
        
        # åˆå§‹åŒ–Whisperæ¨¡å‹
        self.whisper = WhisperModel.from_pretrained(whisper_dir)
        self.whisper = self.whisper.to(device=self.device, dtype=self.weight_dtype).eval()
        self.whisper.requires_grad_(False)
        
        # æ—¶é—´æˆ³
        self.timesteps = torch.tensor([0], device=self.device)
        
        print(f"âœ… è¶…é«˜é€Ÿæ¨ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def load_template_cache(self, template_id):
        """
        åŠ è½½æ¨¡æ¿é¢„å¤„ç†ç¼“å­˜
        
        Args:
            template_id: æ¨¡æ¿ID
            
        Returns:
            tuple: (é¢„å¤„ç†æ•°æ®, å…ƒæ•°æ®)
        """
        print(f"ğŸ“¦ åŠ è½½æ¨¡æ¿ç¼“å­˜: {template_id}")
        
        try:
            data, metadata = self.preprocessor.load_preprocessed_template(template_id)
            print(f"âœ… ç¼“å­˜åŠ è½½æˆåŠŸ")
            print(f"ğŸ“Š å¸§æ•°: {len(data['frame_list_cycle'])}")
            print(f"ğŸ“ åŸå§‹bbox: {metadata['bbox']}")
            return data, metadata
        
        except FileNotFoundError:
            print(f"âŒ æ¨¡æ¿ç¼“å­˜ä¸å­˜åœ¨: {template_id}")
            print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œé¢„å¤„ç†åˆ›å»ºç¼“å­˜")
            raise
    
    def extract_audio_features(self, audio_path, fps=25):
        """
        æå–éŸ³é¢‘ç‰¹å¾
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            fps: å¸§ç‡
            
        Returns:
            list: WhisperéŸ³é¢‘å—
        """
        print(f"ğŸµ æå–éŸ³é¢‘ç‰¹å¾: {audio_path}")
        start_time = time.time()
        
        # æå–éŸ³é¢‘ç‰¹å¾
        whisper_input_features, librosa_length = self.audio_processor.get_audio_feature(
            audio_path, weight_dtype=self.weight_dtype
        )
        
        # ç”ŸæˆWhisperéŸ³é¢‘å—
        whisper_chunks = self.audio_processor.get_whisper_chunk(
            whisper_input_features,
            self.device,
            self.weight_dtype,
            self.whisper,
            librosa_length,
            fps=fps,
            audio_padding_length_left=2,
            audio_padding_length_right=2,
        )
        
        extract_time = time.time() - start_time
        print(f"âœ… éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ")
        print(f"â±ï¸ æå–è€—æ—¶: {extract_time:.3f}ç§’")
        print(f"ğŸ“Š éŸ³é¢‘å—æ•°: {len(whisper_chunks)}")
        
        return whisper_chunks
    
    def ultra_fast_inference(self, 
                           template_id, 
                           audio_path, 
                           output_path,
                           fps=25,
                           save_frames=True):
        """
        è¶…é«˜é€Ÿæ¨ç†ä¸»å‡½æ•°
        
        Args:
            template_id: æ¨¡æ¿ID
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: å¸§ç‡
            save_frames: æ˜¯å¦ä¿å­˜ä¸­é—´å¸§
            
        Returns:
            str: è¾“å‡ºè§†é¢‘è·¯å¾„
        """
        print(f"ğŸš€ å¼€å§‹è¶…é«˜é€Ÿæ¨ç†")
        print(f"ğŸ­ æ¨¡æ¿: {template_id}")
        print(f"ğŸµ éŸ³é¢‘: {audio_path}")
        print(f"ğŸ“¹ è¾“å‡º: {output_path}")
        
        total_start_time = time.time()
        
        # 1. åŠ è½½é¢„å¤„ç†ç¼“å­˜
        template_data, metadata = self.load_template_cache(template_id)
        
        # 2. æå–éŸ³é¢‘ç‰¹å¾
        whisper_chunks = self.extract_audio_features(audio_path, fps)
        video_frames_count = len(whisper_chunks)
        
        # 3. å‡†å¤‡æ‰¹å¤„ç†æ•°æ®ç”Ÿæˆå™¨
        def batch_generator():
            """æ‰¹å¤„ç†æ•°æ®ç”Ÿæˆå™¨"""
            for i in range(0, len(whisper_chunks), self.batch_size):
                batch_end = min(i + self.batch_size, len(whisper_chunks))
                
                # éŸ³é¢‘æ‰¹æ¬¡
                audio_batch = whisper_chunks[i:batch_end]
                
                # æ½œåœ¨ç¼–ç æ‰¹æ¬¡ï¼ˆå¾ªç¯ä½¿ç”¨ï¼‰
                latent_batch = []
                for j in range(len(audio_batch)):
                    frame_idx = (i + j) % len(template_data['input_latent_list_cycle'])
                    latent = template_data['input_latent_list_cycle'][frame_idx]
                    latent_batch.append(latent)
                
                yield torch.stack(audio_batch), torch.stack(latent_batch), i
        
        # 4. è¶…é«˜é€Ÿæ‰¹å¤„ç†æ¨ç†
        print(f"âš¡ å¼€å§‹æ‰¹å¤„ç†æ¨ç†...")
        inference_start_time = time.time()
        
        results = {}  # å­˜å‚¨æ¨ç†ç»“æœ
        
        with torch.no_grad():
            for audio_batch, latent_batch, start_idx in tqdm(
                batch_generator(), 
                desc="æ¨ç†è¿›åº¦",
                total=(len(whisper_chunks) + self.batch_size - 1) // self.batch_size
            ):
                # éŸ³é¢‘ç‰¹å¾ç¼–ç 
                audio_feature_batch = self.pe(audio_batch.to(self.device))
                
                # æ½œåœ¨ç¼–ç å‡†å¤‡
                latent_batch = latent_batch.to(device=self.device, dtype=self.unet.model.dtype)
                
                # UNetæ¨ç†
                pred_latents = self.unet.model(
                    latent_batch,
                    self.timesteps,
                    encoder_hidden_states=audio_feature_batch
                ).sample
                
                # VAEè§£ç 
                pred_latents = pred_latents.to(device=self.device, dtype=self.vae.vae.dtype)
                recon_frames = self.vae.decode_latents(pred_latents)
                
                # å­˜å‚¨ç»“æœ
                for i, frame in enumerate(recon_frames):
                    results[start_idx + i] = frame.cpu().numpy()
        
        inference_time = time.time() - inference_start_time
        print(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.3f}ç§’")
        print(f"âš¡ æ¨ç†é€Ÿåº¦: {len(whisper_chunks) / inference_time:.1f} FPS")
        
        # 5. å›¾åƒèåˆå’Œè§†é¢‘åˆæˆ
        if save_frames:
            print(f"ğŸ¬ å¼€å§‹å›¾åƒèåˆ...")
            blending_start_time = time.time()
            
            final_frames = []
            for i in tqdm(range(video_frames_count), desc="èåˆè¿›åº¦"):
                if i not in results:
                    continue
                
                # è·å–æ¨ç†ç»“æœ
                res_frame = results[i]
                
                # è·å–æ¨¡æ¿æ•°æ®ï¼ˆå¾ªç¯ä½¿ç”¨ï¼‰
                frame_idx = i % len(template_data['frame_list_cycle'])
                ori_frame = template_data['frame_list_cycle'][frame_idx].copy()
                bbox = template_data['coord_list_cycle'][frame_idx]
                mask = template_data['mask_list_cycle'][frame_idx]
                mask_crop_box = template_data['mask_coords_list_cycle'][frame_idx]
                
                # è°ƒæ•´å¤§å°å¹¶èåˆ
                x1, y1, x2, y2 = bbox
                res_frame_resized = cv2.resize(res_frame.astype(np.uint8), (x2 - x1, y2 - y1))
                
                # å›¾åƒèåˆ
                combine_frame = get_image_blending(ori_frame, res_frame_resized, bbox, mask, mask_crop_box)
                final_frames.append(combine_frame)
            
            blending_time = time.time() - blending_start_time
            print(f"âœ… å›¾åƒèåˆå®Œæˆï¼Œè€—æ—¶: {blending_time:.3f}ç§’")
            
            # 6. ä¿å­˜è§†é¢‘
            self._save_video(final_frames, audio_path, output_path, fps)
        
        total_time = time.time() - total_start_time
        print(f"ğŸ‰ è¶…é«˜é€Ÿæ¨ç†å®Œæˆ")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {video_frames_count / total_time:.1f} FPS")
        print(f"ğŸ“¹ è¾“å‡º: {output_path}")
        
        return output_path
    
    def _save_video(self, frames, audio_path, output_path, fps):
        """ä¿å­˜è§†é¢‘"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯ä»¥ä¿å­˜")
        
        print(f"ğŸ’¾ ä¿å­˜è§†é¢‘: {output_path}")
        
        h, w, c = frames[0].shape
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        temp_video = output_path.replace('.mp4', '_temp.mp4')
        
        # å†™å…¥è§†é¢‘å¸§
        out = cv2.VideoWriter(temp_video, fourcc, fps, (w, h))
        for frame in frames:
            out.write(frame)
        out.release()
        
        # åˆå¹¶éŸ³é¢‘
        cmd = (f'ffmpeg -y -i "{temp_video}" -i "{audio_path}" '
               f'-c:v copy -c:a aac -strict experimental "{output_path}" -loglevel quiet')
        os.system(cmd)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_video):
            os.remove(temp_video)
        
        print(f"âœ… è§†é¢‘ä¿å­˜å®Œæˆ")
    
    def benchmark_performance(self, template_id, audio_path, runs=3):
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            template_id: æ¨¡æ¿ID
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            runs: æµ‹è¯•æ¬¡æ•°
        """
        print(f"ğŸƒ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"ğŸ­ æ¨¡æ¿: {template_id}")
        print(f"ğŸµ éŸ³é¢‘: {audio_path}")
        print(f"ğŸ”„ æµ‹è¯•æ¬¡æ•°: {runs}")
        
        times = []
        
        for run in range(runs):
            print(f"\n--- ç¬¬ {run + 1} æ¬¡æµ‹è¯• ---")
            
            start_time = time.time()
            
            # åŠ è½½ç¼“å­˜
            template_data, metadata = self.load_template_cache(template_id)
            
            # æå–éŸ³é¢‘ç‰¹å¾
            whisper_chunks = self.extract_audio_features(audio_path)
            
            # åªåšæ¨ç†ï¼Œä¸ä¿å­˜è§†é¢‘
            with torch.no_grad():
                for i in range(0, len(whisper_chunks), self.batch_size):
                    batch_end = min(i + self.batch_size, len(whisper_chunks))
                    audio_batch = whisper_chunks[i:batch_end]
                    
                    latent_batch = []
                    for j in range(len(audio_batch)):
                        frame_idx = (i + j) % len(template_data['input_latent_list_cycle'])
                        latent = template_data['input_latent_list_cycle'][frame_idx]
                        latent_batch.append(latent)
                    
                    audio_batch = torch.stack(audio_batch)
                    latent_batch = torch.stack(latent_batch)
                    
                    # æ¨ç†
                    audio_feature_batch = self.pe(audio_batch.to(self.device))
                    latent_batch = latent_batch.to(device=self.device, dtype=self.unet.model.dtype)
                    
                    pred_latents = self.unet.model(
                        latent_batch, self.timesteps, encoder_hidden_states=audio_feature_batch
                    ).sample
                    
                    pred_latents = pred_latents.to(device=self.device, dtype=self.vae.vae.dtype)
                    recon_frames = self.vae.decode_latents(pred_latents)
            
            run_time = time.time() - start_time
            fps = len(whisper_chunks) / run_time
            times.append(run_time)
            
            print(f"â±ï¸ è€—æ—¶: {run_time:.3f}ç§’")
            print(f"âš¡ é€Ÿåº¦: {fps:.1f} FPS")
        
        # ç»Ÿè®¡ç»“æœ
        avg_time = np.mean(times)
        avg_fps = len(whisper_chunks) / avg_time
        
        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ ({runs} æ¬¡å¹³å‡):")
        print(f"â±ï¸ å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’")
        print(f"âš¡ å¹³å‡é€Ÿåº¦: {avg_fps:.1f} FPS")
        print(f"ğŸ“ˆ æœ€å¿«é€Ÿåº¦: {len(whisper_chunks) / min(times):.1f} FPS")


def main():
    """å‘½ä»¤è¡Œç•Œé¢"""
    parser = argparse.ArgumentParser(description="è¶…é«˜é€Ÿå®æ—¶MuseTalkæ¨ç†")
    parser.add_argument("--template_id", required=True, help="æ¨¡æ¿ID")
    parser.add_argument("--audio_path", required=True, help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_path", required=True, help="è¾“å‡ºè§†é¢‘è·¯å¾„")
    parser.add_argument("--cache_dir", default="./template_cache", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--device", default="cuda:0", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--fps", type=int, default=25, help="è§†é¢‘å¸§ç‡")
    parser.add_argument("--fp16", action="store_true", default=True, help="ä½¿ç”¨åŠç²¾åº¦")
    parser.add_argument("--benchmark", action="store_true", help="è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument("--benchmark_runs", type=int, default=3, help="åŸºå‡†æµ‹è¯•æ¬¡æ•°")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†ç³»ç»Ÿ
    inference_system = UltraFastRealtimeInference(
        device=args.device,
        cache_dir=args.cache_dir,
        batch_size=args.batch_size,
        fp16=args.fp16
    )
    
    if args.benchmark:
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        inference_system.benchmark_performance(
            args.template_id, 
            args.audio_path, 
            args.benchmark_runs
        )
    else:
        # æ­£å¸¸æ¨ç†
        inference_system.ultra_fast_inference(
            template_id=args.template_id,
            audio_path=args.audio_path,
            output_path=args.output_path,
            fps=args.fps,
            save_frames=True
        )


if __name__ == "__main__":
    main()
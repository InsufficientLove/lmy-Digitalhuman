#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨å±€æŒä¹…åŒ–MuseTalkæœåŠ¡
åŸºäºå®˜æ–¹MuseTalkæ¶æ„ï¼Œå¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼Œé€šè¿‡IPCé€šä¿¡å®ç°çœŸæ­£çš„å®æ—¶æ¨ç†
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import argparse
import time
import threading
import queue
import subprocess
import shutil
import socket
import struct
from pathlib import Path
from tqdm import tqdm
import copy
from transformers import WhisperModel
from moviepy.editor import VideoFileClip, AudioFileClip
import imageio

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'MuseTalk'))

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image, get_image_prepare_material, get_image_blending
from musetalk.utils.audio_processor import AudioProcessor

class GlobalMuseTalkService:
    """å…¨å±€æŒä¹…åŒ–MuseTalkæœåŠ¡ - çœŸæ­£çš„å•ä¾‹æ¨¡å¼"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        self.device = None
        self.vae = None
        self.unet = None
        self.pe = None
        self.whisper = None
        self.audio_processor = None
        self.fp = None
        self.weight_dtype = None
        self.timesteps = None
        self.is_initialized = False
        self.inference_lock = threading.Lock()
        
        # é…ç½®å‚æ•° - åŸºäºå®˜æ–¹MuseTalk
        self.unet_model_path = "./models/musetalk/pytorch_model.bin"
        self.unet_config = "./models/musetalk/musetalk.json"
        self.vae_type = "sd-vae"
        self.whisper_dir = "./models/whisper"
        
        # IPCé€šä¿¡
        self.server_socket = None
        self.is_server_running = False
        
        self._initialized = True
        print("ğŸš€ å…¨å±€MuseTalkæœåŠ¡å®ä¾‹å·²åˆ›å»º")
    
    def initialize_models_once(self, gpu_id=0):
        """å…¨å±€åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼ˆæ•´ä¸ªç¨‹åºç”Ÿå‘½å‘¨æœŸåªæ‰§è¡Œä¸€æ¬¡ï¼‰"""
        if self.is_initialized:
            print("âœ… æ¨¡å‹å·²å…¨å±€åˆå§‹åŒ–ï¼Œç›´æ¥å¤ç”¨")
            return True
            
        try:
            print(f"ğŸ”§ å…¨å±€åˆå§‹åŒ–MuseTalkæ¨¡å‹ (GPU:{gpu_id})...")
            start_time = time.time()
            
            # è®¾ç½®è®¾å¤‡
            self.device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")
            print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {self.device}")
            
            # ğŸš€ åŸºäºå®˜æ–¹MuseTalkæ¶æ„åŠ è½½æ¨¡å‹
            print("ğŸ“¦ åŠ è½½VAE, UNet, PEæ¨¡å‹...")
            self.vae, self.unet, self.pe = load_all_model(
                unet_model_path=self.unet_model_path,
                vae_type=self.vae_type,
                unet_config=self.unet_config,
                device=self.device
            )
            
            # ğŸ”§ å®˜æ–¹ä¼˜åŒ–ï¼šä½¿ç”¨halfç²¾åº¦æå‡æ€§èƒ½
            self.weight_dtype = torch.float16
            self.pe = self.pe.half().to(self.device)
            self.vae.vae = self.vae.vae.half().to(self.device)
            self.unet.model = self.unet.model.half().to(self.device)
            
            self.timesteps = torch.tensor([0], device=self.device)
            
            # åŠ è½½Whisperæ¨¡å‹
            print("ğŸµ åŠ è½½Whisperæ¨¡å‹...")
            self.audio_processor = AudioProcessor(feature_extractor_path=self.whisper_dir)
            self.whisper = WhisperModel.from_pretrained(self.whisper_dir)
            self.whisper = self.whisper.to(device=self.device, dtype=self.weight_dtype).eval()
            self.whisper.requires_grad_(False)
            
            # åˆå§‹åŒ–é¢éƒ¨è§£æå™¨
            print("ğŸ‘¤ åˆå§‹åŒ–é¢éƒ¨è§£æå™¨...")
            self.fp = FaceParsing()
            
            self.is_initialized = True
            init_time = time.time() - start_time
            print(f"âœ… å…¨å±€æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}ç§’")
            print("ğŸ‰ æ¨¡å‹å·²åŠ è½½åˆ°GPUå†…å­˜ï¼Œåç»­æ¨ç†å°†æé€Ÿæ‰§è¡Œ")
            return True
            
        except Exception as e:
            print(f"âŒ å…¨å±€æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def load_template_cache(self, cache_dir, template_id):
        """åŠ è½½æ¨¡æ¿ç¼“å­˜"""
        try:
            # åŠ è½½é¢„å¤„ç†ç¼“å­˜
            cache_file = os.path.join(cache_dir, f"{template_id}_preprocessed.pkl")
            metadata_file = os.path.join(cache_dir, f"{template_id}_metadata.json")
            
            if not os.path.exists(cache_file):
                raise FileNotFoundError(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
            if not os.path.exists(metadata_file):
                raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            
            # åŠ è½½ç¼“å­˜æ•°æ®
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # æå–æ•°æ®
            input_latent_list_cycle = cache_data['input_latent_list_cycle']
            coord_list_cycle = cache_data['coord_list_cycle']
            frame_list_cycle = cache_data['frame_list_cycle']
            mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
            mask_list_cycle = cache_data['mask_list_cycle']
            
            print(f"âœ… æ¨¡æ¿ç¼“å­˜åŠ è½½æˆåŠŸ: {template_id}")
            print(f"   - æ½œåœ¨å‘é‡: {len(input_latent_list_cycle)} å¸§")
            print(f"   - é¢éƒ¨åæ ‡: {len(coord_list_cycle)} å¸§")
            
            return {
                'input_latent_list_cycle': input_latent_list_cycle,
                'coord_list_cycle': coord_list_cycle,
                'frame_list_cycle': frame_list_cycle,
                'mask_coords_list_cycle': mask_coords_list_cycle,
                'mask_list_cycle': mask_list_cycle,
                'metadata': metadata
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡æ¿ç¼“å­˜å¤±è´¥: {str(e)}")
            return None
    
    def ultra_fast_inference(self, template_id, audio_path, output_path, cache_dir, batch_size=8, fps=25):
        """è¶…å¿«é€Ÿæ¨ç† - å¤ç”¨å…¨å±€æ¨¡å‹ï¼Œæ— éœ€é‡å¤åŠ è½½"""
        if not self.is_initialized:
            print("âŒ å…¨å±€æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
        
        # ğŸ”’ æ¨ç†é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
        with self.inference_lock:
            try:
                start_time = time.time()
                print(f"âš¡ å¼€å§‹è¶…å¿«é€Ÿæ¨ç†: {template_id}")
                
                # 1. åŠ è½½æ¨¡æ¿ç¼“å­˜
                cache_data = self.load_template_cache(cache_dir, template_id)
                if not cache_data:
                    return False
                
                input_latent_list_cycle = cache_data['input_latent_list_cycle']
                coord_list_cycle = cache_data['coord_list_cycle']
                frame_list_cycle = cache_data['frame_list_cycle']
                
                # 2. éŸ³é¢‘ç‰¹å¾æå–
                print("ğŸµ æå–éŸ³é¢‘ç‰¹å¾...")
                audio_start = time.time()
                whisper_input_features, librosa_length = self.audio_processor.get_audio_feature(audio_path)
                whisper_chunks = self.audio_processor.get_whisper_chunk(
                    whisper_input_features, 
                    self.device, 
                    self.weight_dtype, 
                    self.whisper, 
                    librosa_length,
                    fps=fps,
                    audio_padding_length_left=2,
                    audio_padding_length_right=2,
                )
                audio_time = time.time() - audio_start
                print(f"âœ… éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {audio_time:.2f}ç§’, éŸ³é¢‘å—æ•°: {len(whisper_chunks)}")
                
                # 3. æ‰¹é‡æ¨ç† - ğŸš€ å¤ç”¨å…¨å±€æ¨¡å‹ï¼Œæé€Ÿæ‰§è¡Œ
                print("âš¡ å¼€å§‹æ‰¹é‡æ¨ç†...")
                inference_start = time.time()
                video_num = len(whisper_chunks)
                gen = datagen(
                    whisper_chunks=whisper_chunks,
                    vae_encode_latents=input_latent_list_cycle,
                    batch_size=batch_size,
                    delay_frame=0,
                    device=self.device,
                )
                
                res_frame_list = []
                for i, (whisper_batch, latent_batch) in enumerate(tqdm(gen, total=int(np.ceil(float(video_num)/batch_size)), desc="æ¨ç†è¿›åº¦")):
                    audio_feature_batch = self.pe(whisper_batch)
                    latent_batch = latent_batch.to(dtype=self.weight_dtype)
                    
                    # ğŸ”¥ æ ¸å¿ƒæ¨ç† - å¤ç”¨å…¨å±€æ¨¡å‹
                    pred_latents = self.unet.model(latent_batch, self.timesteps, encoder_hidden_states=audio_feature_batch).sample
                    recon = self.vae.decode_latents(pred_latents)
                    for res_frame in recon:
                        res_frame_list.append(res_frame)
                
                inference_time = time.time() - inference_start
                print(f"âœ… æ¨ç†å®Œæˆ: {len(res_frame_list)} å¸§, è€—æ—¶: {inference_time:.2f}ç§’")
                
                # 4. å›¾åƒåˆæˆ - ğŸ¨ ä½¿ç”¨å®˜æ–¹get_imageæ–¹æ³•é¿å…é˜´å½±
                print("ğŸ–¼ï¸ åˆæˆå®Œæ•´å›¾åƒ...")
                compose_start = time.time()
                
                # åˆ›å»ºä¸´æ—¶å¸§ç›®å½•
                temp_frames_dir = os.path.join(os.path.dirname(output_path), "temp_frames")
                os.makedirs(temp_frames_dir, exist_ok=True)
                
                for i, res_frame in enumerate(tqdm(res_frame_list, desc="åˆæˆå›¾åƒ")):
                    bbox = coord_list_cycle[i % len(coord_list_cycle)]
                    ori_frame = copy.deepcopy(frame_list_cycle[i % len(frame_list_cycle)])
                    
                    x1, y1, x2, y2 = bbox
                    try:
                        res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                    except:
                        continue
                    
                    # ğŸ¨ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å®˜æ–¹get_imageæ–¹æ³•ï¼Œé¿å…é˜´å½±
                    combine_frame = get_image(
                        image=ori_frame, 
                        face=res_frame, 
                        face_box=[x1, y1, x2, y2], 
                        upper_boundary_ratio=0.5, 
                        expand=1.5, 
                        mode='jaw', 
                        fp=self.fp
                    )
                    
                    # ä¿å­˜å¸§
                    frame_path = os.path.join(temp_frames_dir, f"{i:08d}.png")
                    cv2.imwrite(frame_path, combine_frame)
                
                compose_time = time.time() - compose_start
                print(f"âœ… å›¾åƒåˆæˆå®Œæˆ: è€—æ—¶: {compose_time:.2f}ç§’")
                
                # 5. ç”Ÿæˆè§†é¢‘
                print("ğŸ¬ ç”Ÿæˆè§†é¢‘...")
                video_start = time.time()
                temp_video = output_path.replace('.mp4', '_temp.mp4')
                
                ffmpeg_cmd = [
                    'ffmpeg', '-y', '-v', 'warning',
                    '-r', str(fps),
                    '-f', 'image2',
                    '-i', os.path.join(temp_frames_dir, '%08d.png'),
                    '-vcodec', 'libx264',
                    '-vf', 'format=yuv420p',
                    '-crf', '18',
                    temp_video
                ]
                
                subprocess.run(ffmpeg_cmd, check=True)
                video_time = time.time() - video_start
                print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: è€—æ—¶: {video_time:.2f}ç§’")
                
                # 6. åˆæˆéŸ³é¢‘
                print("ğŸ”Š åˆæˆéŸ³é¢‘...")
                audio_merge_start = time.time()
                
                try:
                    video_clip = VideoFileClip(temp_video)
                    audio_clip = AudioFileClip(audio_path)
                    video_clip = video_clip.set_audio(audio_clip)
                    video_clip.write_videofile(output_path, codec='libx264', audio_codec='aac', fps=fps, verbose=False, logger=None)
                    video_clip.close()
                    audio_clip.close()
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.remove(temp_video)
                    shutil.rmtree(temp_frames_dir)
                    
                except Exception as e:
                    print(f"âš ï¸ éŸ³é¢‘åˆæˆå¤±è´¥ï¼Œä½¿ç”¨æ— éŸ³é¢‘ç‰ˆæœ¬: {str(e)}")
                    shutil.move(temp_video, output_path)
                
                audio_merge_time = time.time() - audio_merge_start
                print(f"âœ… éŸ³é¢‘åˆæˆå®Œæˆ: è€—æ—¶: {audio_merge_time:.2f}ç§’")
                
                total_time = time.time() - start_time
                print(f"ğŸ‰ è¶…å¿«é€Ÿæ¨ç†å®Œæˆ: {output_path}")
                print(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.2f}ç§’ (éŸ³é¢‘:{audio_time:.1f}s + æ¨ç†:{inference_time:.1f}s + åˆæˆ:{compose_time:.1f}s + è§†é¢‘:{video_time:.1f}s + éŸ³é¢‘:{audio_merge_time:.1f}s)")
                
                return True
                
            except Exception as e:
                print(f"âŒ è¶…å¿«é€Ÿæ¨ç†å¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                return False
    
    def start_ipc_server(self, port=9999):
        """å¯åŠ¨IPCæœåŠ¡å™¨ï¼Œæ¥æ”¶æ¨ç†è¯·æ±‚"""
        try:
            self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self.server_socket.bind(('localhost', port))
            self.server_socket.listen(5)
            self.is_server_running = True
            
            print(f"ğŸŒ IPCæœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼Œç›‘å¬ç«¯å£: {port}")
            print("ğŸ“¡ ç­‰å¾…C#å®¢æˆ·ç«¯è¿æ¥...")
            
            while self.is_server_running:
                try:
                    client_socket, addr = self.server_socket.accept()
                    print(f"ğŸ”— å®¢æˆ·ç«¯è¿æ¥: {addr}")
                    
                    # å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
                    threading.Thread(target=self._handle_client, args=(client_socket,)).start()
                    
                except Exception as e:
                    if self.is_server_running:
                        print(f"âŒ æ¥å—è¿æ¥å¤±è´¥: {str(e)}")
                    
        except Exception as e:
            print(f"âŒ IPCæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {str(e)}")
    
    def _handle_client(self, client_socket):
        """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚"""
        try:
            # æ¥æ”¶è¯·æ±‚æ•°æ®
            data_length = struct.unpack('I', client_socket.recv(4))[0]
            data = client_socket.recv(data_length).decode('utf-8')
            request = json.loads(data)
            
            print(f"ğŸ“¨ æ”¶åˆ°æ¨ç†è¯·æ±‚: {request['template_id']}")
            
            # æ‰§è¡Œæ¨ç†
            success = self.ultra_fast_inference(
                template_id=request['template_id'],
                audio_path=request['audio_path'],
                output_path=request['output_path'],
                cache_dir=request['cache_dir'],
                batch_size=request.get('batch_size', 8),
                fps=request.get('fps', 25)
            )
            
            # å‘é€å“åº”
            response = {'success': success, 'output_path': request['output_path'] if success else None}
            response_data = json.dumps(response).encode('utf-8')
            client_socket.send(struct.pack('I', len(response_data)))
            client_socket.send(response_data)
            
            print(f"ğŸ“¤ æ¨ç†å“åº”å·²å‘é€: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚å¤±è´¥: {str(e)}")
        finally:
            client_socket.close()
    
    def stop_server(self):
        """åœæ­¢IPCæœåŠ¡å™¨"""
        self.is_server_running = False
        if self.server_socket:
            self.server_socket.close()
        print("ğŸ›‘ IPCæœåŠ¡å™¨å·²åœæ­¢")

# å…¨å±€æœåŠ¡å®ä¾‹
global_service = GlobalMuseTalkService()

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description='å…¨å±€æŒä¹…åŒ–MuseTalkæœåŠ¡')
    parser.add_argument('--mode', choices=['server', 'client'], default='server', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--port', type=int, default=9999, help='IPCç«¯å£')
    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID')
    
    # å®¢æˆ·ç«¯æ¨¡å¼å‚æ•°
    parser.add_argument('--template_id', type=str, help='æ¨¡æ¿ID')
    parser.add_argument('--audio_path', type=str, help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_path', type=str, help='è¾“å‡ºè§†é¢‘è·¯å¾„')
    parser.add_argument('--cache_dir', type=str, help='ç¼“å­˜ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--fps', type=int, default=25, help='è§†é¢‘å¸§ç‡')
    
    args = parser.parse_args()
    
    if args.mode == 'server':
        # æœåŠ¡å™¨æ¨¡å¼ï¼šå¯åŠ¨æ—¶åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼Œç„¶åç›‘å¬è¯·æ±‚
        print("ğŸš€ å¯åŠ¨å…¨å±€MuseTalkæœåŠ¡å™¨...")
        
        # å…¨å±€åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
        if not global_service.initialize_models_once(args.gpu_id):
            print("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            sys.exit(1)
        
        # å¯åŠ¨IPCæœåŠ¡å™¨
        try:
            global_service.start_ipc_server(args.port)
        except KeyboardInterrupt:
            print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·")
            global_service.stop_server()
            
    else:
        # å®¢æˆ·ç«¯æ¨¡å¼ï¼šç›´æ¥æ‰§è¡Œæ¨ç†
        if not all([args.template_id, args.audio_path, args.output_path, args.cache_dir]):
            print("âŒ å®¢æˆ·ç«¯æ¨¡å¼éœ€è¦æä¾›æ‰€æœ‰æ¨ç†å‚æ•°")
            sys.exit(1)
        
        # åˆå§‹åŒ–æ¨¡å‹
        if not global_service.initialize_models_once(args.gpu_id):
            print("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            sys.exit(1)
        
        # æ‰§è¡Œæ¨ç†
        success = global_service.ultra_fast_inference(
            template_id=args.template_id,
            audio_path=args.audio_path,
            output_path=args.output_path,
            cache_dir=args.cache_dir,
            batch_size=args.batch_size,
            fps=args.fps
        )
        
        sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
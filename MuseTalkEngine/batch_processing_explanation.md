# 批次处理和GPU并行解释

## 什么是批次处理？

想象你要处理100张照片：

### 方式1：逐个处理（无批次）
- 处理第1张照片 → 处理第2张照片 → ... → 处理第100张照片
- 耗时：100次处理

### 方式2：批次处理（batch_size=4）
- 批次1：同时处理第1-4张照片
- 批次2：同时处理第5-8张照片
- ...
- 批次25：同时处理第97-100张照片
- 耗时：25次处理（理论上快4倍）

## 4GPU并行是什么意思？

不是每个GPU做相同的事，而是分工合作：

```
有20帧视频，batch_size=1，则有20个批次：

GPU0: 处理批次0,4,8,12,16  （第1,5,9,13,17帧）
GPU1: 处理批次1,5,9,13,17  （第2,6,10,14,18帧）
GPU2: 处理批次2,6,10,14,18 （第3,7,11,15,19帧）
GPU3: 处理批次3,7,11,15,19 （第4,8,12,16,20帧）
```

## 为什么会内存不足？

### 问题1：批次太大
- batch_size=16 意味着同时处理16帧
- 每帧都需要大量内存
- 16帧同时处理 = 16倍内存需求

### 问题2：数据维度
- 您的latent形状：[1, 8, 247, 164]
- 批次处理时会变成：[16, 8, 247, 164]（如果batch_size=16）
- 这就是为什么会尝试分配48.90GB

## 解决方案

1. **降低batch_size**：从16→4→1
2. **限制并发**：每个GPU同时只处理1个批次
3. **内存清理**：每个批次后清理内存

## 性能影响

- batch_size=16：最快但需要大量内存
- batch_size=4：平衡性能和内存
- batch_size=1：最慢但最省内存

## 推理时间解释

"推理40秒"包含：
1. 数据准备
2. 所有批次的处理时间
3. 等待所有GPU完成
4. 由于内存错误，实际没有生成任何帧，但仍然耗费了时间尝试
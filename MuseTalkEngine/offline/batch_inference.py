#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultra Fast Realtime Inference V2
æè‡´ä¼˜åŒ–ç‰ˆæœ¬ - ç›®æ ‡ï¼šæ¯«ç§’çº§å“åº”ï¼Œ4GPUçœŸå¹¶è¡Œï¼Œé›¶ç­‰å¾…
"""

import os
import sys
import json
import pickle
import torch
import cv2
import numpy as np
import time
import gc
import threading
import queue
import socket
import struct
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import copy
import gc
try:
    from transformers import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: transformers.WhisperModelä¸å¯ç”¨ï¼Œå°†è·³è¿‡Whisperåˆå§‹åŒ–")
    WHISPER_AVAILABLE = False
import imageio
import warnings
warnings.filterwarnings("ignore")

# GPUé…ç½® - ç›´æ¥å®šä¹‰ï¼Œä¸ä»å¤–éƒ¨å¯¼å…¥
GPU_MEMORY_CONFIG = {'batch_size': {'default': 4}}
print("ä½¿ç”¨é»˜è®¤GPUé…ç½®")

# æ·»åŠ MuseTalkæ¨¡å—è·¯å¾„
sys.path.append('/opt/musetalk/repo/MuseTalk')

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image, get_image_blending, get_image_prepare_material
from musetalk.utils.audio_processor import AudioProcessor

# æ€§èƒ½ç›‘æ§ - å·²ç§»é™¤ï¼Œä½¿ç”¨ç®€å•çš„æ—¶é—´è®°å½•
PERFORMANCE_MONITORING = False
print("æ€§èƒ½ç›‘æ§å·²ç¦ç”¨")

print("Ultra Fast Realtime Inference V2 - æ¯«ç§’çº§å“åº”å¼•æ“")
sys.stdout.flush()

class UltraFastMuseTalkService:
    """æè‡´ä¼˜åŒ–çš„MuseTalkæœåŠ¡ - æ¯«ç§’çº§å“åº”"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        # GPUæ¶æ„ - è‡ªåŠ¨é€‚é…å•GPUæˆ–å¤šGPU
        self.gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        if self.gpu_count == 0:
            print("âŒ æœªæ£€æµ‹åˆ°GPU")
            self.devices = []
        else:
            self.devices = [f'cuda:{i}' for i in range(self.gpu_count)]
            print(f"ğŸ® æ£€æµ‹åˆ° {self.gpu_count} ä¸ªGPU")
        
        # æ¯ä¸ªGPUç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹
        self.gpu_models = {}
        self.gpu_locks = {device: threading.Lock() for device in self.devices}
        
        # å…¨å±€æ¨¡å‹ç»„ä»¶ï¼ˆå…±äº«æƒé‡ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        self.shared_vae = None
        self.shared_unet = None
        self.shared_pe = None
        self.shared_whisper = None
        self.shared_audio_processor = None
        self.shared_fp = None
        self.weight_dtype = torch.float16  # ä½¿ç”¨åŠç²¾åº¦æé€Ÿ
        self.timesteps = None
        
        # å†…å­˜æ± å’Œç¼“å­˜ä¼˜åŒ–
        self.template_cache = {}
        self.audio_feature_cache = {}
        self.frame_buffer_pool = queue.Queue(maxsize=1000)
        
        # æé€Ÿå¤„ç†ç®¡é“
        self.inference_executor = ThreadPoolExecutor(max_workers=self.gpu_count)
        self.compose_executor = ThreadPoolExecutor(max_workers=32)  # 32çº¿ç¨‹å¹¶è¡Œåˆæˆ
        self.video_executor = ThreadPoolExecutor(max_workers=4)
        
        # ğŸ® GPUè´Ÿè½½å‡è¡¡
        self.gpu_usage = {device: 0 for device in self.devices}
        self.gpu_queue = {device: queue.Queue(maxsize=10) for device in self.devices}
        
        self.is_initialized = False
        self._initialized = True
        
        # è·å–ç»Ÿä¸€çš„æ¨¡æ¿ç¼“å­˜ç›®å½•
        self.template_cache_dir = os.environ.get('MUSE_TEMPLATE_CACHE_DIR', '/opt/musetalk/template_cache')
        print(f"ä½¿ç”¨æ¨¡æ¿ç¼“å­˜ç›®å½•: {self.template_cache_dir}")
        
        print(f"Ultra Fast Service åˆå§‹åŒ–å®Œæˆ - {self.gpu_count}GPUå¹¶è¡Œæ¶æ„")
        sys.stdout.flush()
    
    def initialize_models_ultra_fast(self):
        """æé€Ÿåˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ - å¹¶è¡ŒåŠ è½½åˆ°æ‰€æœ‰GPU"""
        if self.is_initialized and len(self.gpu_models) > 0:
            print("æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return True
        
        # é‡ç½®åˆå§‹åŒ–çŠ¶æ€ï¼Œå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        self.is_initialized = False
        self.gpu_models = {}
            
        try:
            print(f"å¼€å§‹æé€Ÿåˆå§‹åŒ– - {self.gpu_count}GPUå¹¶è¡ŒåŠ è½½...")
            start_time = time.time()
            
            # å¹¶è¡Œåˆå§‹åŒ–æ‰€æœ‰GPUæ¨¡å‹
            def init_gpu_model(device_id):
                import os  # Fix: import os at function start
                import platform
                import copy
                device = f'cuda:{device_id}'
                print(f"ğŸ® GPU{device_id} å¼€å§‹åˆå§‹åŒ–...")
                
                with torch.cuda.device(device_id):
                    # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šGPU - åªä½¿ç”¨å¯ç”¨çš„sd-vae
                    try:
                        print(f"GPU{device_id} å¼€å§‹åŠ è½½æ¨¡å‹...")
                        
                        # è®¾ç½®æ¨¡å‹è·¯å¾„ç¯å¢ƒå˜é‡
                        os.environ['MUSETALK_MODEL_PATH'] = '/opt/musetalk/models'
                        
                        # æ£€æŸ¥sd-vaeç›®å½•æ˜¯å¦å®Œæ•´
                        sd_vae_path = "/opt/musetalk/models/sd-vae"
                        config_file = os.path.join(sd_vae_path, "config.json")
                        
                        if os.path.exists(config_file):
                            print(f"GPU{device_id} ä½¿ç”¨å®Œæ•´çš„sd-vaeæ¨¡å‹")
                            # ç›´æ¥åŠ è½½æ¨¡å‹ï¼Œä¸æŒ‡å®švae_typeå‚æ•°
                            vae, unet, pe = load_all_model()
                            print(f"GPU{device_id} æ¨¡å‹åŠ è½½æˆåŠŸ")
                        else:
                            print(f"GPU{device_id} sd-vaeé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                            # å°è¯•ç›´æ¥åŠ è½½æ¨¡å‹
                            print(f"GPU{device_id} å°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„...")
                            vae, unet, pe = load_all_model()
                            print(f"GPU{device_id} æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰")
                            
                    except Exception as e:
                        print(f"GPU{device_id} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                        # æ£€æŸ¥æ˜¯å¦æ˜¯UNetæ¨¡å‹é—®é¢˜
                        if "meta tensor" in str(e) or "Cannot copy out" in str(e):
                            print(f"GPU{device_id} UNetæ¨¡å‹æ–‡ä»¶å¯èƒ½æŸåï¼Œå°è¯•é‡æ–°åŠ è½½...")
                            try:
                                # å¼ºåˆ¶æ¸…ç†GPUå†…å­˜
                                torch.cuda.empty_cache()
                                # é‡æ–°å°è¯•åŠ è½½
                                vae, unet, pe = load_all_model(vae_type="sd-vae")
                                print(f"GPU{device_id} é‡æ–°åŠ è½½æˆåŠŸ")
                            except Exception as e3:
                                print(f"GPU{device_id} é‡æ–°åŠ è½½ä¹Ÿå¤±è´¥: {e3}")
                                return None
                        else:
                            print(f"GPU{device_id} å…¶ä»–é”™è¯¯ï¼Œè·³è¿‡æ­¤GPU")
                            return None
                    
                    # ä¼˜åŒ–æ¨¡å‹ - åŠç²¾åº¦+ç¼–è¯‘ä¼˜åŒ– (ä¿®å¤æ¨¡å‹å¯¹è±¡å…¼å®¹æ€§)
                    print(f"GPU{device_id} å¼€å§‹æ¨¡å‹ä¼˜åŒ–...")
                    
                    # ä¿®å¤VAEå¯¹è±¡ - ä½¿ç”¨.vaeå±æ€§
                    if hasattr(vae, 'vae'):
                        vae.vae = vae.vae.to(device).half().eval()
                    elif hasattr(vae, 'to'):
                        vae = vae.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: VAEå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    # ä¿®å¤UNetå¯¹è±¡ - ä½¿ç”¨.modelå±æ€§  
                    if hasattr(unet, 'model'):
                        unet.model = unet.model.to(device).half().eval()
                    elif hasattr(unet, 'to'):
                        unet = unet.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: UNetå¯¹è±¡ç»“æ„ä¸æ˜ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    # ä¿®å¤PEå¯¹è±¡
                    if hasattr(pe, 'to'):
                        pe = pe.to(device).half().eval()
                    else:
                        print(f"è­¦å‘Š: PEå¯¹è±¡æ²¡æœ‰.to()æ–¹æ³•ï¼Œè·³è¿‡ä¼˜åŒ–")
                    
                    print(f"GPU{device_id} åŠç²¾åº¦è½¬æ¢å®Œæˆ")
                    
                    # æ™ºèƒ½æ¨¡å‹ç¼–è¯‘ - ä½¿ç”¨æ›´å®‰å…¨çš„ç¼–è¯‘æ¨¡å¼
                    import platform
                    import os
                    
                    # æ£€æŸ¥æ˜¯å¦ç¦ç”¨torch.compile
                    disable_compile = os.environ.get('DISABLE_TORCH_COMPILE', '0') == '1'
                    
                    if disable_compile:
                        print(f"GPU{device_id} torch.compileå·²ç¦ç”¨ï¼ˆDISABLE_TORCH_COMPILE=1ï¼‰")
                    elif hasattr(torch, 'compile') and platform.system() != 'Windows':
                        try:
                            print(f"GPU{device_id} å¼€å§‹æ¨¡å‹ä¼˜åŒ–ç¼–è¯‘...")
                            
                            # ä½¿ç”¨æ›´å®‰å…¨çš„ç¼–è¯‘æ¨¡å¼ï¼Œé¿å…CUDAå›¾é”™è¯¯
                            # modeé€‰é¡¹ï¼š
                            # - "default": å¹³è¡¡æ¨¡å¼
                            # - "reduce-overhead": æœ€æ¿€è¿›ä¼˜åŒ–ï¼ˆå¯èƒ½å¯¼è‡´é”™è¯¯ï¼‰
                            # - "max-autotune": æœ€å¤§æ€§èƒ½ä½†ç¼–è¯‘æ…¢
                            # - "max-autotune-no-cudagraphs": ç¦ç”¨CUDAå›¾ï¼Œé¿å…TLSé”™è¯¯
                            
                            # å°è¯•ä¸åŒçš„ç¼–è¯‘ç­–ç•¥
                            compile_strategies = [
                                # ç­–ç•¥1ï¼šé»˜è®¤æ¨¡å¼ï¼ˆå¿«é€Ÿç¼–è¯‘ï¼Œé€‚ä¸­ä¼˜åŒ–ï¼‰
                                {
                                    "mode": "default",
                                    "fullgraph": False,
                                },
                                # ç­–ç•¥2ï¼šå‡å°‘å¼€é”€ï¼ˆæœ€å¿«ç¼–è¯‘ï¼‰
                                {
                                    "mode": "reduce-overhead",
                                    "fullgraph": False,
                                    "disable_cudagraphs": True,
                                },
                                # ç­–ç•¥3ï¼šæœ€å¤§è°ƒä¼˜ï¼ˆæ…¢ç¼–è¯‘ï¼Œæœ€ä¼˜æ€§èƒ½ï¼‰- å¤‡é€‰
                                {
                                    "mode": "max-autotune-no-cudagraphs",
                                    "fullgraph": False,
                                    "dynamic": True,
                                },
                            ]
                            
                            # å°è¯•æ‰¾åˆ°å¯ç”¨çš„ç¼–è¯‘ç­–ç•¥
                            compile_options = None
                            for idx, strategy in enumerate(compile_strategies):
                                try:
                                    # æµ‹è¯•ç¼–è¯‘ä¸€ä¸ªå°æ¨¡å‹
                                    test_model = torch.nn.Linear(10, 10).to(device)
                                    torch.compile(test_model, **strategy)
                                    compile_options = strategy
                                    print(f"  ä½¿ç”¨ç¼–è¯‘ç­–ç•¥ {idx+1}: {strategy['mode']}")
                                    break
                                except:
                                    continue
                            
                            if compile_options is None:
                                print(f"  æ‰€æœ‰ç¼–è¯‘ç­–ç•¥éƒ½å¤±è´¥ï¼Œè·³è¿‡ç¼–è¯‘")
                                raise RuntimeError("æ— æ³•æ‰¾åˆ°å¯ç”¨çš„ç¼–è¯‘ç­–ç•¥")
                            
                            # ç¼–è¯‘é€‰é¡¹ï¼šå¯ä»¥å¯ç”¨CUDAå›¾äº†ï¼
                            # å› ä¸ºæˆ‘ä»¬ä¼šä½¿ç”¨ä¸“ç”¨çº¿ç¨‹æ± ï¼Œæ¯ä¸ªGPUä¸€ä¸ªçº¿ç¨‹
                            use_cuda_graphs = os.environ.get('ENABLE_CUDA_GRAPHS', '0') == '1'
                            
                            if use_cuda_graphs:
                                # å¯ç”¨CUDAå›¾çš„æœ€ä¼˜é…ç½®
                                realtime_compile_options = {
                                    "backend": "inductor",
                                    "mode": "max-autotune",     # æœ€æ¿€è¿›ä¼˜åŒ–
                                    "fullgraph": False,
                                    "disable": False,
                                }
                                print(f"  GPU{device_id} ä½¿ç”¨æœ€å¤§ä¼˜åŒ–ç¼–è¯‘ï¼ˆå«CUDAå›¾ï¼‰")
                            else:
                                # ä¿å®ˆæ¨¡å¼ï¼ˆå…¼å®¹ç°æœ‰å¤šçº¿ç¨‹ï¼‰
                                realtime_compile_options = {
                                    "backend": "inductor",
                                    "mode": "reduce-overhead",  # æ— CUDAå›¾
                                    "fullgraph": False,
                                    "disable": False,
                                }
                                print(f"  GPU{device_id} ä½¿ç”¨å®‰å…¨ç¼–è¯‘ï¼ˆæ— CUDAå›¾ï¼‰")
                            
                            # ä¸ºæ¯ä¸ªGPUåˆ›å»ºç‹¬ç«‹çš„ç¼–è¯‘å®ä¾‹
                            print(f"  GPU{device_id} å¼€å§‹ç‹¬ç«‹ç¼–è¯‘...")
                            
                            # UNetç¼–è¯‘ï¼ˆæœ€é‡è¦ï¼‰
                            if hasattr(unet, 'model'):
                                try:
                                    # ç›´æ¥ç¼–è¯‘åŸæ¨¡å‹ï¼Œä¸éœ€è¦deepcopy
                                    # å› ä¸ºæ¯ä¸ªGPUåŠ è½½çš„æ˜¯ç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹
                                    unet.model = torch.compile(unet.model, **realtime_compile_options)
                                    print(f"  âœ… GPU{device_id} UNetç¼–è¯‘å®Œæˆï¼ˆå¤šçº¿ç¨‹å®‰å…¨ï¼‰")
                                except Exception as e:
                                    print(f"  âš ï¸ GPU{device_id} UNetç¼–è¯‘å¤±è´¥: {str(e)[:100]}")
                                    # å¤±è´¥åˆ™ä½¿ç”¨åŸå§‹æ¨¡å‹
                            
                            # VAEç¼–è¯‘ï¼ˆæ¬¡è¦ï¼‰
                            if hasattr(vae, 'vae') and hasattr(vae.vae, 'decoder'):
                                try:
                                    # VAEè§£ç å™¨ä¹Ÿè¦æè‡´ä¼˜åŒ–
                                    vae.vae.decoder = torch.compile(vae.vae.decoder, **realtime_compile_options)
                                    print(f"  âœ… GPU{device_id} VAEè§£ç å™¨ç¼–è¯‘å®Œæˆ")
                                except Exception as e:
                                    print(f"  âš ï¸ GPU{device_id} VAEç¼–è¯‘å¤±è´¥: {str(e)[:100]}")
                            
                            # PEä¹Ÿè¦ç¼–è¯‘ä»¥å‡å°‘å»¶è¿Ÿ
                            if hasattr(pe, 'forward'):
                                try:
                                    pe = torch.compile(pe, **realtime_compile_options)
                                    print(f"  âœ… GPU{device_id} PEéŸ³é¢‘ç¼–ç å™¨ç¼–è¯‘å®Œæˆ")
                                except Exception as e:
                                    print(f"  âš ï¸ GPU{device_id} PEç¼–è¯‘å¤±è´¥: {str(e)[:100]}")
                            
                            print(f"GPU{device_id} æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å®Œæˆï¼ˆå®‰å…¨æ¨¡å¼ï¼‰")
                            
                        except Exception as compile_error:
                            print(f"GPU{device_id} æ¨¡å‹ç¼–è¯‘å¤±è´¥: {compile_error}")
                            print(f"GPU{device_id} ä½¿ç”¨åŸå§‹æ¨¡å‹ï¼ˆæœªä¼˜åŒ–ï¼‰")
                            # ç¼–è¯‘å¤±è´¥ä¸å½±å“è¿è¡Œï¼Œç»§ç»­ä½¿ç”¨åŸå§‹æ¨¡å‹
                    else:
                        if platform.system() == 'Windows':
                            print(f"GPU{device_id} è·³è¿‡ç¼–è¯‘ï¼ˆWindowsä¸æ”¯æŒï¼‰")
                        else:
                            print(f"GPU{device_id} è·³è¿‡ç¼–è¯‘ï¼ˆtorch.compileä¸å¯ç”¨ï¼‰")
                    
                    self.gpu_models[device] = {
                        'vae': vae,
                        'unet': unet,
                        'pe': pe,
                        'device': device
                    }
                    
                    # æ˜¾å­˜ç›‘æ§ - éªŒè¯æ¨¡å‹æ˜¯å¦çœŸæ­£åŠ è½½
                    with torch.cuda.device(device):
                        torch.cuda.synchronize()
                        allocated = torch.cuda.memory_allocated() / (1024**3)
                        reserved = torch.cuda.memory_reserved() / (1024**3)
                        free = torch.cuda.mem_get_info()[0] / (1024**3)
                        total = torch.cuda.mem_get_info()[1] / (1024**3)
                        print(f"GPU{device_id} æ˜¾å­˜çŠ¶æ€:")
                        print(f"  - å·²åˆ†é…: {allocated:.2f}GB")
                        print(f"  - å·²é¢„ç•™: {reserved:.2f}GB")
                        print(f"  - å¯ç”¨: {free:.2f}GB")
                        print(f"  - æ€»é‡: {total:.2f}GB")
                        print(f"  - æ¨¡å‹å ç”¨: ~{reserved:.2f}GB")
                    
                    print(f"GPU{device_id} æ¨¡å‹åŠ è½½å®Œæˆ")
                    return device_id
            
            # SEQUENTIAL_LOADING_FIXED: é¡ºåºåˆå§‹åŒ–é¿å…å¹¶å‘å†²çª
            print(f"å¼€å§‹é¡ºåºåˆå§‹åŒ–{self.gpu_count}ä¸ªGPUï¼ˆé¿å…å¹¶å‘å†²çªï¼‰...")
            successful_gpus = []
            
            for i in range(self.gpu_count):
                print(f"æ­£åœ¨åˆå§‹åŒ–GPU {i}/{self.gpu_count}...")
                try:
                    # åœ¨æ¯ä¸ªGPUåˆå§‹åŒ–å‰æ¸…ç†å†…å­˜
                    torch.cuda.set_device(i)
                    torch.cuda.empty_cache()
                    
                    result = init_gpu_model(i)
                    if result is not None:
                        successful_gpus.append(i)
                        print(f"âœ… GPU{i} åˆå§‹åŒ–æˆåŠŸ ({len(successful_gpus)}/{self.gpu_count})")
                    else:
                        print(f"âŒ GPU{i} åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡")
                except Exception as e:
                    print(f"âŒ GPU{i} åˆå§‹åŒ–å¼‚å¸¸: {e}")
                    # å¦‚æœæ˜¯meta tensoré”™è¯¯ï¼Œå°è¯•é‡è¯•ä¸€æ¬¡
                    if "meta tensor" in str(e) or "Cannot copy out" in str(e):
                        print(f"æ£€æµ‹åˆ°meta tensoré”™è¯¯ï¼Œæ¸…ç†å†…å­˜åé‡è¯•GPU{i}...")
                        try:
                            torch.cuda.empty_cache()
                            import gc
                            gc.collect()
                            result = init_gpu_model(i)
                            if result is not None:
                                successful_gpus.append(i)
                                print(f"âœ… GPU{i} é‡è¯•æˆåŠŸ")
                            else:
                                print(f"âŒ GPU{i} é‡è¯•å¤±è´¥")
                        except Exception as retry_e:
                            print(f"âŒ GPU{i} é‡è¯•å¼‚å¸¸: {retry_e}")
            
            if len(successful_gpus) == 0:
                print("æ‰€æœ‰GPUåˆå§‹åŒ–éƒ½å¤±è´¥äº†")
                return False
            elif len(successful_gpus) < self.gpu_count:
                print(f"éƒ¨åˆ†GPUåˆå§‹åŒ–æˆåŠŸ: {successful_gpus}/{list(range(self.gpu_count))}")
                # æ›´æ–°å¯ç”¨GPUåˆ—è¡¨
                self.devices = [f'cuda:{i}' for i in successful_gpus]
                self.gpu_count = len(successful_gpus)
                print(f"è°ƒæ•´ä¸ºä½¿ç”¨{self.gpu_count}ä¸ªGPU: {self.devices}")
            else:
                print(f"æ‰€æœ‰{self.gpu_count}ä¸ªGPUåˆå§‹åŒ–å®Œæˆ")
            
            # å…±äº«ç»„ä»¶åˆå§‹åŒ–ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
            print("åˆå§‹åŒ–å…±äº«ç»„ä»¶...")
            device0 = self.devices[0]
            
            # Whisperå’ŒAudioProcessoråœ¨CPUä¸Šï¼Œæ‰€æœ‰GPUå…±äº«
            whisper_dir = "./models/whisper"
            if WHISPER_AVAILABLE and os.path.exists(whisper_dir):
                print("å¼€å§‹åŠ è½½Whisperæ¨¡å‹...")
                try:
                    self.shared_whisper = WhisperModel.from_pretrained(whisper_dir).eval()
                    # å°†Whisperæ¨¡å‹ç§»åˆ°GPU0å¹¶ä¿æŒfloat32ï¼ˆWhisperä¸æ”¯æŒhalfï¼‰
                    if torch.cuda.is_available():
                        self.shared_whisper = self.shared_whisper.to(self.devices[0])
                        print(f"Whisperæ¨¡å‹åŠ è½½å®Œæˆï¼Œå·²ç§»è‡³{self.devices[0]}")
                    else:
                        print("Whisperæ¨¡å‹åŠ è½½å®Œæˆï¼ˆCPUæ¨¡å¼ï¼‰")
                except Exception as e:
                    print(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.shared_whisper = None
            else:
                if not WHISPER_AVAILABLE:
                    print("è·³è¿‡Whisperæ¨¡å‹åŠ è½½ - transformers.WhisperModelä¸å¯ç”¨")
                else:
                    print(f"è·³è¿‡Whisperæ¨¡å‹åŠ è½½ - ç›®å½•ä¸å­˜åœ¨: {whisper_dir}")
                self.shared_whisper = None
            
            print("åˆå§‹åŒ–AudioProcessor...")
            try:
                # AudioProcessoréœ€è¦whisperæ¨¡å‹è·¯å¾„
                if os.path.exists(whisper_dir):
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=whisper_dir)
                    print("AudioProcessoråˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"è­¦å‘Š: Whisperç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤AudioProcessor")
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=None)
                    print("AudioProcessoråˆå§‹åŒ–å®Œæˆ (æ— Whisper)")
            except Exception as e:
                print(f"AudioProcessoråˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„AudioProcessorå¤‡ç”¨å®ä¾‹
                try:
                    print("å°è¯•åˆ›å»ºå¤‡ç”¨AudioProcessor...")
                    self.shared_audio_processor = AudioProcessor(feature_extractor_path=None)
                    print("å¤‡ç”¨AudioProcessoråˆ›å»ºæˆåŠŸ")
                except:
                    self.shared_audio_processor = None
                    print("AudioProcessorå®Œå…¨å¤±è´¥ï¼ŒéŸ³é¢‘åŠŸèƒ½å°†ä¸å¯ç”¨")
            
            print("åˆå§‹åŒ–FaceParsing...")
            try:
                self.shared_fp = FaceParsing()
                print("FaceParsingåˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"FaceParsingåˆå§‹åŒ–å¤±è´¥: {e}")
                self.shared_fp = None
            
            # æ—¶é—´æ­¥é•¿
            print("è®¾ç½®æ—¶é—´æ­¥é•¿...")
            self.timesteps = torch.tensor([0], device=device0, dtype=torch.long)
            print("æ—¶é—´æ­¥é•¿è®¾ç½®å®Œæˆ")
            
            init_time = time.time() - start_time
            print(f"æé€Ÿåˆå§‹åŒ–å®Œæˆï¼è€—æ—¶: {init_time:.2f}ç§’")
            print(f"{self.gpu_count}GPUå¹¶è¡Œå¼•æ“å°±ç»ª - æ¯«ç§’çº§å“åº”æ¨¡å¼")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"æé€Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def get_optimal_gpu(self):
        """æ™ºèƒ½GPUè´Ÿè½½å‡è¡¡"""
        # é€‰æ‹©ä½¿ç”¨ç‡æœ€ä½çš„GPU
        optimal_gpu = min(self.gpu_usage.items(), key=lambda x: x[1])[0]
        self.gpu_usage[optimal_gpu] += 1
        return optimal_gpu
    
    def release_gpu(self, device):
        """é‡Šæ”¾GPUèµ„æº"""
        if device in self.gpu_usage:
            self.gpu_usage[device] = max(0, self.gpu_usage[device] - 1)
    
    def ultra_fast_inference_parallel(self, template_id, audio_path, output_path, cache_dir=None, batch_size=None, fps=25, auto_adjust=True, streaming=False, skip_frames=1):
        """æé€Ÿå¹¶è¡Œæ¨ç† - æ¯«ç§’çº§å“åº”
        
        Args:
            auto_adjust: æ˜¯å¦è‡ªåŠ¨è°ƒæ•´batch_sizeï¼ˆOOMæ—¶è‡ªåŠ¨é™çº§ï¼‰
            streaming: æ˜¯å¦å¯ç”¨æµå¼æ¨ç†ï¼ˆWebRTCå®æ—¶é€šè®¯ï¼‰
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜ç›®å½•
        if cache_dir is None:
            cache_dir = os.path.join(self.template_cache_dir, template_id)
        
        # æ™ºèƒ½æ‰¹æ¬¡å¤§å°é€‰æ‹© - åŸºäºå¯ç”¨æ˜¾å­˜å’Œå®é™…æµ‹è¯•
        if batch_size is None:
            # è·å–æ‰€æœ‰GPUçš„å¯ç”¨æ˜¾å­˜
            try:
                min_free_memory = float('inf')
                for gpu_id in range(self.gpu_count):
                    torch.cuda.set_device(gpu_id)
                    free_memory = torch.cuda.mem_get_info()[0] / (1024**3)  # è½¬æ¢ä¸ºGB
                    min_free_memory = min(min_free_memory, free_memory)
                    print(f"GPU {gpu_id} å¯ç”¨æ˜¾å­˜: {free_memory:.1f}GB")
                
                print(f"æœ€å°å¯ç”¨æ˜¾å­˜: {min_free_memory:.1f}GB")
                
                # åŸºäºå®æµ‹ï¼šæ¨¡å‹å ç”¨3.62GB + æ¨ç†æ—¶æ¯å¸§çº¦1.5GB
                # 43GBå¯ç”¨æ˜¾å­˜ - 3.62GBæ¨¡å‹ = 39GBå¯ç”¨äºæ¨ç†
                # 39GB / 1.5GB per frame = å¯å¤„ç†26å¸§
                # ä½†ä¸ºäº†ç¨³å®šæ€§å’Œé€Ÿåº¦å¹³è¡¡ï¼Œè®¾ç½®åˆç†çš„batch_size
                
                if min_free_memory > 40:  # 40GBä»¥ä¸Š - å¯ä»¥å®‰å…¨å¤„ç†12-16å¸§
                    batch_size = 12
                    print(f"âœ… æ˜¾å­˜å……è¶³({min_free_memory:.1f}GB)ï¼Œè®¾ç½®batch_size=12")
                elif min_free_memory > 30:  # 30-40GB - å¯ä»¥å¤„ç†8-10å¸§
                    batch_size = 8
                    print(f"âœ… æ˜¾å­˜è‰¯å¥½({min_free_memory:.1f}GB)ï¼Œè®¾ç½®batch_size=8")
                elif min_free_memory > 20:  # 20-30GB - å¯ä»¥å¤„ç†6å¸§
                    batch_size = 6
                    print(f"âš ï¸ æ˜¾å­˜é€‚ä¸­({min_free_memory:.1f}GB)ï¼Œè®¾ç½®batch_size=6")
                elif min_free_memory > 10:  # 10-20GB - å¯ä»¥å¤„ç†4å¸§
                    batch_size = 4
                    print(f"âš ï¸ æ˜¾å­˜åå°‘({min_free_memory:.1f}GB)ï¼Œè®¾ç½®batch_size=4")
                else:  # 10GBä»¥ä¸‹
                    batch_size = 2
                    print(f"âŒ æ˜¾å­˜ä¸è¶³({min_free_memory:.1f}GB)ï¼Œè®¾ç½®batch_size=2")
                    
                print(f"åŸºäºå¯ç”¨æ˜¾å­˜({min_free_memory:.1f}GB)ï¼Œè®¾ç½®batch_size={batch_size}")
                
                # åŒå¡ä¼˜åŒ–æç¤º
                if self.gpu_count > 1:
                    total_frames = 361  # ç¤ºä¾‹å¸§æ•°
                    batches_needed = (total_frames + batch_size - 1) // batch_size
                    batches_per_gpu = batches_needed // self.gpu_count
                    print(f"ğŸ“Š åŒGPUå¹¶è¡Œå¤„ç†æ–¹æ¡ˆï¼š")
                    print(f"   - æ€»å¸§æ•°: {total_frames}")
                    print(f"   - æ¯æ‰¹æ¬¡: {batch_size}å¸§")
                    print(f"   - æ€»æ‰¹æ¬¡: {batches_needed}")
                    print(f"   - æ¯GPUå¤„ç†: ~{batches_per_gpu}æ‰¹æ¬¡")
                    print(f"   - é¢„è®¡æ¨ç†æ¬¡æ•°: {batches_needed}æ¬¡")
                    
            except Exception as e:
                print(f"æ˜¾å­˜æ£€æµ‹å¤±è´¥: {e}")
                # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨æœ€ä¿å®ˆå€¼
                batch_size = 1
                print(f"ä½¿ç”¨æœ€ä¿å®ˆbatch_size=1")
        
        print(f"ğŸ” æ¨ç†é…ç½®: GPUæ•°={self.gpu_count}, batch_size={batch_size}")
        
        if not self.is_initialized:
            print("æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
        
        try:
            total_start = time.time()
            print(f"å¼€å§‹æé€Ÿå¹¶è¡Œæ¨ç†: {template_id}")
            
            # 1. å¹¶è¡ŒåŠ è½½æ¨¡æ¿ç¼“å­˜ + éŸ³é¢‘ç‰¹å¾æå–
            def load_template_cache_async():
                return self.load_template_cache_optimized(cache_dir, template_id)
            
            def extract_audio_features_async():
                return self.extract_audio_features_ultra_fast(audio_path, fps)
            
            # å…³é”®ä¼˜åŒ–ï¼šå¹¶è¡Œæ‰§è¡Œç¼“å­˜åŠ è½½å’ŒéŸ³é¢‘å¤„ç†
            with ThreadPoolExecutor(max_workers=2) as prep_executor:
                cache_future = prep_executor.submit(load_template_cache_async)
                audio_future = prep_executor.submit(extract_audio_features_async)
                
                cache_data = cache_future.result()
                whisper_chunks = audio_future.result()
            
            if not cache_data:
                print("é”™è¯¯: æ— æ³•åŠ è½½æ¨¡æ¿ç¼“å­˜")
                return False
                
            if whisper_chunks is None:
                print("é”™è¯¯: éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥")
                return False
            
            prep_time = time.time() - total_start
            print(f"å¹¶è¡Œé¢„å¤„ç†å®Œæˆ: {prep_time:.3f}s")
            
            # 2. å¤šGPUå¹¶è¡Œæ¨ç†ï¼ˆæ”¯æŒè·³å¸§åŠ é€Ÿï¼‰
            inference_start = time.time()
            
            # è·³å¸§å¤„ç†ï¼šåªæ¨ç†éƒ¨åˆ†å¸§ï¼Œå…¶ä½™æ’å€¼
            if skip_frames > 1:
                print(f"âš¡ è·³å¸§æ¨¡å¼ï¼šæ¯{skip_frames}å¸§æ¨ç†1æ¬¡")
                # åªå–éœ€è¦æ¨ç†çš„å¸§
                selected_indices = list(range(0, len(whisper_chunks), skip_frames))
                selected_whisper = [whisper_chunks[i] for i in selected_indices]
                
                # æ¨ç†é€‰ä¸­çš„å¸§
                key_frames = self.execute_4gpu_parallel_inference(
                    selected_whisper, cache_data, batch_size
                )
                
                # æ’å€¼ç”Ÿæˆä¸­é—´å¸§
                res_frame_list = self.interpolate_frames(key_frames, len(whisper_chunks), skip_frames)
                print(f"æ¨ç†{len(key_frames)}å…³é”®å¸§ï¼Œæ’å€¼ç”Ÿæˆ{len(res_frame_list)}å¸§")
            else:
                res_frame_list = self.execute_4gpu_parallel_inference(
                    whisper_chunks, cache_data, batch_size
                )
            
            inference_time = time.time() - inference_start
            print(f"{self.gpu_count}GPUå¹¶è¡Œæ¨ç†å®Œæˆ: {inference_time:.3f}s, {len(res_frame_list)}å¸§")
            
            # 3. æé€Ÿå¹¶è¡Œå›¾åƒåˆæˆ
            compose_start = time.time()
            video_frames = self.ultra_fast_compose_frames(res_frame_list, cache_data)
            compose_time = time.time() - compose_start
            print(f"ğŸ¨ å¹¶è¡Œå›¾åƒåˆæˆå®Œæˆ: {compose_time:.3f}s")
            
            # 4. æé€Ÿè§†é¢‘ç”Ÿæˆ
            video_start = time.time()
            success = self.generate_video_ultra_fast(video_frames, audio_path, output_path, fps)
            video_time = time.time() - video_start
            print(f"è§†é¢‘ç”Ÿæˆå®Œæˆ: {video_time:.3f}s")
            
            total_time = time.time() - total_start
            print(f"æé€Ÿæ¨ç†å®Œæˆï¼æ€»è€—æ—¶: {total_time:.3f}s")
            print(f"æ€§èƒ½åˆ†è§£: é¢„å¤„ç†:{prep_time:.3f}s + æ¨ç†:{inference_time:.3f}s + åˆæˆ:{compose_time:.3f}s + è§†é¢‘:{video_time:.3f}s")
            
            # æ€§èƒ½æ•°æ®å·²åœ¨ä¸Šé¢æ‰“å°
            
            return success
            
        except Exception as e:
            print(f"æé€Ÿæ¨ç†å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def execute_4gpu_parallel_inference(self, whisper_chunks, cache_data, batch_size):
        """å¤šGPUå¹¶è¡Œæ¨ç† - åŠ¨æ€é€‚é…GPUæ•°é‡"""
        from musetalk.utils.utils import datagen
        
        print(f"âš™ï¸ æ‰§è¡Œ{self.gpu_count}GPUå¹¶è¡Œæ¨ç†ï¼Œbatch_size={batch_size}")
        
        # æ¨ç†å‰æ¸…ç†æ‰€æœ‰GPUå†…å­˜
        for device in self.devices:
            with torch.cuda.device(device):
                torch.cuda.empty_cache()
        
        input_latent_list_cycle = cache_data['input_latent_list_cycle']
        video_num = len(whisper_chunks)
        
        # æ·»åŠ æ‰¹æ¬¡ä¼˜åŒ–å»ºè®®
        print(f"éŸ³é¢‘å¸§æ•°: {video_num}")
        if video_num > 50 and batch_size < 4:
            # åŸºäºå®é™…GPUå†…å­˜æƒ…å†µçš„å»ºè®®
            if video_num > 100:
                suggested_batch_size = 6  # é•¿éŸ³é¢‘ç”¨æ›´å¤§æ‰¹æ¬¡
            elif video_num > 50:
                suggested_batch_size = 4  # ä¸­ç­‰éŸ³é¢‘
            else:
                suggested_batch_size = 3  # çŸ­éŸ³é¢‘
            
            print(f"âš ï¸ å½“å‰batch_size={batch_size}å¯èƒ½å¤ªå°ï¼Œå»ºè®®ä½¿ç”¨batch_size={suggested_batch_size}")
            print(f"  è¿™å°†å‡å°‘æ‰¹æ¬¡æ•°ä»{video_num // batch_size}åˆ°{video_num // suggested_batch_size}")
            print(f"  é¢„è®¡å¯èŠ‚çœ{(video_num // batch_size - video_num // suggested_batch_size) * 5}ç§’")
        
        # ç”Ÿæˆæ‰€æœ‰æ‰¹æ¬¡
        gen = datagen(
            whisper_chunks=whisper_chunks,
            vae_encode_latents=input_latent_list_cycle,
            batch_size=batch_size,
            delay_frame=0,
            device='cpu'  # åœ¨CPUä¸Šç”Ÿæˆæ•°æ®ï¼Œé¿å…GPU0å†…å­˜å‹åŠ›
        )
        all_batches = list(gen)
        total_batches = len(all_batches)
        
        print(f"{self.gpu_count}GPUå¹¶è¡Œå¤„ç† {total_batches} æ‰¹æ¬¡...")
        
        # å…³é”®ä¼˜åŒ–ï¼šæ¯ä¸ªGPUå¤„ç†ç‹¬ç«‹çš„æ‰¹æ¬¡ï¼Œæ— éœ€åŒæ­¥
        def process_batch_on_gpu(batch_info):
            batch_idx, (whisper_batch, latent_batch) = batch_info
            
            # æ™ºèƒ½GPUåˆ†é… - ç¡®ä¿ä½¿ç”¨æœ‰æ•ˆçš„GPU
            target_device = self.devices[batch_idx % self.gpu_count]
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿GPUæ¨¡å‹å­˜åœ¨
            if target_device not in self.gpu_models:
                print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx}: GPU {target_device} æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡")
                return batch_idx, []
            
            gpu_models = self.gpu_models[target_device]
            
            # å®æ—¶ç›‘æ§æ˜¾å­˜ä½¿ç”¨ç‡
            with torch.cuda.device(target_device):
                free_mem_before = torch.cuda.mem_get_info()[0] / (1024**3)
                total_mem = torch.cuda.mem_get_info()[1] / (1024**3)
                used_mem_before = total_mem - free_mem_before
                usage_percent = (used_mem_before / total_mem) * 100
                
                print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx} -> GPU {target_device}")
                print(f"  æ‰¹æ¬¡å¤§å°: {whisper_batch.shape[0]}å¸§")
                print(f"  æ˜¾å­˜ä½¿ç”¨: {used_mem_before:.1f}/{total_mem:.1f}GB ({usage_percent:.1f}%)")
                
                # å¦‚æœæ˜¾å­˜ä½¿ç”¨è¶…è¿‡90%ï¼Œè·³è¿‡æ‰¹æ¬¡é¿å…OOM
                if usage_percent > 90:
                    print(f"âš ï¸ GPU {target_device} æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜({usage_percent:.1f}%)ï¼Œè·³è¿‡æ‰¹æ¬¡")
                    return batch_idx, []
            
            try:
                
                # å…³é”®ï¼šæ•°æ®ç§»åŠ¨åˆ°ç›®æ ‡GPU
                with torch.cuda.device(target_device):
                    whisper_batch = whisper_batch.to(target_device, dtype=self.weight_dtype, non_blocking=True)
                    latent_batch = latent_batch.to(target_device, dtype=self.weight_dtype, non_blocking=True)
                    # ç¡®ä¿timestepsåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    if self.timesteps is not None:
                        timesteps = self.timesteps.to(target_device)
                    else:
                        # å¦‚æœtimestepsæœªåˆå§‹åŒ–ï¼Œåˆ›å»ºä¸€ä¸ª
                        timesteps = torch.tensor([0], device=target_device, dtype=torch.long)
                    
                    # æ ¸å¿ƒæ¨ç† - ä½¿ç”¨ç‹¬ç«‹çš„GPUæ¨¡å‹
                    with torch.no_grad():
                        # è°ƒè¯•ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
                        if 'pe' not in gpu_models or gpu_models['pe'] is None:
                            raise ValueError(f"PEæ¨¡å‹åœ¨{target_device}ä¸Šæœªåˆå§‹åŒ–")
                        if 'unet' not in gpu_models or gpu_models['unet'] is None:
                            raise ValueError(f"UNetæ¨¡å‹åœ¨{target_device}ä¸Šæœªåˆå§‹åŒ–")
                        if 'vae' not in gpu_models or gpu_models['vae'] is None:
                            raise ValueError(f"VAEæ¨¡å‹åœ¨{target_device}ä¸Šæœªåˆå§‹åŒ–")
                        
                        # ç°åœ¨é¢„å¤„ç†ç›´æ¥ç”Ÿæˆ8é€šé“latentï¼Œä¸éœ€è¦å†è¿›è¡Œé€šé“æ•°æ£€æŸ¥å’Œè½¬æ¢
                        audio_features = gpu_models['pe'](whisper_batch)
                        pred_latents = gpu_models['unet'].model(
                            latent_batch, timesteps, encoder_hidden_states=audio_features
                        ).sample
                        recon_frames = gpu_models['vae'].decode_latents(pred_latents)
                    
                    # ç«‹å³ç§»å›CPUé‡Šæ”¾GPUå†…å­˜
                    # æ£€æŸ¥è¿”å›ç±»å‹ï¼Œå¦‚æœå·²ç»æ˜¯numpyæ•°ç»„å°±ç›´æ¥ä½¿ç”¨
                    if isinstance(recon_frames, list):
                        result_frames = recon_frames
                    elif isinstance(recon_frames, np.ndarray):
                        result_frames = [recon_frames[i] for i in range(recon_frames.shape[0])]
                    else:
                        # å¦‚æœæ˜¯torch tensorï¼Œè½¬æ¢ä¸ºnumpy
                        result_frames = [frame.cpu().numpy() if hasattr(frame, 'cpu') else frame for frame in recon_frames]
                    
                    # æ¸…ç†GPUå†…å­˜
                    del whisper_batch, latent_batch, audio_features, pred_latents, recon_frames
                    torch.cuda.empty_cache()
                    
                    return batch_idx, result_frames
                    
            except torch.cuda.OutOfMemoryError as oom_error:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} GPU {target_device} OOMé”™è¯¯!")
                print(f"   é”™è¯¯è¯¦æƒ…: {str(oom_error)}")
                # è·å–å½“å‰æ˜¾å­˜çŠ¶æ€
                with torch.cuda.device(target_device):
                    free_mem = torch.cuda.mem_get_info()[0] / (1024**3)
                    total_mem = torch.cuda.mem_get_info()[1] / (1024**3)
                    allocated = torch.cuda.memory_allocated() / (1024**3)
                    print(f"   GPU {target_device} æ˜¾å­˜: å·²ç”¨{allocated:.1f}GB / å¯ç”¨{free_mem:.1f}GB / æ€»é‡{total_mem:.1f}GB")
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                return batch_idx, []
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} GPU {target_device} å¤±è´¥!")
                print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
                print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
                # æ‰“å°å †æ ˆè·Ÿè¸ª
                import traceback
                traceback.print_exc()
                # å¤±è´¥æ—¶æ¸…ç†GPUå†…å­˜
                with torch.cuda.device(target_device):
                    torch.cuda.empty_cache()
                return batch_idx, []
        
        # çœŸæ­£çš„4GPUå¹¶è¡Œæ‰§è¡Œ
        res_frame_list = []
        batch_results = {}
        
        # ä½¿ç”¨æ›´å¤§çš„å¹¶å‘æ•°ï¼Œè®©GPUä¿æŒå¿™ç¢Œ
        max_workers = self.gpu_count * 2  # å…è®¸æ›´å¤šå¹¶å‘
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ç›´æ¥æäº¤æ‰€æœ‰æ‰¹æ¬¡ï¼Œè®©çº¿ç¨‹æ± ç®¡ç†è°ƒåº¦
            futures = {}
            for batch_idx, batch_info in enumerate(all_batches):
                future = executor.submit(process_batch_on_gpu, (batch_idx, batch_info))
                futures[future] = batch_idx
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(futures):
                batch_idx, frames = future.result()
                batch_results[batch_idx] = frames
                completed += 1
                if completed % 10 == 0 or completed == total_batches:
                    print(f"è¿›åº¦: {completed}/{total_batches} æ‰¹æ¬¡å®Œæˆ")
        
        # å¤„ç†å®Œæ‰€æœ‰æ‰¹æ¬¡åï¼Œåªæ¸…ç†ä¸€æ¬¡å†…å­˜
        if total_batches > 20:  # åªæœ‰åœ¨æ‰¹æ¬¡å¾ˆå¤šæ—¶æ‰æ¸…ç†
            for device in self.devices:
                with torch.cuda.device(device):
                    torch.cuda.empty_cache()
        
        # æŒ‰é¡ºåºåˆå¹¶ç»“æœ
        for i in range(total_batches):
            if i in batch_results:
                res_frame_list.extend(batch_results[i])
        
        return res_frame_list
    
    def ultra_fast_compose_frames(self, res_frame_list, cache_data):
        """æé€Ÿå¹¶è¡Œå›¾åƒåˆæˆ - 32çº¿ç¨‹"""
        coord_list_cycle = cache_data['coord_list_cycle']
        frame_list_cycle = cache_data['frame_list_cycle']
        mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
        mask_list_cycle = cache_data['mask_list_cycle']
        
        print(f"ğŸ¨ å¼€å§‹32çº¿ç¨‹å¹¶è¡Œåˆæˆ {len(res_frame_list)} å¸§...")
        
        def compose_single_frame(frame_info):
            i, res_frame = frame_info
            try:
                bbox = coord_list_cycle[i % len(coord_list_cycle)]
                ori_frame = copy.deepcopy(frame_list_cycle[i % len(frame_list_cycle)])
                
                x1, y1, x2, y2 = bbox
                res_frame = cv2.resize(res_frame.astype(np.uint8), (x2-x1, y2-y1))
                
                # ä½¿ç”¨ä¼˜åŒ–çš„blending
                mask_coords = mask_coords_list_cycle[i % len(mask_coords_list_cycle)]
                mask = mask_list_cycle[i % len(mask_list_cycle)]
                
                combine_frame = get_image_blending(
                    image=ori_frame,
                    face=res_frame, 
                    face_box=[x1, y1, x2, y2],
                    mask_array=mask,
                    crop_box=mask_coords
                )
                
                return i, combine_frame
                
            except Exception as e:
                print(f"åˆæˆç¬¬{i}å¸§å¤±è´¥: {str(e)}")
                return i, None
        
        # 32çº¿ç¨‹å¹¶è¡Œåˆæˆ
        composed_frames = {}
        with ThreadPoolExecutor(max_workers=32) as executor:
            frame_futures = {
                executor.submit(compose_single_frame, (i, frame)): i 
                for i, frame in enumerate(res_frame_list)
            }
            
            for future in as_completed(frame_futures):
                frame_idx, composed_frame = future.result()
                if composed_frame is not None:
                    composed_frames[frame_idx] = composed_frame
        
        # æŒ‰é¡ºåºæ’åˆ—
        video_frames = []
        for i in range(len(res_frame_list)):
            if i in composed_frames:
                video_frames.append(composed_frames[i])
        
        print(f"å¹¶è¡Œåˆæˆå®Œæˆ: {len(video_frames)} å¸§")
        return video_frames
    
    def extract_audio_features_ultra_fast(self, audio_path, fps):
        """æé€ŸéŸ³é¢‘ç‰¹å¾æå– - ä¼˜åŒ–ç‰ˆ"""
        try:
            import time
            start = time.time()
            
            # æ£€æŸ¥AudioProcessoræ˜¯å¦å¯ç”¨
            if self.shared_audio_processor is None:
                raise ValueError("AudioProcessoræœªåˆå§‹åŒ–")
            
            # éŸ³é¢‘ç‰¹å¾ç¼“å­˜ï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„ï¼‰
            audio_cache_key = f"{audio_path}_{fps}"
            if audio_cache_key in self.audio_feature_cache:
                print(f"âœ… ä½¿ç”¨ç¼“å­˜çš„éŸ³é¢‘ç‰¹å¾")
                return self.audio_feature_cache[audio_cache_key]
                
            whisper_input_features, librosa_length = self.shared_audio_processor.get_audio_feature(audio_path)
            print(f"éŸ³é¢‘åŠ è½½è€—æ—¶: {time.time() - start:.3f}s")
            
            # ç¡®ä¿Whisperä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
            # Whisperæ¨¡å‹å§‹ç»ˆä½¿ç”¨float32ï¼Œä¸æ”¯æŒhalf precision
            whisper_dtype = torch.float32
            
            # å¦‚æœè¾“å…¥ç‰¹å¾åœ¨GPUä¸Šä¸”æ˜¯halfç±»å‹ï¼Œè½¬æ¢ä¸ºfloat32
            if isinstance(whisper_input_features, torch.Tensor):
                if whisper_input_features.dtype == torch.float16:
                    whisper_input_features = whisper_input_features.float()
            
            whisper_chunks = self.shared_audio_processor.get_whisper_chunk(
                whisper_input_features, 
                self.devices[0],  # Whisperåœ¨GPU0
                whisper_dtype,  # ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
                self.shared_whisper, 
                librosa_length,
                fps=fps,
                audio_padding_length_left=2,
                audio_padding_length_right=2,
            )
            return whisper_chunks
        except Exception as e:
            print(f"éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {str(e)}")
            return None
    
    def load_template_cache_optimized(self, cache_dir, template_id):
        """ä¼˜åŒ–çš„æ¨¡æ¿ç¼“å­˜åŠ è½½"""
        try:
            cache_file = os.path.join(cache_dir, f"{template_id}_preprocessed.pkl")
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            return cache_data
            
        except Exception as e:
            print(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def generate_video_ultra_fast(self, video_frames, audio_path, output_path, fps):
        """æé€Ÿè§†é¢‘ç”Ÿæˆ"""
        try:
            # ç›´æ¥å†…å­˜ç”Ÿæˆï¼Œæ— ä¸´æ—¶æ–‡ä»¶
            print(f"ç›´æ¥ç”Ÿæˆè§†é¢‘: {len(video_frames)} å¸§")
            
            # ç”Ÿæˆæ— éŸ³é¢‘è§†é¢‘
            temp_video = output_path.replace('.mp4', '_temp.mp4')
            imageio.mimwrite(
                temp_video, video_frames, 'FFMPEG', 
                fps=fps, codec='libx264', pixelformat='yuv420p',
                output_params=['-preset', 'ultrafast', '-crf', '23']
            )
            
            # å¹¶è¡ŒéŸ³é¢‘åˆæˆ
            try:
                from moviepy.editor import VideoFileClip, AudioFileClip
                video_clip = VideoFileClip(temp_video)
                audio_clip = AudioFileClip(audio_path)
                
                final_clip = video_clip.set_audio(audio_clip)
                final_clip.write_videofile(
                    output_path, 
                    codec='libx264', 
                    audio_codec='aac', 
                    fps=fps,
                    preset='ultrafast',
                    verbose=False, 
                    logger=None
                )
                
                final_clip.close()
                audio_clip.close()
                video_clip.close()
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(temp_video)
                
            except Exception as e:
                print(f"éŸ³é¢‘åˆæˆå¤±è´¥ï¼Œä½¿ç”¨æ— éŸ³é¢‘ç‰ˆæœ¬: {str(e)}")
                os.rename(temp_video, output_path)
            
            return True
            
        except Exception as e:
            print(f"è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
            return False

# å…¨å±€æœåŠ¡å®ä¾‹
global_service = UltraFastMuseTalkService()

def start_ultra_fast_service(port=28888):
    """å¯åŠ¨æé€ŸæœåŠ¡"""
    print(f"å¯åŠ¨Ultra Fast Service - ç«¯å£: {port}")
    
    # ç¡®ä¿å·¥ä½œç›®å½•å’Œè·¯å¾„æ­£ç¡®
    import os
    from pathlib import Path
    
    # å¦‚æœä¸åœ¨æ­£ç¡®çš„å·¥ä½œç›®å½•ï¼Œåˆ‡æ¢åˆ°MuseTalkç›®å½•
    current_dir = Path.cwd()
    if not (current_dir / "models" / "musetalkV15" / "unet.pth").exists():
        # å°è¯•æ‰¾åˆ°MuseTalkç›®å½•
        script_dir = Path(__file__).parent
        musetalk_dir = script_dir.parent / "MuseTalk"
        if musetalk_dir.exists():
            os.chdir(musetalk_dir)
            print(f"å·¥ä½œç›®å½•åˆ‡æ¢åˆ°: {musetalk_dir}")
        else:
            print(f"è­¦å‘Š: æ— æ³•æ‰¾åˆ°MuseTalkæ¨¡å‹ç›®å½•")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("å¼€å§‹åˆå§‹åŒ–Ultra Fastæ¨¡å‹...")
    try:
        if not global_service.initialize_models_ultra_fast():
            print("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ - è¿”å›False")
            return
        print("æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"æ¨¡å‹åˆå§‹åŒ–å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # æ€§èƒ½ç›‘æ§å·²ç¦ç”¨
    
    # å¯åŠ¨IPCæœåŠ¡å™¨
    try:
        server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_socket.bind(('0.0.0.0', port))
        server_socket.listen(5)
        
        # éªŒè¯ç›‘å¬çŠ¶æ€
        sock_name = server_socket.getsockname()
        print(f"âœ… SocketæˆåŠŸç»‘å®šåˆ°: {sock_name}")
        print(f"Ultra Fast Service å°±ç»ª - ç›‘å¬ç«¯å£: {port}")
        print("æ¯«ç§’çº§å“åº”æ¨¡å¼å·²å¯ç”¨")
        
        while True:
            try:
                client_socket, addr = server_socket.accept()
                print(f"ğŸ”— å®¢æˆ·ç«¯è¿æ¥: {addr}")
                
                # å¤„ç†è¯·æ±‚
                thread = threading.Thread(
                    target=handle_client_ultra_fast, 
                    args=(client_socket,)
                )
                thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
                thread.start()
                print(f"å¯åŠ¨å¤„ç†çº¿ç¨‹: {thread.name}")
                
            except Exception as e:
                print(f"è¿æ¥å¤„ç†å¤±è´¥: {str(e)}")
                
    except Exception as e:
        print(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")

def handle_client_ultra_fast(client_socket):
    """å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚ - æé€Ÿç‰ˆæœ¬"""
    try:
        while True:  # ä¸»å¾ªç¯å¤„ç†å¤šä¸ªè¯·æ±‚
            # æ¥æ”¶è¯·æ±‚ - ä½¿ç”¨æ¢è¡Œç¬¦åè®®ï¼ˆä¸C#ç«¯åŒ¹é…ï¼‰
            buffer = b''
            while True:
                chunk = client_socket.recv(1)
                if not chunk:
                    print("å®¢æˆ·ç«¯å…³é—­è¿æ¥")
                    return  # é€€å‡ºå‡½æ•°
                buffer += chunk
                if chunk == b'\n':
                    break
            
            if not buffer:
                break
                
            data = buffer.decode('utf-8').strip()
            if not data:
                print("æ”¶åˆ°ç©ºæ•°æ®ï¼Œè·³è¿‡")
                continue
                
            try:
                request = json.loads(data)
                command = request.get('command', '')
                
                # åªæ‰“å°épingå‘½ä»¤çš„æ—¥å¿—
                if command != 'ping':
                    print(f"æ”¶åˆ°æ•°æ®: {repr(data[:200])}")
                
                # å¤„ç†ä¸åŒçš„å‘½ä»¤
                if command == 'preprocess':
                    # å¤„ç†é¢„å¤„ç†è¯·æ±‚ - å…¼å®¹ä¸¤ç§å­—æ®µå
                    template_id = request.get('templateId') or request.get('template_id')
                    template_image_path = request.get('templateImagePath') or request.get('template_image_path')
                    bbox_shift = request.get('bboxShift', 0) or request.get('bbox_shift', 0)
                    
                    print(f"å¤„ç†é¢„å¤„ç†è¯·æ±‚: template_id={template_id}, image_path={template_image_path}")
                    
                    # ä¿®æ­£è·¯å¾„ï¼šC#å®¹å™¨çš„è·¯å¾„éœ€è¦è½¬æ¢ä¸ºPythonå®¹å™¨èƒ½è®¿é—®çš„è·¯å¾„
                    # /app/wwwroot/templates/xxx.jpg -> /opt/musetalk/repo/LmyDigitalHuman/wwwroot/templates/xxx.jpg
                    if template_image_path and '/app/wwwroot/templates/' in template_image_path:
                        filename = os.path.basename(template_image_path)
                        template_image_path = f"/opt/musetalk/repo/LmyDigitalHuman/wwwroot/templates/{filename}"
                        print(f"ä¿®æ­£å›¾ç‰‡è·¯å¾„: {template_image_path}")
                    
                    # è°ƒç”¨çœŸæ­£çš„é¢„å¤„ç†åŠŸèƒ½
                    try:
                        # å¯¼å…¥é¢„å¤„ç†æ¨¡å—
                        # å¯¼å…¥é¢„å¤„ç†å™¨
                        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                        from core.preprocessing import OptimizedPreprocessor
                        
                        # è·å–ç¼“å­˜ç›®å½•
                        cache_dir = os.environ.get('MUSE_TEMPLATE_CACHE_DIR', '/opt/musetalk/template_cache')
                        
                        # åˆ›å»ºé¢„å¤„ç†å™¨å¹¶æ‰§è¡Œ
                        preprocessor = OptimizedPreprocessor()
                        preprocessor.initialize_models()
                        success = preprocessor.preprocess_template_ultra_fast(
                            template_path=template_image_path,
                            output_dir=cache_dir,
                            template_id=template_id
                        )
                        
                        response = {
                            'success': success,
                            'templateId': template_id,
                            'message': 'Preprocessing completed' if success else 'Preprocessing failed',
                            'processTime': 1.0  # å®é™…å¤„ç†æ—¶é—´
                        }
                        print(f"é¢„å¤„ç†{'æˆåŠŸ' if success else 'å¤±è´¥'}: {template_id}")
                        
                    except Exception as e:
                        print(f"é¢„å¤„ç†å¼‚å¸¸: {e}")
                        import traceback
                        traceback.print_exc()
                        response = {
                            'success': False,
                            'templateId': template_id,
                            'message': f'Preprocessing error: {str(e)}',
                            'processTime': 0
                        }
                    
                    # å‘é€å“åº”ï¼ˆæ¢è¡Œç¬¦ç»“å°¾ï¼‰
                    response_json = json.dumps(response) + '\n'
                    client_socket.send(response_json.encode('utf-8'))
                    print(f"âœ… å‘é€é¢„å¤„ç†å“åº”: {template_id}, ç»“æœ: {response['success']}")
                    
                elif command == 'ping':
                    response = {'success': True, 'message': 'pong'}
                    client_socket.send((json.dumps(response) + '\n').encode('utf-8'))
                    # æ³¨é‡Šæ‰pingæ—¥å¿—ï¼Œé¿å…åˆ·å±
                    # print("âœ… å‘é€pongå“åº”")
                    
                elif command == 'inference' or 'template_id' in request:
                    # æ¨ç†è¯·æ±‚
                    print(f"ğŸ“¨ æé€Ÿæ¨ç†è¯·æ±‚: {request.get('template_id')}")
                    
                    # ä¸è¦å¼ºåˆ¶ä½¿ç”¨batch_sizeï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–
                    received_batch_size = request.get('batch_size', None)  # Noneè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©
                    if received_batch_size:
                        print(f"ğŸ“Š ä½¿ç”¨æŒ‡å®šçš„batch_size: {received_batch_size}")
                    else:
                        print(f"ğŸ“Š å°†æ ¹æ®æ˜¾å­˜è‡ªåŠ¨é€‰æ‹©batch_size")
                    
                    # æé€Ÿæ¨ç†
                    start_time = time.time()
                    success = global_service.ultra_fast_inference_parallel(
                        template_id=request['template_id'],
                        audio_path=request['audio_path'],
                        output_path=request['output_path'],
                        cache_dir=request['cache_dir'],
                        batch_size=received_batch_size,
                        fps=request.get('fps', 25)
                    )
                    
                    process_time = time.time() - start_time
                    print(f"æé€Ÿæ¨ç†å®Œæˆ: {process_time:.3f}s, ç»“æœ: {success}")
                    
                    # å‘é€å“åº”ï¼ˆæ¢è¡Œç¬¦ç»“å°¾ï¼‰
                    response = {'Success': success, 'OutputPath': request['output_path'] if success else None}
                    client_socket.send((json.dumps(response) + '\n').encode('utf-8'))
                    
                else:
                    print(f"æœªçŸ¥å‘½ä»¤: {command}")
                    response = {'success': False, 'message': f'Unknown command: {command}'}
                    client_socket.send((json.dumps(response) + '\n').encode('utf-8'))
                    
            except json.JSONDecodeError as e:
                print(f"JSONè§£æé”™è¯¯: {e}, æ•°æ®: {repr(data[:200])}")
                error_response = {'success': False, 'message': f'JSON parse error: {str(e)}'}
                client_socket.send((json.dumps(error_response) + '\n').encode('utf-8'))
            except Exception as e:
                print(f"å¤„ç†è¯·æ±‚å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                error_response = {'success': False, 'message': str(e)}
                client_socket.send((json.dumps(error_response) + '\n').encode('utf-8'))
        
    except Exception as e:
        print(f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        client_socket.close()

def main():
    """ä¸»å…¥å£å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, default=28888, help='æœåŠ¡ç«¯å£')
    args = parser.parse_args()
    
    # å¯åŠ¨æœåŠ¡
    start_ultra_fast_service(args.port)

if __name__ == "__main__":
    main()
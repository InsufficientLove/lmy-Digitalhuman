#!/usr/bin/env python3
"""
åŸºäºå®˜æ–¹MuseTalkçš„æ­£ç¡®å®æ—¶æ¨ç†å®ç°
ç›´æ¥ä½¿ç”¨é¢„å¤„ç†ç¼“å­˜ï¼Œä¸é‡å¤é¢„å¤„ç†

å‚è€ƒ: https://github.com/TMElyralab/MuseTalk
ä½œè€…: Claude Sonnet (åŸºäºå®˜æ–¹å®ç°)
"""

import argparse
import os
import numpy as np
import cv2
import torch
import glob
import pickle
import sys
from tqdm import tqdm
import copy
import json
import time
import threading
import queue
from transformers import WhisperModel

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen
from musetalk.utils.preprocessing import read_imgs
from musetalk.utils.blending import get_image_blending
from musetalk.utils.utils import load_all_model
from musetalk.utils.audio_processor import AudioProcessor


@torch.no_grad()
class OfficialMuseTalkAvatar:
    """åŸºäºå®˜æ–¹å®ç°çš„MuseTalk Avatar"""
    
    def __init__(self, template_id, cache_dir, device, batch_size=8):
        self.template_id = template_id
        self.cache_dir = cache_dir
        self.device = device
        self.batch_size = batch_size
        self.idx = 0
        
        # åŠ è½½ç¼“å­˜æ•°æ®
        self.load_cache()
    
    def load_cache(self):
        """åŠ è½½é¢„å¤„ç†ç¼“å­˜æ•°æ®"""
        print(f"ğŸ”„ åŠ è½½æ¨¡æ¿ç¼“å­˜: {self.template_id}")
        
        # ä½¿ç”¨ç°æœ‰çš„é¢„å¤„ç†ç¼“å­˜æ–‡ä»¶
        cache_file = os.path.join(self.cache_dir, f"{self.template_id}_preprocessed.pkl")
        metadata_file = os.path.join(self.cache_dir, f"{self.template_id}_metadata.json")
        
        if not os.path.exists(cache_file):
            raise FileNotFoundError(f"é¢„å¤„ç†ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
        if not os.path.exists(metadata_file):
            raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
        
        # åŠ è½½é¢„å¤„ç†ç¼“å­˜æ•°æ®
        with open(cache_file, 'rb') as f:
            cache_data = pickle.load(f)
        
        # æå–æ•°æ®
        self.input_latent_list_cycle = cache_data['input_latent_list_cycle']
        self.coord_list_cycle = cache_data['coord_list_cycle']
        self.frame_list_cycle = cache_data['frame_list_cycle']
        self.mask_coords_list_cycle = cache_data['mask_coords_list_cycle']
        self.mask_list_cycle = cache_data['mask_list_cycle']
        
        print(f"âœ… åŠ è½½æ½œåœ¨å‘é‡: {len(self.input_latent_list_cycle)} å¸§")
        print(f"âœ… åŠ è½½é¢éƒ¨åæ ‡: {len(self.coord_list_cycle)} å¸§")
        print(f"âœ… åŠ è½½æ©ç åæ ‡: {len(self.mask_coords_list_cycle)} å¸§")
        
        # åŠ è½½å…ƒæ•°æ®
        with open(metadata_file, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        print(f"âœ… ç¼“å­˜åŠ è½½å®Œæˆ: {self.template_id}")
    
    def process_frames(self, res_frame_queue, video_len, output_dir):
        """å¤„ç†ç”Ÿæˆçš„å¸§"""
        os.makedirs(output_dir, exist_ok=True)
        frame_idx = 0
        
        while frame_idx < video_len:
            try:
                res_frame = res_frame_queue.get(block=True, timeout=1)
            except queue.Empty:
                continue
            
            # è·å–å¯¹åº”çš„åæ ‡å’Œæ©ç 
            cycle_idx = frame_idx % len(self.coord_list_cycle)
            bbox = self.coord_list_cycle[cycle_idx]
            ori_frame = copy.deepcopy(self.frame_list_cycle[cycle_idx])
            
            x1, y1, x2, y2 = bbox
            try:
                res_frame = cv2.resize(res_frame.astype(np.uint8), (x2 - x1, y2 - y1))
            except:
                frame_idx += 1
                continue
            
            mask = self.mask_list_cycle[cycle_idx]
            mask_crop_box = self.mask_coords_list_cycle[cycle_idx]
            combine_frame = get_image_blending(ori_frame, res_frame, bbox, mask, mask_crop_box)
            
            # ä¿å­˜å¸§
            cv2.imwrite(f"{output_dir}/{str(frame_idx).zfill(8)}.png", combine_frame)
            frame_idx += 1
    
    @torch.no_grad()
    def inference(self, audio_path, output_path, fps, models):
        """æ‰§è¡Œæ¨ç†ç”Ÿæˆè§†é¢‘"""
        print(f"ğŸš€ å¼€å§‹æ¨ç†: {self.template_id}")
        
        vae, unet, pe, audio_processor, whisper = models
        weight_dtype = unet.model.dtype
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = os.path.join(os.path.dirname(output_path), "temp_frames")
        os.makedirs(temp_dir, exist_ok=True)
        
        ############################################## æå–éŸ³é¢‘ç‰¹å¾ ##############################################
        start_time = time.time()
        whisper_input_features, librosa_length = audio_processor.get_audio_feature(audio_path, weight_dtype=weight_dtype)
        whisper_chunks = audio_processor.get_whisper_chunk(
            whisper_input_features,
            self.device,
            weight_dtype,
            whisper,
            librosa_length,
            fps=fps,
            audio_padding_length_left=0,
            audio_padding_length_right=0,
        )
        print(f"âœ… éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {(time.time() - start_time) * 1000:.1f}ms, éŸ³é¢‘å—æ•°: {len(whisper_chunks)}")
        
        ############################################## æ‰¹é‡æ¨ç† ##############################################
        video_num = len(whisper_chunks)
        res_frame_queue = queue.Queue()
        self.idx = 0
        
        # å¯åŠ¨å¸§å¤„ç†çº¿ç¨‹
        process_thread = threading.Thread(target=self.process_frames, args=(res_frame_queue, video_num, temp_dir))
        process_thread.start()
        
        # æ‰¹é‡ç”Ÿæˆæ•°æ®
        gen = datagen(whisper_chunks, self.input_latent_list_cycle, self.batch_size)
        start_time = time.time()
        
        for i, (whisper_batch, latent_batch) in enumerate(tqdm(gen, total=int(np.ceil(float(video_num) / self.batch_size)))):
            audio_feature_batch = pe(whisper_batch.to(self.device))
            latent_batch = latent_batch.to(device=self.device, dtype=unet.model.dtype)
            
            # UNetæ¨ç†
            pred_latents = unet.model(latent_batch,
                                    torch.tensor([0], device=self.device),
                                    encoder_hidden_states=audio_feature_batch).sample
            pred_latents = pred_latents.to(device=self.device, dtype=vae.vae.dtype)
            
            # VAEè§£ç 
            recon = vae.decode_latents(pred_latents)
            for res_frame in recon:
                res_frame_queue.put(res_frame)
        
        # ç­‰å¾…å¸§å¤„ç†å®Œæˆ
        process_thread.join()
        
        inference_time = time.time() - start_time
        print(f'âœ… æ¨ç†å®Œæˆ: {video_num} å¸§, è€—æ—¶: {inference_time:.2f}ç§’')
        
        # åˆæˆè§†é¢‘
        cmd_img2video = f"ffmpeg -y -v warning -r {fps} -f image2 -i {temp_dir}/%08d.png -vcodec libx264 -vf format=yuv420p -crf 18 {output_path}"
        print(f"ğŸ¬ åˆæˆè§†é¢‘: {cmd_img2video}")
        result = os.system(cmd_img2video)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)
        
        if result == 0:
            print(f"âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
            return output_path
        else:
            raise RuntimeError("è§†é¢‘åˆæˆå¤±è´¥")


def main():
    parser = argparse.ArgumentParser(description="å®˜æ–¹MuseTalkå®æ—¶æ¨ç†")
    parser.add_argument("--template_id", required=True, help="æ¨¡æ¿ID")
    parser.add_argument("--audio_path", required=True, help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_path", required=True, help="è¾“å‡ºè§†é¢‘è·¯å¾„")
    parser.add_argument("--cache_dir", default="./model_states", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--device", default="cuda:0", help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--batch_size", type=int, default=8, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--fps", type=int, default=25, help="è§†é¢‘å¸§ç‡")
    parser.add_argument("--unet_model_path", default="models/musetalk/pytorch_model.bin", help="UNetæ¨¡å‹è·¯å¾„")
    parser.add_argument("--unet_config", default="models/musetalk/musetalk.json", help="UNeté…ç½®è·¯å¾„")
    parser.add_argument("--vae_type", default="sd-vae", help="VAEç±»å‹")
    parser.add_argument("--whisper_dir", default="models/whisper", help="Whisperæ¨¡å‹ç›®å½•")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”„ åŠ è½½MuseTalkæ¨¡å‹...")
    vae, unet, pe = load_all_model(
        unet_model_path=args.unet_model_path,
        vae_type=args.vae_type,
        unet_config=args.unet_config,
        device=device
    )
    
    # è®¾ç½®åŠç²¾åº¦
    pe = pe.half().to(device)
    vae.vae = vae.vae.half().to(device)
    unet.model = unet.model.half().to(device)
    
    # åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨å’ŒWhisperæ¨¡å‹
    audio_processor = AudioProcessor(feature_extractor_path=args.whisper_dir)
    weight_dtype = unet.model.dtype
    whisper = WhisperModel.from_pretrained(args.whisper_dir)
    whisper = whisper.to(device=device, dtype=weight_dtype).eval()
    whisper.requires_grad_(False)
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åˆ›å»ºAvatarå¹¶æ‰§è¡Œæ¨ç†
    avatar = OfficialMuseTalkAvatar(
        template_id=args.template_id,
        cache_dir=args.cache_dir,
        device=device,
        batch_size=args.batch_size
    )
    
    models = (vae, unet, pe, audio_processor, whisper)
    result_path = avatar.inference(args.audio_path, args.output_path, args.fps, models)
    print(f"ğŸ‰ æ¨ç†å®Œæˆ: {result_path}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœåŠ¡å¯åŠ¨å™¨ - æ ¹æ®GPUé…ç½®é€‰æ‹©æœ€ä½³æœåŠ¡
"""

import os
import sys
import torch
import argparse

def detect_gpu_config():
    """æ£€æµ‹GPUé…ç½®"""
    if not torch.cuda.is_available():
        return "cpu", 0, 0
    
    gpu_count = torch.cuda.device_count()
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    print(f"ğŸ” æ£€æµ‹åˆ°GPUé…ç½®:")
    print(f"   GPUæ•°é‡: {gpu_count}")
    print(f"   GPUå‹å·: {gpu_name}")
    print(f"   æ˜¾å­˜å¤§å°: {gpu_memory:.1f}GB")
    
    return gpu_name, gpu_count, gpu_memory

def main():
    parser = argparse.ArgumentParser(description='MuseTalkæœåŠ¡å¯åŠ¨å™¨')
    parser.add_argument('--port', type=int, default=28888, help='æœåŠ¡ç«¯å£')
    parser.add_argument('--service', type=str, choices=['auto', 'single', 'multi', 'ultra'], 
                       default='auto', help='æœåŠ¡ç±»å‹')
    args = parser.parse_args()
    
    gpu_name, gpu_count, gpu_memory = detect_gpu_config()
    
    # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æœåŠ¡
    if args.service == 'auto':
        if gpu_count == 0:
            print("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œæ— æ³•è¿è¡Œ")
            sys.exit(1)
        elif gpu_count == 1:
            if gpu_memory >= 40:  # 40GBä»¥ä¸Šæ˜¾å­˜
                print("âœ… é€‰æ‹©: å•GPUä¼˜åŒ–æœåŠ¡ï¼ˆå¤§æ˜¾å­˜ä¼˜åŒ–ï¼‰")
                service_module = 'single_gpu_optimized_service'
            else:
                print("âœ… é€‰æ‹©: æ ‡å‡†å•GPUæœåŠ¡")
                service_module = 'global_musetalk_service'
        else:
            print(f"âœ… é€‰æ‹©: {gpu_count}GPUå¹¶è¡ŒæœåŠ¡")
            service_module = 'ultra_fast_realtime_inference_v2'
    else:
        service_map = {
            'single': 'single_gpu_optimized_service',
            'multi': 'ultra_fast_realtime_inference_v2',
            'ultra': 'ultra_fast_realtime_inference_v2'
        }
        service_module = service_map[args.service]
        print(f"âœ… æ‰‹åŠ¨é€‰æ‹©: {service_module}")
    
    # å¯¼å…¥å¹¶å¯åŠ¨æœåŠ¡
    try:
        if service_module == 'single_gpu_optimized_service':
            from single_gpu_optimized_service import start_optimized_service
            start_optimized_service(args.port)
        elif service_module == 'ultra_fast_realtime_inference_v2':
            from ultra_fast_realtime_inference_v2 import start_ultra_fast_service
            start_ultra_fast_service(args.port)
        elif service_module == 'global_musetalk_service':
            from global_musetalk_service import start_global_service
            start_global_service(args.port)
        else:
            print(f"âŒ æœªçŸ¥æœåŠ¡: {service_module}")
            sys.exit(1)
            
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥æœåŠ¡æ¨¡å—: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
çœŸÂ·ä¼ªå®æ—¶å¤„ç†å™¨ - æ ¸å¿ƒå®ç°
ç›®æ ‡ï¼š1-2ç§’å»¶è¿Ÿçš„æµç•…å¯¹è¯ä½“éªŒ
"""

import os
import sys
import time
import json
import asyncio
import threading
import queue
import librosa
import numpy as np
import soundfile as sf
from pathlib import Path
from typing import Dict, List, Optional, Tuple, AsyncGenerator
from concurrent.futures import ThreadPoolExecutor
from collections import deque

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class RealtimeSegmentProcessor:
    """çœŸå®æ—¶åˆ†æ®µå¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        self.service = None
        self.template_cache = {}
        
        # ä¼˜åŒ–çš„åˆ†æ®µå‚æ•°
        self.optimal_segment_duration = 1.0  # æœ€ä¼˜1ç§’åˆ†æ®µï¼ˆå¹³è¡¡å»¶è¿Ÿå’Œè´¨é‡ï¼‰
        self.max_segment_duration = 1.5      # æœ€é•¿1.5ç§’
        self.min_segment_duration = 0.3      # æœ€çŸ­0.3ç§’ï¼ˆé¿å…å¤ªç¢ç‰‡åŒ–ï¼‰
        
        # é¢„å¤„ç†ç¼“å†²åŒº
        self.audio_buffer = deque(maxlen=100)  # éŸ³é¢‘ç¼“å†²
        self.result_queue = asyncio.Queue()    # ç»“æœé˜Ÿåˆ—
        
        # å¹¶è¡Œå¤„ç†çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # æ€§èƒ½ç›‘æ§
        self.metrics = {
            'segment_count': 0,
            'total_process_time': 0,
            'average_latency': 0
        }
        
    def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡ - é¢„åŠ è½½æ¨¡å‹"""
        from offline.batch_inference import UltraFastMuseTalkService
        
        print("ğŸš€ åˆå§‹åŒ–å®æ—¶å¤„ç†å™¨...")
        self.service = UltraFastMuseTalkService()
        self.service.initialize_models()
        
        # é¢„çƒ­æ¨¡å‹ï¼ˆå‡å°‘é¦–æ¬¡æ¨ç†å»¶è¿Ÿï¼‰
        self._warmup_models()
        print("âœ… å®æ—¶å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _warmup_models(self):
        """é¢„çƒ­æ¨¡å‹ - å‡å°‘é¦–æ¬¡æ¨ç†å»¶è¿Ÿ"""
        try:
            print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
            # åˆ›å»ºä¸€ä¸ªçŸ­éŸ³é¢‘è¿›è¡Œé¢„çƒ­
            dummy_audio = np.zeros(16000)  # 1ç§’é™éŸ³
            dummy_path = "/tmp/warmup.wav"
            sf.write(dummy_path, dummy_audio, 16000)
            
            # æ‰§è¡Œä¸€æ¬¡æ¨ç†é¢„çƒ­GPU
            if hasattr(self.service, 'extract_audio_features_ultra_fast'):
                self.service.extract_audio_features_ultra_fast(dummy_path, 25)
            
            os.remove(dummy_path)
            print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
    
    def smart_segment_audio(self, audio_data: np.ndarray, sr: int = 16000) -> List[Dict]:
        """æ™ºèƒ½éŸ³é¢‘åˆ†æ®µ - åŸºäºè¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
        segments = []
        
        # ä½¿ç”¨librosaçš„onset detectionæ‰¾åˆ°è¯­éŸ³æ®µè½
        onset_frames = librosa.onset.onset_detect(
            y=audio_data, 
            sr=sr, 
            units='samples',
            backtrack=True
        )
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°onsetï¼Œä½¿ç”¨å›ºå®šåˆ†æ®µ
        if len(onset_frames) == 0:
            return self._fixed_segment_audio(audio_data, sr)
        
        # åŸºäºonsetæ™ºèƒ½åˆ†æ®µ
        current_start = 0
        segment_index = 0
        
        for i, onset in enumerate(onset_frames):
            segment_duration = (onset - current_start) / sr
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
            if segment_duration >= self.optimal_segment_duration:
                # åˆ›å»ºæ®µ
                segment_data = audio_data[current_start:onset]
                if len(segment_data) > int(self.min_segment_duration * sr):
                    segments.append(self._create_segment(
                        segment_data, sr, segment_index, current_start / sr
                    ))
                    segment_index += 1
                    current_start = onset
            elif segment_duration >= self.max_segment_duration:
                # å¼ºåˆ¶åˆ†æ®µï¼ˆé¿å…å¤ªé•¿ï¼‰
                end_pos = current_start + int(self.max_segment_duration * sr)
                segment_data = audio_data[current_start:end_pos]
                segments.append(self._create_segment(
                    segment_data, sr, segment_index, current_start / sr
                ))
                segment_index += 1
                current_start = end_pos
        
        # å¤„ç†æœ€åä¸€æ®µ
        if current_start < len(audio_data):
            segment_data = audio_data[current_start:]
            if len(segment_data) > int(self.min_segment_duration * sr):
                segments.append(self._create_segment(
                    segment_data, sr, segment_index, current_start / sr
                ))
        
        return segments
    
    def _fixed_segment_audio(self, audio_data: np.ndarray, sr: int) -> List[Dict]:
        """å›ºå®šé•¿åº¦åˆ†æ®µï¼ˆåå¤‡æ–¹æ¡ˆï¼‰"""
        segments = []
        segment_samples = int(self.optimal_segment_duration * sr)
        segment_index = 0
        
        for i in range(0, len(audio_data), segment_samples):
            segment_data = audio_data[i:i + segment_samples]
            if len(segment_data) > int(self.min_segment_duration * sr):
                segments.append(self._create_segment(
                    segment_data, sr, segment_index, i / sr
                ))
                segment_index += 1
        
        return segments
    
    def _create_segment(self, audio_data: np.ndarray, sr: int, index: int, start_time: float) -> Dict:
        """åˆ›å»ºéŸ³é¢‘æ®µä¿¡æ¯"""
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        segment_path = f"/tmp/rt_segment_{index}_{int(time.time()*1000)}.wav"
        sf.write(segment_path, audio_data, sr)
        
        duration = len(audio_data) / sr
        frames = int(duration * 25)  # 25fps
        
        return {
            'index': index,
            'path': segment_path,
            'start_time': start_time,
            'duration': duration,
            'frames': frames,
            'audio_data': audio_data  # ä¿ç•™åŸå§‹æ•°æ®ç”¨äºå¿«é€Ÿå¤„ç†
        }
    
    async def process_segment_async(
        self,
        template_id: str,
        segment_info: Dict,
        priority: int = 0
    ) -> Dict:
        """å¼‚æ­¥å¤„ç†å•ä¸ªéŸ³é¢‘ç‰‡æ®µ - ä¼˜åŒ–ç‰ˆ"""
        start_time = time.time()
        
        try:
            # åŠ¨æ€è°ƒæ•´æ¨ç†å‚æ•°
            num_frames = segment_info['frames']
            
            # æé€Ÿæ¨¡å¼ï¼šçŸ­æ®µç”¨å°batchï¼Œé•¿æ®µè·³å¸§
            if num_frames <= 12:  # 0.5ç§’ä»¥å†…
                batch_size = num_frames
                skip_frames = 1
            elif num_frames <= 25:  # 1ç§’ä»¥å†…
                batch_size = 12
                skip_frames = 2
            else:  # 1ç§’ä»¥ä¸Š
                batch_size = 8
                skip_frames = 3
            
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            output_path = f"/tmp/rt_video_{template_id}_{segment_info['index']}_{int(time.time()*1000)}.mp4"
            
            # è·å–ç¼“å­˜ç›®å½•
            cache_dir = os.environ.get('MUSE_TEMPLATE_CACHE_DIR', '/opt/musetalk/template_cache')
            
            # å¼‚æ­¥æ‰§è¡Œæ¨ç†
            loop = asyncio.get_event_loop()
            success = await loop.run_in_executor(
                self.executor,
                self._run_inference,
                template_id,
                segment_info['path'],
                output_path,
                cache_dir,
                batch_size,
                skip_frames
            )
            
            process_time = time.time() - start_time
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.metrics['segment_count'] += 1
            self.metrics['total_process_time'] += process_time
            self.metrics['average_latency'] = self.metrics['total_process_time'] / self.metrics['segment_count']
            
            result = {
                'success': success,
                'index': segment_info['index'],
                'path': output_path if success else None,
                'duration': segment_info['duration'],
                'frames': num_frames,
                'process_time': process_time,
                'start_time': segment_info['start_time'],
                'latency': process_time * 1000  # æ¯«ç§’
            }
            
            # æ€§èƒ½æ—¥å¿—
            if process_time > 1.5:
                print(f"âš ï¸ æ®µ {segment_info['index']} å¤„ç†è¾ƒæ…¢: {process_time:.2f}ç§’")
            else:
                print(f"âš¡ æ®µ {segment_info['index']} å¿«é€Ÿå®Œæˆ: {process_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            print(f"âŒ æ®µå¤„ç†å¼‚å¸¸: {e}")
            return {
                'success': False,
                'index': segment_info['index'],
                'error': str(e),
                'process_time': time.time() - start_time
            }
    
    def _run_inference(self, template_id, audio_path, output_path, cache_dir, batch_size, skip_frames):
        """è¿è¡Œæ¨ç†ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        return self.service.ultra_fast_inference_parallel(
            template_id=template_id,
            audio_path=audio_path,
            output_path=output_path,
            cache_dir=cache_dir,
            batch_size=batch_size,
            skip_frames=skip_frames,
            streaming=True,
            auto_adjust=True  # è‡ªåŠ¨è°ƒæ•´batch_sizeé¿å…OOM
        )
    
    async def process_audio_stream(
        self,
        template_id: str,
        audio_stream: AsyncGenerator,
        callback=None
    ):
        """å¤„ç†å®æ—¶éŸ³é¢‘æµ"""
        print("ğŸ™ï¸ å¼€å§‹å¤„ç†å®æ—¶éŸ³é¢‘æµ...")
        
        # éŸ³é¢‘ç¼“å†²åŒº
        audio_buffer = []
        buffer_duration = 0
        segment_tasks = []
        
        async for audio_chunk in audio_stream:
            # ç´¯ç§¯éŸ³é¢‘
            audio_buffer.append(audio_chunk)
            buffer_duration += len(audio_chunk) / 16000  # å‡è®¾16kHz
            
            # å½“ç¼“å†²åŒºè¾¾åˆ°æœ€ä¼˜é•¿åº¦æ—¶å¤„ç†
            if buffer_duration >= self.optimal_segment_duration:
                # åˆå¹¶éŸ³é¢‘
                audio_data = np.concatenate(audio_buffer)
                
                # åˆ›å»ºæ®µ
                segment = self._create_segment(
                    audio_data, 
                    16000, 
                    len(segment_tasks),
                    len(segment_tasks) * self.optimal_segment_duration
                )
                
                # å¼‚æ­¥å¤„ç†ï¼ˆä¸é˜»å¡ï¼‰
                task = asyncio.create_task(
                    self.process_segment_async(template_id, segment)
                )
                segment_tasks.append(task)
                
                # æ¸…ç©ºç¼“å†²åŒº
                audio_buffer = []
                buffer_duration = 0
                
                # æ£€æŸ¥å®Œæˆçš„ä»»åŠ¡
                for task in segment_tasks:
                    if task.done():
                        result = await task
                        if callback and result['success']:
                            await callback(result)
        
        # å¤„ç†å‰©ä½™éŸ³é¢‘
        if audio_buffer:
            audio_data = np.concatenate(audio_buffer)
            if len(audio_data) > int(self.min_segment_duration * 16000):
                segment = self._create_segment(
                    audio_data, 16000, len(segment_tasks), 
                    len(segment_tasks) * self.optimal_segment_duration
                )
                result = await self.process_segment_async(template_id, segment)
                if callback and result['success']:
                    await callback(result)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if segment_tasks:
            results = await asyncio.gather(*segment_tasks)
            print(f"âœ… å¤„ç†å®Œæˆ: {len(results)}æ®µ, å¹³å‡å»¶è¿Ÿ: {self.metrics['average_latency']:.1f}ms")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import glob
            for pattern in ["/tmp/rt_segment_*.wav", "/tmp/rt_video_*.mp4"]:
                for file in glob.glob(pattern):
                    try:
                        os.remove(file)
                    except:
                        pass
            
            # å…³é—­çº¿ç¨‹æ± 
            self.executor.shutdown(wait=False)
            
            print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å¤±è´¥: {e}")


class StreamingPipeline:
    """å®Œæ•´çš„æµå¼å¤„ç†ç®¡é“"""
    
    def __init__(self):
        self.processor = RealtimeSegmentProcessor()
        self.whisper_service = None  # WhisperNetæœåŠ¡
        self.websocket_clients = {}
        
    async def initialize(self):
        """åˆå§‹åŒ–ç®¡é“"""
        print("ğŸš€ åˆå§‹åŒ–æµå¼ç®¡é“...")
        self.processor.initialize()
        # TODO: åˆå§‹åŒ–WhisperNet
        print("âœ… æµå¼ç®¡é“å°±ç»ª")
    
    async def handle_realtime_conversation(
        self,
        websocket,
        template_id: str
    ):
        """å¤„ç†å®æ—¶å¯¹è¯"""
        client_id = id(websocket)
        self.websocket_clients[client_id] = websocket
        
        try:
            print(f"ğŸ‘¤ å®¢æˆ·ç«¯ {client_id} è¿æ¥")
            
            # å‘é€å°±ç»ªä¿¡å·
            await websocket.send(json.dumps({
                'type': 'ready',
                'message': 'ç³»ç»Ÿå°±ç»ªï¼Œè¯·å¼€å§‹è¯´è¯'
            }))
            
            # å¤„ç†æ¶ˆæ¯
            async for message in websocket:
                data = json.loads(message)
                
                if data['type'] == 'audio_chunk':
                    # å¤„ç†éŸ³é¢‘å—
                    audio_data = np.frombuffer(
                        bytes.fromhex(data['audio']), 
                        dtype=np.float32
                    )
                    
                    # åˆ›å»ºéŸ³é¢‘æµç”Ÿæˆå™¨
                    async def audio_generator():
                        yield audio_data
                    
                    # å¤„ç†å¹¶å‘é€ç»“æœ
                    await self.processor.process_audio_stream(
                        template_id,
                        audio_generator(),
                        callback=lambda r: self._send_result(websocket, r)
                    )
                    
                elif data['type'] == 'complete_audio':
                    # å¤„ç†å®Œæ•´éŸ³é¢‘
                    audio_path = data['audio_path']
                    await self._process_complete_audio(
                        websocket, template_id, audio_path
                    )
                    
        except Exception as e:
            print(f"âŒ å®¢æˆ·ç«¯ {client_id} é”™è¯¯: {e}")
        finally:
            del self.websocket_clients[client_id]
            print(f"ğŸ‘‹ å®¢æˆ·ç«¯ {client_id} æ–­å¼€")
    
    async def _send_result(self, websocket, result):
        """å‘é€å¤„ç†ç»“æœ"""
        await websocket.send(json.dumps({
            'type': 'video_segment',
            'data': {
                'index': result['index'],
                'video_url': f"/videos/{Path(result['path']).name}",
                'duration': result['duration'],
                'latency': result['latency'],
                'timestamp': time.time()
            }
        }))
    
    async def _process_complete_audio(self, websocket, template_id, audio_path):
        """å¤„ç†å®Œæ•´éŸ³é¢‘æ–‡ä»¶"""
        # åŠ è½½éŸ³é¢‘
        audio_data, sr = librosa.load(audio_path, sr=16000)
        
        # æ™ºèƒ½åˆ†æ®µ
        segments = self.processor.smart_segment_audio(audio_data, sr)
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ®µ
        tasks = []
        for segment in segments:
            task = asyncio.create_task(
                self.processor.process_segment_async(template_id, segment)
            )
            tasks.append(task)
        
        # é€ä¸ªå‘é€ç»“æœï¼ˆä¿æŒé¡ºåºï¼‰
        for task in tasks:
            result = await task
            if result['success']:
                await self._send_result(websocket, result)
        
        # å‘é€å®Œæˆä¿¡å·
        await websocket.send(json.dumps({
            'type': 'complete',
            'message': 'å¤„ç†å®Œæˆ',
            'metrics': self.processor.metrics
        }))


# WebSocketæœåŠ¡å™¨å…¥å£
async def start_websocket_server(host='0.0.0.0', port=8766):
    """å¯åŠ¨WebSocketæœåŠ¡å™¨"""
    import websockets
    
    pipeline = StreamingPipeline()
    await pipeline.initialize()
    
    async def handler(websocket, path):
        # ä»è·¯å¾„è·å–template_id
        template_id = path.strip('/') or 'default'
        await pipeline.handle_realtime_conversation(websocket, template_id)
    
    print(f"ğŸŒ WebSocketæœåŠ¡å™¨å¯åŠ¨: ws://{host}:{port}")
    async with websockets.serve(handler, host, port):
        await asyncio.Future()  # æ°¸è¿œè¿è¡Œ


if __name__ == "__main__":
    # æµ‹è¯•å…¥å£
    asyncio.run(start_websocket_server())
#!/usr/bin/env python3
"""
é›†æˆåŒ–æµå¼å¤„ç†ç®¡é“
æ•´åˆWhisperNet ASR + LLM + TTS + MuseTalk
å®ç°ç«¯åˆ°ç«¯çš„ä¼ªå®æ—¶å¯¹è¯
"""

import os
import sys
import time
import json
import asyncio
import numpy as np
import websockets
from typing import Dict, Optional, AsyncGenerator
from concurrent.futures import ThreadPoolExecutor
import queue
import threading

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from streaming.realtime_processor import RealtimeSegmentProcessor


class IntegratedRealtimePipeline:
    """é›†æˆçš„å®æ—¶å¤„ç†ç®¡é“"""
    
    def __init__(self):
        # æ ¸å¿ƒç»„ä»¶
        self.musetalk_processor = RealtimeSegmentProcessor()
        
        # è¿æ¥é…ç½®
        self.csharp_api_url = "http://lmy-digitalhuman:5000"  # C#æœåŠ¡åœ°å€
        self.whisper_endpoint = f"{self.csharp_api_url}/api/whisper/stream"
        self.llm_endpoint = f"{self.csharp_api_url}/api/llm/stream"
        self.tts_endpoint = f"{self.csharp_api_url}/api/tts/stream"
        
        # å¤„ç†é˜Ÿåˆ—
        self.audio_queue = asyncio.Queue(maxsize=10)
        self.text_queue = asyncio.Queue(maxsize=10)
        self.tts_queue = asyncio.Queue(maxsize=10)
        self.video_queue = asyncio.Queue(maxsize=10)
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # æ€§èƒ½ç›‘æ§
        self.metrics = {
            'asr_latency': [],
            'llm_latency': [],
            'tts_latency': [],
            'musetalk_latency': [],
            'total_latency': []
        }
        
        # å½“å‰ä¼šè¯çŠ¶æ€
        self.current_session = None
        self.template_id = None
        
    async def initialize(self, template_id: str):
        """åˆå§‹åŒ–ç®¡é“"""
        print("ğŸš€ åˆå§‹åŒ–é›†æˆç®¡é“...")
        self.template_id = template_id
        
        # åˆå§‹åŒ–MuseTalk
        await asyncio.get_event_loop().run_in_executor(
            self.executor,
            self.musetalk_processor.initialize
        )
        
        print("âœ… é›†æˆç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    async def process_user_audio_stream(
        self,
        audio_stream: AsyncGenerator,
        websocket=None
    ):
        """å¤„ç†ç”¨æˆ·éŸ³é¢‘æµ - å®Œæ•´çš„å¯¹è¯æµç¨‹"""
        print("ğŸ™ï¸ å¼€å§‹å¤„ç†ç”¨æˆ·éŸ³é¢‘...")
        
        # å¯åŠ¨å¹¶è¡Œå¤„ç†ä»»åŠ¡
        tasks = [
            asyncio.create_task(self._asr_task(audio_stream)),
            asyncio.create_task(self._llm_task()),
            asyncio.create_task(self._tts_task()),
            asyncio.create_task(self._musetalk_task()),
            asyncio.create_task(self._output_task(websocket))
        ]
        
        try:
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡
            await asyncio.gather(*tasks)
        except Exception as e:
            print(f"âŒ ç®¡é“å¤„ç†é”™è¯¯: {e}")
            # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
            for task in tasks:
                task.cancel()
    
    async def _asr_task(self, audio_stream: AsyncGenerator):
        """ASRä»»åŠ¡ - è¯­éŸ³è¯†åˆ«"""
        print("ğŸ¤ ASRä»»åŠ¡å¯åŠ¨")
        
        # éŸ³é¢‘ç¼“å†²
        audio_buffer = []
        buffer_duration = 0
        
        async for audio_chunk in audio_stream:
            start_time = time.time()
            
            # ç´¯ç§¯éŸ³é¢‘
            audio_buffer.append(audio_chunk)
            buffer_duration += len(audio_chunk) / 16000
            
            # æ¯0.5ç§’è¯†åˆ«ä¸€æ¬¡ï¼ˆå¹³è¡¡å®æ—¶æ€§å’Œå‡†ç¡®æ€§ï¼‰
            if buffer_duration >= 0.5:
                # åˆå¹¶éŸ³é¢‘
                audio_data = np.concatenate(audio_buffer)
                
                # è°ƒç”¨WhisperNetï¼ˆé€šè¿‡C#æœåŠ¡ï¼‰
                text = await self._call_whisper(audio_data)
                
                if text:
                    # æ”¾å…¥æ–‡æœ¬é˜Ÿåˆ—
                    await self.text_queue.put({
                        'text': text,
                        'timestamp': time.time()
                    })
                    
                    # è®°å½•å»¶è¿Ÿ
                    latency = (time.time() - start_time) * 1000
                    self.metrics['asr_latency'].append(latency)
                    print(f"ğŸ“ è¯†åˆ«: {text} (å»¶è¿Ÿ: {latency:.0f}ms)")
                
                # æ¸…ç©ºç¼“å†²
                audio_buffer = []
                buffer_duration = 0
    
    async def _llm_task(self):
        """LLMä»»åŠ¡ - ç”Ÿæˆå›å¤"""
        print("ğŸ¤– LLMä»»åŠ¡å¯åŠ¨")
        
        sentence_buffer = []
        
        while True:
            try:
                # è·å–è¯†åˆ«æ–‡æœ¬
                text_data = await self.text_queue.get()
                start_time = time.time()
                
                # ç´¯ç§¯æˆå¥å­
                sentence_buffer.append(text_data['text'])
                
                # æ£€æµ‹å¥å­ç»“æŸï¼ˆç®€å•è§„åˆ™ï¼‰
                if any(punct in text_data['text'] for punct in ['ã€‚', 'ï¼Ÿ', 'ï¼', '.', '?', '!']):
                    sentence = ''.join(sentence_buffer)
                    sentence_buffer = []
                    
                    # è°ƒç”¨LLMç”Ÿæˆå›å¤ï¼ˆæµå¼ï¼‰
                    async for response_chunk in self._call_llm_stream(sentence):
                        # æ”¾å…¥TTSé˜Ÿåˆ—
                        await self.tts_queue.put({
                            'text': response_chunk,
                            'timestamp': time.time()
                        })
                        
                        # è®°å½•é¦–å­—å»¶è¿Ÿ
                        if len(self.metrics['llm_latency']) == 0:
                            latency = (time.time() - start_time) * 1000
                            self.metrics['llm_latency'].append(latency)
                            print(f"ğŸ’¬ LLMé¦–å­—: {latency:.0f}ms")
                            
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"âŒ LLMé”™è¯¯: {e}")
    
    async def _tts_task(self):
        """TTSä»»åŠ¡ - è¯­éŸ³åˆæˆ"""
        print("ğŸ”Š TTSä»»åŠ¡å¯åŠ¨")
        
        text_buffer = []
        buffer_chars = 0
        
        while True:
            try:
                # è·å–æ–‡æœ¬
                text_data = await self.tts_queue.get()
                start_time = time.time()
                
                # ç´¯ç§¯æ–‡æœ¬
                text_buffer.append(text_data['text'])
                buffer_chars += len(text_data['text'])
                
                # æ¯10ä¸ªå­—æˆ–é‡åˆ°æ ‡ç‚¹å°±åˆæˆ
                if buffer_chars >= 10 or any(p in text_data['text'] for p in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']):
                    text_to_synthesize = ''.join(text_buffer)
                    text_buffer = []
                    buffer_chars = 0
                    
                    # è°ƒç”¨TTS
                    audio_path = await self._call_tts(text_to_synthesize)
                    
                    if audio_path:
                        # æ”¾å…¥è§†é¢‘ç”Ÿæˆé˜Ÿåˆ—
                        await self.video_queue.put({
                            'audio_path': audio_path,
                            'text': text_to_synthesize,
                            'timestamp': time.time()
                        })
                        
                        # è®°å½•å»¶è¿Ÿ
                        latency = (time.time() - start_time) * 1000
                        self.metrics['tts_latency'].append(latency)
                        print(f"ğŸµ TTSå®Œæˆ: {latency:.0f}ms")
                        
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"âŒ TTSé”™è¯¯: {e}")
    
    async def _musetalk_task(self):
        """MuseTalkä»»åŠ¡ - ç”Ÿæˆæ•°å­—äººè§†é¢‘"""
        print("ğŸ­ MuseTalkä»»åŠ¡å¯åŠ¨")
        
        segment_index = 0
        
        while True:
            try:
                # è·å–éŸ³é¢‘
                audio_data = await self.video_queue.get()
                start_time = time.time()
                
                # åˆ›å»ºæ®µä¿¡æ¯
                segment_info = {
                    'index': segment_index,
                    'path': audio_data['audio_path'],
                    'start_time': segment_index,
                    'duration': 1.0,  # ä¼°ç®—
                    'frames': 25  # ä¼°ç®—
                }
                
                # å¤„ç†è§†é¢‘
                result = await self.musetalk_processor.process_segment_async(
                    self.template_id,
                    segment_info,
                    priority=segment_index
                )
                
                if result['success']:
                    # è®°å½•å»¶è¿Ÿ
                    latency = (time.time() - start_time) * 1000
                    self.metrics['musetalk_latency'].append(latency)
                    print(f"ğŸ¬ è§†é¢‘ç”Ÿæˆ: {latency:.0f}ms")
                    
                    # æ·»åŠ åˆ°è¾“å‡º
                    result['text'] = audio_data['text']
                    await self.video_queue.put(result)
                
                segment_index += 1
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"âŒ MuseTalké”™è¯¯: {e}")
    
    async def _output_task(self, websocket):
        """è¾“å‡ºä»»åŠ¡ - å‘é€ç»“æœåˆ°å®¢æˆ·ç«¯"""
        print("ğŸ“¤ è¾“å‡ºä»»åŠ¡å¯åŠ¨")
        
        if not websocket:
            return
        
        while True:
            try:
                # è·å–è§†é¢‘ç»“æœ
                result = await self.video_queue.get()
                
                # å‘é€åˆ°å®¢æˆ·ç«¯
                await websocket.send(json.dumps({
                    'type': 'video_segment',
                    'data': {
                        'index': result['index'],
                        'video_path': result['path'],
                        'text': result.get('text', ''),
                        'latency': result.get('latency', 0),
                        'timestamp': time.time()
                    }
                }))
                
                # è®¡ç®—æ€»å»¶è¿Ÿ
                total_latency = sum([
                    np.mean(self.metrics['asr_latency']) if self.metrics['asr_latency'] else 0,
                    np.mean(self.metrics['llm_latency']) if self.metrics['llm_latency'] else 0,
                    np.mean(self.metrics['tts_latency']) if self.metrics['tts_latency'] else 0,
                    np.mean(self.metrics['musetalk_latency']) if self.metrics['musetalk_latency'] else 0
                ])
                
                print(f"â±ï¸ ç«¯åˆ°ç«¯å»¶è¿Ÿ: {total_latency:.0f}ms")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"âŒ è¾“å‡ºé”™è¯¯: {e}")
    
    async def _call_whisper(self, audio_data: np.ndarray) -> Optional[str]:
        """è°ƒç”¨WhisperNetæœåŠ¡"""
        # TODO: å®é™…è°ƒç”¨C#çš„WhisperNetæœåŠ¡
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿ200mså»¶è¿Ÿ
        return "ä½ å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ"
    
    async def _call_llm_stream(self, text: str) -> AsyncGenerator:
        """è°ƒç”¨LLMæœåŠ¡ï¼ˆæµå¼ï¼‰"""
        # TODO: å®é™…è°ƒç”¨C#çš„LLMæœåŠ¡
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        response = "éå¸¸æ„Ÿè°¢æ‚¨çš„æé—®ã€‚æˆ‘æ˜¯æ‚¨çš„æ•°å­—äººåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"
        
        # æ¨¡æ‹Ÿæµå¼è¿”å›
        for i in range(0, len(response), 5):
            await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿ50mså»¶è¿Ÿ
            yield response[i:i+5]
    
    async def _call_tts(self, text: str) -> Optional[str]:
        """è°ƒç”¨TTSæœåŠ¡"""
        # TODO: å®é™…è°ƒç”¨C#çš„TTSæœåŠ¡
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿè·¯å¾„
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿ100mså»¶è¿Ÿ
        return f"/tmp/tts_audio_{int(time.time()*1000)}.wav"
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'asr_avg_latency': np.mean(self.metrics['asr_latency']) if self.metrics['asr_latency'] else 0,
            'llm_first_token_latency': self.metrics['llm_latency'][0] if self.metrics['llm_latency'] else 0,
            'tts_avg_latency': np.mean(self.metrics['tts_latency']) if self.metrics['tts_latency'] else 0,
            'musetalk_avg_latency': np.mean(self.metrics['musetalk_latency']) if self.metrics['musetalk_latency'] else 0,
            'total_segments_processed': len(self.metrics['musetalk_latency']),
            'estimated_total_latency': sum([
                np.mean(self.metrics['asr_latency']) if self.metrics['asr_latency'] else 0,
                self.metrics['llm_latency'][0] if self.metrics['llm_latency'] else 0,
                np.mean(self.metrics['tts_latency']) if self.metrics['tts_latency'] else 0,
                np.mean(self.metrics['musetalk_latency']) if self.metrics['musetalk_latency'] else 0
            ])
        }


class RealtimeWebSocketServer:
    """å®æ—¶WebSocketæœåŠ¡å™¨"""
    
    def __init__(self):
        self.pipeline = IntegratedRealtimePipeline()
        self.clients = {}
        
    async def handle_client(self, websocket, path):
        """å¤„ç†å®¢æˆ·ç«¯è¿æ¥"""
        client_id = id(websocket)
        self.clients[client_id] = websocket
        
        try:
            print(f"ğŸ‘¤ å®¢æˆ·ç«¯ {client_id} è¿æ¥")
            
            # è·å–template_id
            template_id = path.strip('/') or 'default'
            
            # åˆå§‹åŒ–ç®¡é“
            await self.pipeline.initialize(template_id)
            
            # å‘é€å°±ç»ªä¿¡å·
            await websocket.send(json.dumps({
                'type': 'ready',
                'message': 'ç³»ç»Ÿå°±ç»ª',
                'template_id': template_id
            }))
            
            # å¤„ç†æ¶ˆæ¯
            async for message in websocket:
                data = json.loads(message)
                
                if data['type'] == 'start_conversation':
                    # å¼€å§‹å¯¹è¯
                    print("ğŸ¬ å¼€å§‹å®æ—¶å¯¹è¯")
                    
                    # åˆ›å»ºéŸ³é¢‘æµç”Ÿæˆå™¨
                    async def audio_generator():
                        while True:
                            msg = await websocket.recv()
                            msg_data = json.loads(msg)
                            if msg_data['type'] == 'audio_chunk':
                                audio_bytes = bytes.fromhex(msg_data['audio'])
                                audio_data = np.frombuffer(audio_bytes, dtype=np.float32)
                                yield audio_data
                            elif msg_data['type'] == 'end_audio':
                                break
                    
                    # å¤„ç†éŸ³é¢‘æµ
                    await self.pipeline.process_user_audio_stream(
                        audio_generator(),
                        websocket
                    )
                    
                elif data['type'] == 'get_metrics':
                    # è¿”å›æ€§èƒ½æŒ‡æ ‡
                    metrics = self.pipeline.get_performance_report()
                    await websocket.send(json.dumps({
                        'type': 'metrics',
                        'data': metrics
                    }))
                    
        except websockets.ConnectionClosed:
            print(f"ğŸ‘‹ å®¢æˆ·ç«¯ {client_id} æ–­å¼€")
        except Exception as e:
            print(f"âŒ å®¢æˆ·ç«¯ {client_id} é”™è¯¯: {e}")
        finally:
            if client_id in self.clients:
                del self.clients[client_id]
    
    async def start(self, host='0.0.0.0', port=8766):
        """å¯åŠ¨æœåŠ¡å™¨"""
        print(f"ğŸŒ å®æ—¶WebSocketæœåŠ¡å™¨å¯åŠ¨: ws://{host}:{port}")
        async with websockets.serve(self.handle_client, host, port):
            await asyncio.Future()


# ä¸»å…¥å£
async def main():
    """ä¸»å‡½æ•°"""
    server = RealtimeWebSocketServer()
    await server.start()


if __name__ == "__main__":
    asyncio.run(main())
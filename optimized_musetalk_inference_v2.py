#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MuseTalkæè‡´æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ V2
è§£å†³è„¸éƒ¨ç°è‰²é—®é¢˜å’Œæ€§èƒ½ç“¶é¢ˆ

å…³é”®ä¼˜åŒ–ï¼š
1. çœŸæ­£çš„æ¨¡å‹æŒä¹…åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
2. ä¿®å¤é¢éƒ¨blendingé—®é¢˜
3. æé€Ÿæ¨ç†ï¼Œä½¿ç”¨é¢„å¤„ç†ç¼“å­˜
"""

import argparse
import os
import json
import time
import threading
import queue
import multiprocessing as mp
from pathlib import Path
from omegaconf import OmegaConf
import numpy as np
import cv2
import torch
import glob
import pickle
import copy
from tqdm import tqdm
from transformers import WhisperModel

from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.utils import datagen, load_all_model
from musetalk.utils.preprocessing import get_landmark_and_bbox, read_imgs
from musetalk.utils.blending import get_image_prepare_material, get_image_blending
from musetalk.utils.audio_processor import AudioProcessor

class PersistentMuseTalkInference:
    """æŒä¹…åŒ–MuseTalkæ¨ç†å™¨ - è§£å†³æ€§èƒ½å’Œè´¨é‡é—®é¢˜"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda:0")
        self.models_loaded = False
        self.templates = {}
        
        # ğŸš€ å…³é”®ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰æŒä¹…åŒ–æ¨¡å‹
        self.model_cache_path = "persistent_models.pkl"
        if os.path.exists(self.model_cache_path):
            print("ğŸš€ å‘ç°æŒä¹…åŒ–æ¨¡å‹ï¼Œæé€ŸåŠ è½½ä¸­...")
            self._load_persistent_models()
        else:
            print("ğŸ”„ é¦–æ¬¡åˆå§‹åŒ–ï¼ŒåŠ è½½æ¨¡å‹åˆ°GPU...")
            self._initialize_models()
            self._save_persistent_models()
            
        print("âœ… æŒä¹…åŒ–MuseTalkæ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰"""
        print("ğŸ® åˆå§‹åŒ–GPUæ¨¡å‹...")
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        self.audio_processor, self.vae, self.unet, self.pe = load_all_model(
            self.config.version, self.config.fp16, self.device
        )
        
        # åˆå§‹åŒ–é¢éƒ¨è§£æå™¨
        self.fp = FaceParsing(device=self.device)
        
        self.models_loaded = True
        print("âœ… GPUæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _save_persistent_models(self):
        """ä¿å­˜æ¨¡å‹çŠ¶æ€ï¼ˆç”¨äºä¸‹æ¬¡å¿«é€Ÿå¯åŠ¨ï¼‰"""
        try:
            model_state = {
                'models_loaded': self.models_loaded,
                'device': str(self.device),
                'config': self.config,
                'saved_at': time.time()
            }
            with open(self.model_cache_path, 'wb') as f:
                pickle.dump(model_state, f)
            print(f"ğŸ’¾ æ¨¡å‹çŠ¶æ€å·²ä¿å­˜: {self.model_cache_path}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ¨¡å‹çŠ¶æ€å¤±è´¥: {e}")
    
    def _load_persistent_models(self):
        """åŠ è½½æŒä¹…åŒ–æ¨¡å‹çŠ¶æ€"""
        try:
            with open(self.model_cache_path, 'rb') as f:
                model_state = pickle.load(f)
            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆå› ä¸ºæ¨¡å‹å¯¹è±¡æ— æ³•åºåˆ—åŒ–ï¼‰
            self._initialize_models()
            print("âœ… æŒä¹…åŒ–æ¨¡å‹çŠ¶æ€åŠ è½½å®Œæˆ")
            return True
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æŒä¹…åŒ–çŠ¶æ€å¤±è´¥ï¼Œå°†é‡æ–°åˆå§‹åŒ–: {e}")
            return False
    
    def preprocess_template(self, template_id, template_path):
        """é¢„å¤„ç†æ¨¡æ¿ï¼ˆæ°¸ä¹…åŒ–ä¿å­˜ï¼‰"""
        cache_dir = os.path.join("model_states", template_id)
        os.makedirs(cache_dir, exist_ok=True)
        cache_file = os.path.join(cache_dir, "template_cache.pkl")
        
        if os.path.exists(cache_file):
            print(f"âœ… åŠ è½½æ¨¡æ¿ç¼“å­˜: {template_id}")
            with open(cache_file, 'rb') as f:
                template_data = pickle.load(f)
            self.templates[template_id] = template_data
            return template_data
        
        print(f"ğŸ”„ é¢„å¤„ç†æ¨¡æ¿: {template_id}")
        
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(template_path)
        if img is None:
            raise ValueError(f"æ— æ³•è¯»å–æ¨¡æ¿å›¾ç‰‡: {template_path}")
        
        # è·å–é¢éƒ¨åæ ‡
        coord_list, frame_list = get_landmark_and_bbox([template_path], self.config.bbox_shift)
        
        # é¢„è®¡ç®—VAEç¼–ç 
        input_latent_list = []
        for bbox, frame in zip(coord_list, frame_list):
            x1, y1, x2, y2 = bbox
            if x1 >= x2 or y1 >= y2:
                continue
                
            h, w = frame.shape[:2]
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w, int(x2)), min(h, int(y2))
            
            if self.config.version == "v15":
                y2 = y2 + self.config.extra_margin
                y2 = min(y2, frame.shape[0])
            
            crop_frame = frame[y1:y2, x1:x2]
            resized_crop_frame = cv2.resize(crop_frame, (256, 256), interpolation=cv2.INTER_LANCZOS4)
            
            with torch.no_grad():
                latents = self.vae.get_latents_for_unet(resized_crop_frame)
                input_latent_list.append(latents)
        
        # åˆ›å»ºå¾ªç¯åˆ—è¡¨
        frame_list_cycle = frame_list + frame_list[::-1]
        coord_list_cycle = coord_list + coord_list[::-1]
        input_latent_list_cycle = input_latent_list + input_latent_list[::-1]
        
        # é¢„è®¡ç®—é¢éƒ¨è§£æ
        mask_coords_list_cycle = []
        mask_list_cycle = []
        
        for i, frame in enumerate(frame_list_cycle):
            x1, y1, x2, y2 = coord_list_cycle[i]
            h, w = frame.shape[:2]
            x1, y1 = max(0, int(x1)), max(0, int(y1))
            x2, y2 = min(w, int(x2)), min(h, int(y2))
            
            if x1 >= x2 or y1 >= y2:
                mask_coords_list_cycle.append([0, 0, 256, 256])
                mask_list_cycle.append(np.ones((256, 256), dtype=np.uint8) * 255)
                continue
            
            try:
                # ğŸ¯ å…³é”®ï¼šä½¿ç”¨æ­£ç¡®çš„é¢éƒ¨è§£ææ¨¡å¼
                mode = self.config.parsing_mode if self.config.version == "v15" else "raw"
                mask, crop_box = get_image_prepare_material(frame, [x1, y1, x2, y2], fp=self.fp, mode=mode)
                mask_coords_list_cycle.append(crop_box)
                mask_list_cycle.append(mask)
            except Exception as e:
                print(f"[WARN] é¢éƒ¨è§£æå¤±è´¥: {e}")
                mask_coords_list_cycle.append([x1, y1, x2, y2])
                mask_list_cycle.append(np.ones((256, 256), dtype=np.uint8) * 255)
        
        # ä¿å­˜æ¨¡æ¿æ•°æ®
        template_data = {
            'template_id': template_id,
            'frame_list_cycle': frame_list_cycle,
            'coord_list_cycle': coord_list_cycle,
            'input_latent_list_cycle': input_latent_list_cycle,
            'mask_coords_list_cycle': mask_coords_list_cycle,
            'mask_list_cycle': mask_list_cycle,
            'preprocessed_at': time.time()
        }
        
        # æ°¸ä¹…åŒ–ä¿å­˜
        with open(cache_file, 'wb') as f:
            pickle.dump(template_data, f)
        
        self.templates[template_id] = template_data
        print(f"âœ… æ¨¡æ¿é¢„å¤„ç†å®Œæˆå¹¶ä¿å­˜: {template_id}")
        return template_data
    
    def inference(self, template_id, audio_path, output_path, fps=25):
        """æé€Ÿæ¨ç†"""
        if template_id not in self.templates:
            template_path = self._find_template_path(template_id)
            self.preprocess_template(template_id, template_path)
        
        template_data = self.templates[template_id]
        
        print(f"ğŸµ æå–éŸ³é¢‘ç‰¹å¾...")
        # éŸ³é¢‘å¤„ç†
        whisper_feature = self.audio_processor.audio2feat(audio_path)
        whisper_chunks = self.audio_processor.feature2chunks(feature_array=whisper_feature, fps=fps)
        
        print(f"ğŸ® å¼€å§‹GPUæ¨ç†...")
        # GPUæ¨ç†
        video_num = len(whisper_chunks)
        res_frame_list = []
        
        # ğŸš€ æ‰¹å¤„ç†æ¨ç†
        batch_size = min(self.config.batch_size * 2, 128)  # ä¼˜åŒ–æ‰¹å¤§å°
        
        for i in tqdm(range(0, video_num, batch_size), desc="GPUæ¨ç†"):
            batch_end = min(i + batch_size, video_num)
            batch_chunks = whisper_chunks[i:batch_end]
            
            # æ‰¹å¤„ç†
            batch_latents = []
            for chunk in batch_chunks:
                frame_idx = i % len(template_data['input_latent_list_cycle'])
                input_latent = template_data['input_latent_list_cycle'][frame_idx]
                batch_latents.append(input_latent)
            
            # GPUæ¨ç†
            with torch.no_grad():
                batch_latents = torch.stack(batch_latents)
                batch_chunks = torch.stack([torch.tensor(chunk) for chunk in batch_chunks])
                
                # ä½¿ç”¨halfç²¾åº¦åŠ é€Ÿ
                if self.config.fp16:
                    batch_latents = batch_latents.half()
                    batch_chunks = batch_chunks.half()
                
                batch_results = self.unet.forward_with_cfg(batch_latents, batch_chunks, self.config.cfg_scale)
                batch_frames = self.vae.decode_latents(batch_results)
                
                for frame in batch_frames:
                    res_frame_list.append(frame)
        
        print(f"ğŸ¬ åˆæˆæœ€ç»ˆè§†é¢‘...")
        # è§†é¢‘åˆæˆ - ä½¿ç”¨ä¼˜åŒ–çš„blending
        final_frames = []
        for i, res_frame in enumerate(tqdm(res_frame_list, desc="è§†é¢‘åˆæˆ")):
            frame_idx = i % len(template_data['frame_list_cycle'])
            ori_frame = template_data['frame_list_cycle'][frame_idx]
            bbox = template_data['coord_list_cycle'][frame_idx]
            mask = template_data['mask_list_cycle'][frame_idx]
            mask_crop_box = template_data['mask_coords_list_cycle'][frame_idx]
            
            # ğŸ¯ å…³é”®ï¼šæ­£ç¡®çš„blendingè°ƒç”¨
            try:
                combine_frame = get_image_blending(ori_frame, res_frame, bbox, mask, mask_crop_box)
                final_frames.append(combine_frame)
            except Exception as e:
                print(f"[WARN] Blendingå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¸§: {e}")
                final_frames.append(ori_frame)
        
        # ä¿å­˜è§†é¢‘
        self._save_video(final_frames, audio_path, output_path, fps)
        print(f"âœ… æ¨ç†å®Œæˆ: {output_path}")
        return output_path
    
    def _find_template_path(self, template_id):
        """æŸ¥æ‰¾æ¨¡æ¿æ–‡ä»¶è·¯å¾„"""
        template_dir = os.getenv('TEMPLATE_DIR', './wwwroot/templates')
        extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        
        for ext in extensions:
            path = os.path.join(template_dir, f"{template_id}{ext}")
            if os.path.exists(path):
                return path
        
        raise FileNotFoundError(f"æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_id}")
    
    def _save_video(self, frames, audio_path, output_path, fps):
        """ä¿å­˜è§†é¢‘æ–‡ä»¶"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯ä»¥ä¿å­˜")
        
        # ä½¿ç”¨cv2.VideoWriterç›´æ¥å†™å…¥
        h, w, c = frames[0].shape
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path.replace('.mp4', '_temp.mp4'), fourcc, fps, (w, h))
        
        for frame in frames:
            out.write(frame)
        out.release()
        
        # åˆå¹¶éŸ³é¢‘
        temp_video = output_path.replace('.mp4', '_temp.mp4')
        cmd = f'ffmpeg -y -i "{temp_video}" -i "{audio_path}" -c:v copy -c:a aac -strict experimental "{output_path}"'
        os.system(cmd)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_video):
            os.remove(temp_video)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--template_id", required=True)
    parser.add_argument("--audio_path", required=True)
    parser.add_argument("--output_path", required=True)
    parser.add_argument("--template_dir", default="./wwwroot/templates")
    parser.add_argument("--fps", type=int, default=25)
    parser.add_argument("--batch_size", type=int, default=64)
    parser.add_argument("--unet_config", default="models/musetalk/musetalk.json")
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['TEMPLATE_DIR'] = args.template_dir
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.unet_config)
    config.batch_size = args.batch_size
    
    print("ğŸš€ å¯åŠ¨æŒä¹…åŒ–MuseTalkæ¨ç†å™¨...")
    
    # åˆ›å»ºæ¨ç†å™¨
    inference_engine = PersistentMuseTalkInference(config)
    
    # æ‰§è¡Œæ¨ç†
    start_time = time.time()
    result_path = inference_engine.inference(
        args.template_id, 
        args.audio_path, 
        args.output_path, 
        args.fps
    )
    
    elapsed_time = time.time() - start_time
    print(f"ğŸ‰ æ¨ç†å®Œæˆ! è€—æ—¶: {elapsed_time:.2f}s")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result_path}")

if __name__ == "__main__":
    main()
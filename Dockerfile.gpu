# Multi-stage build for MuseTalk GPU deployment
# 支持CUDA 12.x版本的完整Docker镜像

# Stage 1: Base CUDA image with Python (使用CUDA 12.1版本，兼容12.9)
FROM nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04 AS base

# 防止交互式提示
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    CUDA_HOME=/usr/local/cuda \
    PATH=/usr/local/cuda/bin:${PATH} \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH} \
    TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9;9.0" \
    FORCE_CUDA=1 \
    CUDA_DEVICE_ORDER=PCI_BUS_ID

# 安装系统依赖
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Python和构建工具
    python3.10 \
    python3.10-dev \
    python3.10-distutils \
    python3-pip \
    python3-setuptools \
    build-essential \
    cmake \
    ninja-build \
    pkg-config \
    # Git和网络工具
    git \
    git-lfs \
    wget \
    curl \
    ca-certificates \
    # 音视频处理
    ffmpeg \
    libavcodec-dev \
    libavformat-dev \
    libavutil-dev \
    libswscale-dev \
    libswresample-dev \
    # OpenCV依赖
    libopencv-dev \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    libgl1-mesa-glx \
    libglib2.0-dev \
    # 其他工具
    vim \
    tmux \
    htop \
    iotop \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# 创建python别名
RUN ln -sf /usr/bin/python3.10 /usr/bin/python && \
    ln -sf /usr/bin/python3.10 /usr/bin/python3

# 升级pip
RUN python -m pip install --no-cache-dir --upgrade pip setuptools wheel

# Stage 2: Python dependencies
FROM base AS python-deps

WORKDIR /tmp

# 安装PyTorch和相关库（使用CUDA 12.1版本，兼容12.x）
RUN pip install --no-cache-dir \
    torch==2.2.0+cu121 \
    torchvision==0.17.0+cu121 \
    torchaudio==2.2.0+cu121 \
    --index-url https://download.pytorch.org/whl/cu121

# 复制requirements文件（如果存在）
COPY requirements.txt* /tmp/

# 安装MuseTalk核心依赖
RUN if [ -f /tmp/requirements.txt ]; then \
        pip install --no-cache-dir -r /tmp/requirements.txt; \
    fi

# 安装额外的依赖
RUN pip install --no-cache-dir \
    # 深度学习相关
    transformers==4.39.2 \
    diffusers==0.30.2 \
    accelerate==0.28.0 \
    einops==0.8.1 \
    omegaconf \
    # 音视频处理
    opencv-python==4.9.0.80 \
    opencv-contrib-python==4.9.0.80 \
    imageio[ffmpeg] \
    imageio-ffmpeg \
    moviepy \
    soundfile==0.12.1 \
    librosa==0.11.0 \
    ffmpeg-python \
    # Web和API
    gradio==5.24.0 \
    fastapi \
    uvicorn[standard] \
    # 监控和性能
    nvidia-ml-py3 \
    pynvml \
    gpustat \
    psutil \
    py-cpuinfo \
    # 工具库
    numpy==1.23.5 \
    scipy \
    pandas \
    matplotlib \
    pillow \
    tqdm \
    pyyaml \
    requests \
    gdown \
    huggingface_hub

# Stage 3: Application setup
FROM python-deps AS app

# 设置工作目录
WORKDIR /app

# 创建必要的目录结构
RUN mkdir -p \
    /app/models \
    /app/inputs \
    /app/outputs \
    /app/logs \
    /app/cache \
    /app/temp \
    /app/configs \
    /app/MuseTalk \
    /app/MuseTalkEngine

# 设置Python路径
ENV PYTHONPATH=/app:/app/MuseTalk:${PYTHONPATH}

# 创建模型下载脚本
RUN cat > /app/download_models.py << 'EOF'
#!/usr/bin/env python3
import os
import sys
import subprocess
from pathlib import Path

def download_models():
    """下载所有必需的模型"""
    models_dir = Path("/app/models")
    models_dir.mkdir(exist_ok=True)
    
    # 检查是否已下载
    if (models_dir / "musetalkV15" / "unet.pth").exists():
        print("Models already downloaded")
        return
    
    print("Downloading models...")
    
    # 设置镜像（如果在中国）
    hf_endpoint = os.environ.get('HF_ENDPOINT', 'https://huggingface.co')
    if os.environ.get('USE_CHINA_MIRROR', 'false').lower() == 'true':
        hf_endpoint = 'https://hf-mirror.com'
        os.environ['HF_ENDPOINT'] = hf_endpoint
        print(f"Using HuggingFace mirror: {hf_endpoint}")
    
    # 安装huggingface-cli
    subprocess.run([sys.executable, "-m", "pip", "install", "-q", "huggingface_hub[cli]"], check=False)
    
    # 下载模型
    commands = [
        ["huggingface-cli", "download", "TMElyralab/MuseTalk", 
         "--local-dir", "/app/models", 
         "--include", "musetalkV15/*", "musetalk/*"],
        ["huggingface-cli", "download", "stabilityai/sd-vae-ft-mse",
         "--local-dir", "/app/models/sd-vae",
         "--include", "*.bin", "*.json"],
        ["huggingface-cli", "download", "openai/whisper-tiny",
         "--local-dir", "/app/models/whisper",
         "--include", "*.bin", "*.json"],
        ["huggingface-cli", "download", "yzd-v/DWPose",
         "--local-dir", "/app/models/dwpose",
         "--include", "*.pth"],
        ["huggingface-cli", "download", "ByteDance/LatentSync",
         "--local-dir", "/app/models/syncnet",
         "--include", "*.pt"],
    ]
    
    for cmd in commands:
        try:
            print(f"Downloading: {cmd[2]}")
            subprocess.run(cmd, check=False, capture_output=True)
        except Exception as e:
            print(f"Warning downloading {cmd[2]}: {e}")
    
    # 下载face-parse-bisent（使用gdown）
    try:
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", "gdown"], check=False)
        import gdown
        os.makedirs("/app/models/face-parse-bisent", exist_ok=True)
        gdown.download("https://drive.google.com/uc?id=154JgKpzCPW82qINcVieuPH3fZ2e0P812", 
                      "/app/models/face-parse-bisent/79999_iter.pth", quiet=False)
    except Exception as e:
        print(f"Warning downloading face-parse-bisent: {e}")
    
    print("Model download completed")

if __name__ == "__main__":
    download_models()
EOF

RUN chmod +x /app/download_models.py

# 创建启动脚本
RUN cat > /app/entrypoint.sh << 'EOF'
#!/bin/bash
set -e

echo "=========================================="
echo "MuseTalk Docker Container Starting..."
echo "=========================================="

# 显示环境信息
echo "System Information:"
echo "  - OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)"
echo "  - Kernel: $(uname -r)"
echo "  - Python: $(python --version)"

# 显示GPU信息
echo -e "\nGPU Information:"
if nvidia-smi &>/dev/null; then
    nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader
    echo "  - CUDA Runtime: $(nvidia-smi | grep "CUDA Version" | awk '{print $9}')"
else
    echo "  - No GPU detected (running in CPU mode)"
fi

# 显示PyTorch信息
echo -e "\nPyTorch Configuration:"
python -c "
import torch
print(f'  - PyTorch Version: {torch.__version__}')
print(f'  - CUDA Available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'  - CUDA Version: {torch.version.cuda}')
    print(f'  - cuDNN Version: {torch.backends.cudnn.version()}')
    print(f'  - GPU Count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'  - GPU {i}: {torch.cuda.get_device_name(i)}')
" 2>/dev/null || echo "  - PyTorch check failed"

# 检查环境变量
echo -e "\nEnvironment Configuration:"
echo "  - CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-all}"
echo "  - RUN_MODE: ${RUN_MODE:-inference}"
echo "  - BATCH_SIZE_PER_GPU: ${BATCH_SIZE_PER_GPU:-8}"
echo "  - INFERENCE_MODE: ${INFERENCE_MODE:-parallel}"

# 克隆MuseTalk代码（如果不存在）
if [ ! -d "/app/MuseTalk/.git" ] && [ "${AUTO_CLONE_REPO:-true}" = "true" ]; then
    echo -e "\nCloning MuseTalk repository..."
    cd /app
    git clone https://github.com/TMElyralab/MuseTalk.git || echo "Clone failed, continuing..."
fi

# 下载模型（如果需要）
if [ "${AUTO_DOWNLOAD_MODELS:-true}" = "true" ]; then
    echo -e "\nChecking models..."
    python /app/download_models.py
fi

# 创建必要的目录
mkdir -p /app/logs /app/outputs /app/inputs /app/cache

# 根据启动模式执行不同的命令
case "${RUN_MODE:-shell}" in
    "benchmark")
        echo -e "\nRunning benchmark..."
        if [ -f "/app/benchmark_gpu.py" ]; then
            exec python /app/benchmark_gpu.py --config ${GPU_CONFIG:-auto}
        else
            echo "Benchmark script not found"
            exec python -c "import torch; print(f'GPUs available: {torch.cuda.device_count()}')"
        fi
        ;;
    "inference")
        echo -e "\nRunning inference..."
        if [ -f "/app/MuseTalkEngine/musetalk_realtime_engine.py" ]; then
            exec python /app/MuseTalkEngine/musetalk_realtime_engine.py \
                --video ${VIDEO_PATH:-/app/inputs/video.mp4} \
                --audio ${AUDIO_PATH:-/app/inputs/audio.wav} \
                --output ${OUTPUT_PATH:-/app/outputs/result.mp4} \
                --mode ${INFERENCE_MODE:-parallel} \
                --gpus ${GPU_IDS:-auto}
        else
            echo "Inference engine not found"
            exec /bin/bash
        fi
        ;;
    "gradio")
        echo -e "\nStarting Gradio interface..."
        if [ -f "/app/MuseTalk/app.py" ]; then
            cd /app/MuseTalk
            exec python app.py \
                --server_name 0.0.0.0 \
                --server_port ${WEB_PORT:-7860} \
                --share
        else
            echo "Gradio app not found"
            exec /bin/bash
        fi
        ;;
    "api")
        echo -e "\nStarting API server..."
        if [ -f "/app/api_server.py" ]; then
            exec uvicorn api_server:app \
                --host 0.0.0.0 \
                --port ${API_PORT:-8000} \
                --workers ${API_WORKERS:-1}
        else
            echo "API server not found, starting test server..."
            exec python -m http.server ${API_PORT:-8000}
        fi
        ;;
    "shell")
        echo -e "\nStarting interactive shell..."
        echo "You can now run commands inside the container."
        echo "Example: python /app/benchmark_gpu.py --config auto"
        exec /bin/bash
        ;;
    *)
        echo -e "\nExecuting custom command: $@"
        exec "$@"
        ;;
esac
EOF

RUN chmod +x /app/entrypoint.sh

# 创建健康检查脚本
RUN cat > /app/healthcheck.py << 'EOF'
#!/usr/bin/env python3
import sys
import torch

try:
    # 检查PyTorch
    if not torch.__version__:
        print("PyTorch not installed")
        sys.exit(1)
    
    # 检查CUDA（可选）
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"Health check passed: {gpu_count} GPUs available")
    else:
        print("Health check passed: CPU mode")
    
    sys.exit(0)
    
except Exception as e:
    print(f"Health check failed: {e}")
    sys.exit(1)
EOF

RUN chmod +x /app/healthcheck.py

# Stage 4: Production image
FROM app AS production

# 复制应用代码（构建时添加）
# 注意：实际代码将通过volume挂载或在运行时复制
COPY MuseTalk* /app/
COPY *.py /app/
COPY *.sh /app/
COPY *.yml /app/
COPY *.yaml /app/
COPY .env* /app/

# 暴露端口
EXPOSE 8000 8080 7860 9090

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python /app/healthcheck.py || exit 1

# 设置入口点
ENTRYPOINT ["/app/entrypoint.sh"]

# 默认命令
CMD ["shell"]
# å®Œæ•´çš„LinuxæœåŠ¡å™¨éƒ¨ç½²å‘½ä»¤æ­¥éª¤

## ğŸ“‹ å‰ç½®å‡†å¤‡
ç¡®ä¿æœåŠ¡å™¨å·²å®‰è£…ï¼š
- Docker 20.10+
- NVIDIA Driver
- NVIDIA Container Toolkit

---

## ğŸš€ å®Œæ•´å‘½ä»¤æ­¥éª¤

### Step 1: SSHç™»å½•åˆ°æœåŠ¡å™¨

```bash
# ç™»å½•åˆ°æ‚¨çš„LinuxæœåŠ¡å™¨
ssh user@your-server-ip

# æˆ–ä½¿ç”¨å¯†é’¥
ssh -i your-key.pem user@your-server-ip
```

### Step 2: æ£€æŸ¥ç¯å¢ƒ

```bash
# æ£€æŸ¥GPUå’ŒCUDAç‰ˆæœ¬
nvidia-smi

# æ£€æŸ¥Docker
docker --version

# æ£€æŸ¥NVIDIA Container Toolkit
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# å¦‚æœä¸Šé¢å‘½ä»¤å¤±è´¥ï¼Œå®‰è£…NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### Step 3: å…‹éš†æŒ‡å®šåˆ†æ”¯ä»£ç 

```bash
# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ~/musetalk-test && cd ~/musetalk-test

# å…‹éš†æŒ‡å®šåˆ†æ”¯ (cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5)
git clone -b cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5 \
    https://github.com/your-username/your-repo.git .

# éªŒè¯å½“å‰åˆ†æ”¯
git branch
# åº”è¯¥æ˜¾ç¤º: * cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5

# æŸ¥çœ‹æ–‡ä»¶
ls -la
```

### Step 4: è®¾ç½®æƒé™å’Œé…ç½®

```bash
# èµ‹äºˆè„šæœ¬æ‰§è¡Œæƒé™
chmod +x deploy.sh setup_gpu_env.sh

# åˆ›å»ºå¿…è¦çš„ç›®å½•
mkdir -p inputs outputs logs models

# å¤åˆ¶å¹¶ç¼–è¾‘ç¯å¢ƒé…ç½®
cp .env.example .env

# ç¼–è¾‘é…ç½®ï¼ˆæ ¹æ®æ‚¨çš„GPUè°ƒæ•´ï¼‰
cat > .env << 'EOF'
# GPUé…ç½®
CUDA_VISIBLE_DEVICES=0,1,2,3
GPU_CONFIG_MODE=auto
BATCH_SIZE_PER_GPU=8
INFERENCE_MODE=parallel

# æ€§èƒ½ä¼˜åŒ–
ENABLE_AMP=true
ENABLE_CUDNN_BENCHMARK=true

# è¿è¡Œæ¨¡å¼
RUN_MODE=shell
AUTO_DOWNLOAD_MODELS=true
AUTO_CLONE_REPO=true

# ä½¿ç”¨ä¸­å›½é•œåƒï¼ˆå¦‚æœåœ¨ä¸­å›½ï¼‰
USE_CHINA_MIRROR=false
HF_ENDPOINT=https://huggingface.co

# æœåŠ¡é…ç½®
API_PORT=8000
WEB_PORT=7860
LOG_LEVEL=INFO
EOF
```

### Step 5: æ„å»ºDockeré•œåƒ

```bash
# æ„å»ºDockeré•œåƒï¼ˆæ”¯æŒCUDA 12.xï¼‰
docker build -f Dockerfile.gpu -t musetalk:gpu-test .

# æŸ¥çœ‹æ„å»ºçš„é•œåƒ
docker images | grep musetalk
```

### Step 6: é¦–æ¬¡è¿è¡Œæµ‹è¯•ï¼ˆäº¤äº’å¼Shellï¼‰

```bash
# å¯åŠ¨äº¤äº’å¼å®¹å™¨è¿›è¡Œæµ‹è¯•
docker run -it --rm \
    --name musetalk-test \
    --runtime=nvidia \
    --gpus all \
    -v $(pwd):/workspace \
    -v $(pwd)/models:/app/models \
    -v $(pwd)/inputs:/app/inputs \
    -v $(pwd)/outputs:/app/outputs \
    -e RUN_MODE=shell \
    musetalk:gpu-test
```

### Step 7: åœ¨å®¹å™¨å†…æ‰§è¡Œæµ‹è¯•

```bash
# === ä»¥ä¸‹å‘½ä»¤åœ¨Dockerå®¹å™¨å†…æ‰§è¡Œ ===

# 1. æ£€æŸ¥ç¯å¢ƒ
python -c "
import torch
print(f'PyTorch Version: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'CUDA Version: {torch.version.cuda}')
print(f'GPU Count: {torch.cuda.device_count()}')
for i in range(torch.cuda.device_count()):
    props = torch.cuda.get_device_properties(i)
    print(f'GPU {i}: {props.name}, Memory: {props.total_memory/1024**3:.1f}GB')
"

# 2. å…‹éš†MuseTalkå®˜æ–¹ä»£ç ï¼ˆå¦‚æœéœ€è¦ï¼‰
if [ ! -d "/app/MuseTalk" ]; then
    cd /app
    git clone https://github.com/TMElyralab/MuseTalk.git
fi

# 3. ä¸‹è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰
python /app/download_models.py

# 4. è¿è¡ŒGPUåŸºå‡†æµ‹è¯•
cd /workspace
python benchmark_gpu.py --config auto --quick

# 5. é€€å‡ºå®¹å™¨
exit
```

### Step 8: è¿è¡Œå®Œæ•´æµ‹è¯•

```bash
# å•GPUæµ‹è¯•ï¼ˆRTX 4090D 48GBï¼‰
docker run --rm \
    --runtime=nvidia \
    --gpus '"device=0"' \
    -v $(pwd):/workspace \
    -e CUDA_VISIBLE_DEVICES=0 \
    -e GPU_CONFIG=single_4090d_48gb \
    -e BATCH_SIZE_PER_GPU=20 \
    musetalk:gpu-test \
    bash -c "cd /workspace && python benchmark_gpu.py --config single_4090d_48gb"

# å¤šGPUæµ‹è¯•ï¼ˆ4x RTX 4090 24GBï¼‰
docker run --rm \
    --runtime=nvidia \
    --gpus all \
    -v $(pwd):/workspace \
    -e CUDA_VISIBLE_DEVICES=0,1,2,3 \
    -e GPU_CONFIG=quad_4090_24gb \
    -e BATCH_SIZE_PER_GPU=8 \
    musetalk:gpu-test \
    bash -c "cd /workspace && python benchmark_gpu.py --config quad_4090_24gb"
```

### Step 9: ä½¿ç”¨Docker Composeè¿è¡ŒæœåŠ¡

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose -f docker-compose.gpu.yml up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose -f docker-compose.gpu.yml ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.gpu.yml logs -f musetalk-gpu

# è¿›å…¥è¿è¡Œä¸­çš„å®¹å™¨
docker exec -it musetalk-gpu-service bash

# åœæ­¢æœåŠ¡
docker-compose -f docker-compose.gpu.yml down
```

### Step 10: è¿è¡Œæ¨ç†æµ‹è¯•

```bash
# å‡†å¤‡æµ‹è¯•æ–‡ä»¶
echo "è¯·å°†æµ‹è¯•è§†é¢‘æ”¾å…¥ inputs/video.mp4"
echo "è¯·å°†æµ‹è¯•éŸ³é¢‘æ”¾å…¥ inputs/audio.wav"

# è¿è¡Œæ¨ç†
docker run --rm \
    --runtime=nvidia \
    --gpus all \
    -v $(pwd):/workspace \
    -v $(pwd)/inputs:/app/inputs \
    -v $(pwd)/outputs:/app/outputs \
    -e RUN_MODE=inference \
    -e VIDEO_PATH=/app/inputs/video.mp4 \
    -e AUDIO_PATH=/app/inputs/audio.wav \
    -e OUTPUT_PATH=/app/outputs/result.mp4 \
    musetalk:gpu-test

# æŸ¥çœ‹è¾“å‡º
ls -la outputs/
```

---

## âœ… æµ‹è¯•æˆåŠŸååˆå¹¶åˆ°ä¸»åˆ†æ”¯

### Step 11: æäº¤æµ‹è¯•ç»“æœ

```bash
# ä¿å­˜æµ‹è¯•ç»“æœ
mkdir -p test_results
cp benchmark_results_*.json test_results/
cp outputs/* test_results/ 2>/dev/null || true

# æäº¤åˆ°å½“å‰åˆ†æ”¯
git add test_results/
git commit -m "test: GPU parallelization benchmark results

- Tested on [GPUå‹å·]
- Single GPU: [æ€§èƒ½æ•°æ®]
- Multi GPU: [æ€§èƒ½æ•°æ®]
- Docker deployment successful"

git push origin cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5
```

### Step 12: åˆå¹¶åˆ°ä¸»åˆ†æ”¯

```bash
# æ–¹æ³•1: åœ¨æœåŠ¡å™¨ä¸Šç›´æ¥åˆå¹¶ï¼ˆå¦‚æœæœ‰æƒé™ï¼‰
git checkout main
git pull origin main
git merge cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5
git push origin main

# æ–¹æ³•2: é€šè¿‡GitHub/GitLabåˆ›å»ºPull Request
echo "è¯·åœ¨GitHub/GitLabä¸Šåˆ›å»ºPull Requestï¼š"
echo "From: cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5"
echo "To: main"
```

---

## ğŸš¢ åœ¨å…¶ä»–æœåŠ¡å™¨éƒ¨ç½²

### Step 13: å¯¼å‡ºDockeré•œåƒ

```bash
# åœ¨ç¬¬ä¸€å°æœåŠ¡å™¨ä¸Š
docker save musetalk:gpu-test | gzip > musetalk-gpu-test.tar.gz
ls -lh musetalk-gpu-test.tar.gz
```

### Step 14: åœ¨æ–°æœåŠ¡å™¨ä¸Šéƒ¨ç½²

```bash
# åœ¨æ–°æœåŠ¡å™¨ä¸Š
# 1. ä¼ è¾“é•œåƒæ–‡ä»¶
scp user@first-server:~/musetalk-test/musetalk-gpu-test.tar.gz .

# 2. åŠ è½½é•œåƒ
docker load < musetalk-gpu-test.tar.gz

# 3. å…‹éš†ç›¸åŒåˆ†æ”¯çš„ä»£ç 
git clone -b cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5 \
    https://github.com/your-username/your-repo.git musetalk-test
cd musetalk-test

# 4. è¿è¡Œæµ‹è¯•
docker run --rm --runtime=nvidia --gpus all \
    -v $(pwd):/workspace \
    musetalk:gpu-test \
    python /workspace/benchmark_gpu.py --config auto
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§å‘½ä»¤

```bash
# å®æ—¶ç›‘æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç›‘æ§Dockerå®¹å™¨
docker stats musetalk-test

# æŸ¥çœ‹å®¹å™¨å†…GPUä½¿ç”¨
docker exec musetalk-test nvidia-smi

# ç›‘æ§å†…å­˜ä½¿ç”¨
docker exec musetalk-test python -c "
import torch
for i in range(torch.cuda.device_count()):
    print(f'GPU {i}:')
    print(f'  Allocated: {torch.cuda.memory_allocated(i)/1024**3:.2f} GB')
    print(f'  Reserved: {torch.cuda.memory_reserved(i)/1024**3:.2f} GB')
"
```

---

## ğŸ”§ æ•…éšœæ’é™¤å‘½ä»¤

```bash
# å¦‚æœGPUä¸å¯è§
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# å¦‚æœæ¨¡å‹ä¸‹è½½å¤±è´¥ï¼ˆä½¿ç”¨ä¸­å›½é•œåƒï¼‰
docker run -it --rm \
    -v $(pwd)/models:/app/models \
    -e HF_ENDPOINT=https://hf-mirror.com \
    musetalk:gpu-test \
    python /app/download_models.py

# æ¸…ç†Dockerèµ„æº
docker system prune -a --volumes

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—
docker logs musetalk-test --tail 100 2>&1 | grep -i error

# è°ƒè¯•æ¨¡å¼
docker run -it --rm \
    --runtime=nvidia \
    --gpus all \
    -v $(pwd):/workspace \
    --entrypoint /bin/bash \
    musetalk:gpu-test
```

---

## âœ… å®Œæ•´æµ‹è¯•æ£€æŸ¥æ¸…å•

```bash
# è¿è¡Œæ­¤è„šæœ¬æ£€æŸ¥æ‰€æœ‰é¡¹ç›®
cat > check_deployment.sh << 'EOF'
#!/bin/bash
echo "=== Deployment Check ==="
echo "1. Docker Version:"
docker --version

echo -e "\n2. NVIDIA Docker:"
docker run --rm --runtime=nvidia --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi | head -5

echo -e "\n3. Current Branch:"
git branch | grep "*"

echo -e "\n4. Docker Images:"
docker images | grep musetalk

echo -e "\n5. GPU Test:"
docker run --rm --runtime=nvidia --gpus all musetalk:gpu-test python -c "
import torch
print(f'GPUs Available: {torch.cuda.device_count()}')
"

echo -e "\n6. Model Files:"
ls -la models/ 2>/dev/null | head -5 || echo "Models not downloaded yet"

echo -e "\n=== Check Complete ==="
EOF

chmod +x check_deployment.sh
./check_deployment.sh
```

---

## ğŸ“ æ€»ç»“

å®Œæ•´æµç¨‹ï¼š
1. SSHç™»å½•æœåŠ¡å™¨
2. å…‹éš†æŒ‡å®šåˆ†æ”¯ `cursor/test-muse-talk-gpu-parallelization-and-batch-inference-84c5`
3. æ„å»ºDockeré•œåƒ
4. è¿è¡Œæµ‹è¯•
5. éªŒè¯æˆåŠŸååˆå¹¶åˆ°mainåˆ†æ”¯

è¿™å¥—å‘½ä»¤å·²ç»ï¼š
- âœ… é€‚é…CUDA 12.9
- âœ… åŒ…å«å®Œæ•´çš„Pythonç¯å¢ƒ
- âœ… è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
- âœ… æ”¯æŒå¤šGPUå¹¶è¡Œ
- âœ… å¯åœ¨å¤šæœåŠ¡å™¨å¤ç”¨